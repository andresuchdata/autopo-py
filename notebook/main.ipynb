{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e2257",
   "metadata": {},
   "source": [
    "### Add Supply Chain params for AutoPO\n",
    "\n",
    "- Safety stock - (max sales x max lead time) - (avg sales x avg lead time)\n",
    "- Reorder point - avg sales x avg lead time + safety stock\n",
    "- Stock cover days (for 21 days) - avg sales x 21\n",
    "- RoP_Reference (1 -> RoP > Stock cover days, 0 -> RoP < Stock cover days)\n",
    "- Current stock days cover -> Current stock / avg sales\n",
    "- Is_open_po (1 -> Current Stock < Reorder point, 0 -> otherwise)\n",
    "- Initial_Qty_PO - Reorder point - Current stock\n",
    "\n",
    "- Is_emergency_PO - 1 -> Current stock days cover <= max lead time\n",
    "\n",
    "- Emergency_PO_Qty - (max lead time - Current stock days cover) x Avg sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468e9d6",
   "metadata": {},
   "source": [
    "# Stock health Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f8fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading store data...\n",
      "✅ Loaded 28. Miss Glam Aceh.xlsx with 3545 rows\n",
      "✅ Loaded 20. Miss Glam Tanjung Pinang.xlsx with 4050 rows\n",
      "✅ Loaded 30. Miss Glam Sei Penuh.xlsx with 3604 rows\n",
      "✅ Loaded 21. Miss Glam Sutomo.xlsx with 5744 rows\n",
      "✅ Loaded 3. Miss Glam Jambi.xlsx with 5270 rows\n",
      "✅ Loaded 15. Miss Glam Tembilahan.xlsx with 4358 rows\n",
      "✅ Loaded 7. Miss Glam Lampung.xlsx with 4464 rows\n",
      "✅ Loaded 14. Miss Glam Solok.xlsx with 4681 rows\n",
      "✅ Loaded 4. Miss Glam Bukittinggi.xlsx with 5320 rows\n",
      "✅ Loaded 16. Miss Glam Lubuk Linggau.xlsx with 4422 rows\n",
      "✅ Loaded 33. Miss Glam Balikpapan.xlsx with 4588 rows\n",
      "✅ Loaded 2. Miss Glam Pekanbaru.xlsx with 5973 rows\n",
      "✅ Loaded 23. Miss Glam Halat.xlsx with 4814 rows\n",
      "✅ Loaded 26. Miss Glam Dr. Mansyur.xlsx with 4959 rows\n",
      "✅ Loaded 17. Miss Glam Dumai.xlsx with 4789 rows\n",
      "✅ Loaded 19. Miss Glam Rantau Prapat.xlsx with 4281 rows\n",
      "✅ Loaded 9. Miss Glam Medan.xlsx with 5790 rows\n",
      "✅ Loaded 18. Miss Glam Kedaton.xlsx with 4233 rows\n",
      "✅ Loaded 25. Miss Glam Sudirman.xlsx with 5649 rows\n",
      "✅ Loaded 31. Miss Glam Mayang.xlsx with 4289 rows\n",
      "✅ Loaded 13.Miss Glam Payakumbuh.xlsx with 5323 rows\n",
      "✅ Loaded 1. Miss Glam Padang.xlsx with 6781 rows\n",
      "✅ Loaded 10. Miss Glam Palembang.xlsx with 4881 rows\n",
      "✅ Loaded 8. Miss Glam Bengkulu.xlsx with 3964 rows\n",
      "✅ Loaded 5. Miss Glam Panam.xlsx with 5286 rows\n",
      "✅ Loaded 22. Miss Glam Pasaman Barat.xlsx with 3846 rows\n",
      "✅ Loaded 32. Miss Glam Soeta.xlsx with 4544 rows\n",
      "✅ Loaded 24. Miss Glam Duri.xlsx with 3996 rows\n",
      "✅ Loaded 27. Miss Glam P. Sidimpuan.xlsx with 3782 rows\n",
      "✅ Loaded 29. Miss Glam Marpoyan.xlsx with 4656 rows\n",
      "✅ Loaded 6. Miss Glam Muaro Bungo.xlsx with 4932 rows\n",
      "✅ Loaded 12. Miss Glam Bangka.xlsx with 4559 rows\n",
      "✅ Loaded 11. Miss Glam Damar.xlsx with 6658 rows\n",
      "\n",
      "Calculating stock health metrics...\n",
      "\n",
      "Creating dashboard...\n",
      "\n",
      "✅ Dashboard saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output_analysis/stock_health_dashboard_20251127_113959.html\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    185\u001b[39m     fig = main()\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Stock Health Dashboard for Miss Glam\n",
    "# This notebook processes store stock data and creates an interactive dashboard for monitoring stock health\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path('/Users/andresuchitra/dev/missglam/autopo/notebook/output/complete')\n",
    "OUTPUT_DIR = Path('/Users/andresuchitra/dev/missglam/autopo/notebook/output_analysis')\n",
    "\n",
    "def load_store_data():\n",
    "    \"\"\"Load and combine data from all store files\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in DATA_DIR.glob('*.xlsx'):\n",
    "        try:\n",
    "            store_name = file_path.stem.replace('Miss Glam', '').strip()\n",
    "            df = pd.read_excel(file_path)\n",
    "            df['Store'] = store_name\n",
    "            all_data.append(df)\n",
    "            print(f\"✅ Loaded {file_path.name} with {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data files found. Please check the DATA_DIR path.\")\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "def calculate_stock_health(df):\n",
    "    \"\"\"Calculate stock health metrics\"\"\"\n",
    "    # Ensure we have required columns\n",
    "    required_cols = ['current_stock_days_cover', 'HPP', 'Stock']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_cols)}\")\n",
    "    \n",
    "    # Define stock health categories\n",
    "    conditions = [\n",
    "        (df['current_stock_days_cover'] > 30),\n",
    "        (df['current_stock_days_cover'] > 20) & (df['current_stock_days_cover'] <= 30),\n",
    "        (df['current_stock_days_cover'] > 6) & (df['current_stock_days_cover'] <= 20),\n",
    "        (df['current_stock_days_cover'] > 0) & (df['current_stock_days_cover'] <= 6),\n",
    "        (df['current_stock_days_cover'] <= 0)\n",
    "    ]\n",
    "    \n",
    "    health_labels = ['Blue (30+ days)', 'Green (21-30 days)', 'Yellow (7-20 days)', 'Red (1-6 days)', 'Black (0 days)']\n",
    "    health_colors = ['#1f77b4', '#2ca02c', '#ffd700', '#ff7f0e', '#000000']\n",
    "    \n",
    "    df['Health_Group'] = np.select(conditions, health_labels, default='Unknown')\n",
    "    df['Health_Color'] = np.select(conditions, health_colors, default='#CCCCCC')\n",
    "    \n",
    "    # Calculate total value\n",
    "    df['Total_Value'] = df['HPP'] * df['Stock']\n",
    "    \n",
    "    return df, health_labels, health_colors\n",
    "\n",
    "def create_dashboard(df, health_labels, health_colors):\n",
    "    \"\"\"Create interactive dashboard\"\"\"\n",
    "    # Create summary metrics\n",
    "    total_skus = len(df)\n",
    "    total_stores = df['Store'].nunique()\n",
    "    total_value = df['Total_Value'].sum() / 1e6  # in millions\n",
    "    \n",
    "    # Create health group summary\n",
    "    health_summary = df.groupby('Health_Group').agg(\n",
    "        SKU_Count=('SKU', 'count'),\n",
    "        Total_Qty=('Stock', 'sum'),\n",
    "        Total_Value=('Total_Value', 'sum')\n",
    "    ).reindex(health_labels).reset_index()\n",
    "    \n",
    "    # Create figures\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\", \"colspan\": 2}, None]],\n",
    "        subplot_titles=(\"SKU Distribution by Health Group\", \n",
    "                       \"Total Value by Health Group\",\n",
    "                       \"Stock Quantity by Health Group\")\n",
    "    )\n",
    "    \n",
    "    # Pie chart for SKU distribution\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=health_summary['Health_Group'],\n",
    "            values=health_summary['SKU_Count'],\n",
    "            marker_colors=health_colors,\n",
    "            name=\"SKU Distribution\",\n",
    "            hole=0.4\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Bar chart for total value\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=health_summary['Health_Group'],\n",
    "            y=health_summary['Total_Value'] / 1e6,  # in millions\n",
    "            marker_color=health_colors,\n",
    "            name=\"Total Value (Million)\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Bar chart for total quantity\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=health_summary['Health_Group'],\n",
    "            y=health_summary['Total_Qty'],\n",
    "            marker_color=health_colors,\n",
    "            name=\"Total Quantity\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Stock Health Dashboard\",\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    # Add annotations for summary metrics\n",
    "    fig.add_annotation(\n",
    "        text=f\"<b>Total SKUs:</b> {total_skus:,}<br>\" +\n",
    "             f\"<b>Total Stores:</b> {total_stores}<br>\" +\n",
    "             f\"<b>Total Value:</b> {total_value:,.2f}M\",\n",
    "        align='left',\n",
    "        showarrow=False,\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        x=0.05,\n",
    "        y=0.95,\n",
    "        bordercolor='black',\n",
    "        borderwidth=1,\n",
    "        bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data for analysis\"\"\"\n",
    "    # Ensure we have required columns\n",
    "    df_output = df.copy()\n",
    "\n",
    "    # rename 'Stok' to 'Stock'\n",
    "    df_output.rename(columns={'Stok': 'Stock'}, inplace=True)\n",
    "    \n",
    "    return df_output\n",
    "\n",
    "def main():\n",
    "    # Create output directory if it doesn't exist\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load and process data\n",
    "    print(\"Loading store data...\")\n",
    "    df = load_store_data()\n",
    "\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    print(\"\\nCalculating stock health metrics...\")\n",
    "    df, health_labels, health_colors = calculate_stock_health(df)\n",
    "    \n",
    "    # Create dashboard\n",
    "    print(\"\\nCreating dashboard...\")\n",
    "    fig = create_dashboard(df, health_labels, health_colors)\n",
    "    \n",
    "    # Save dashboard\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = OUTPUT_DIR / f\"stock_health_dashboard_{timestamp}.html\"\n",
    "    fig.write_html(str(output_file))\n",
    "    \n",
    "    print(f\"\\n✅ Dashboard saved to: {output_file}\")\n",
    "    return fig\n",
    "\n",
    "# Run the dashboard\n",
    "if __name__ == \"__main__\":\n",
    "    fig = main()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6ba5",
   "metadata": {},
   "source": [
    "# Final batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce491334",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = [\n",
    "    'HPP', 'Harga', 'Ranking', 'Grade', 'Terjual', 'Stok', 'Lost Days',\n",
    "    'Velocity Capped', 'Daily Sales', 'Lead Time', 'Max. Daily Sales',\n",
    "    'Max. Lead Time', 'Min. Order', 'Safety Stok', 'ROP', '3W Cover',\n",
    "    'Sedang PO', 'Suggested', 'Amount', 'Promo Factor', 'Delay Factor',\n",
    "    'Stock Cover', 'Days to Backup', 'Qty to Backup'\n",
    "]\n",
    "\n",
    "NA_VALUES = {\n",
    "    'NAN', 'NA', '#N/A', 'NULL', 'NONE', '', '?', '-', 'INF', '-INF',\n",
    "    '+INF', 'INFINITY', '-INFINITY', '1.#INF', '-1.#INF', '1.#QNAN'\n",
    "}\n",
    "\n",
    "def _patch_openpyxl_number_casting():\n",
    "    \"\"\"Ensure openpyxl won't crash when encountering NAN/INF in numeric cells.\"\"\"\n",
    "    print(\"Calling _patch_openpyxl_number_casting...\")\n",
    "\n",
    "    try:\n",
    "        from openpyxl.worksheet import _reader\n",
    "\n",
    "        original_cast = _reader._cast_number\n",
    "\n",
    "        def _safe_cast_number(value):  # pragma: no cover - monkey patch\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().upper() in NA_VALUES:\n",
    "                    return 0\n",
    "            try:\n",
    "                return original_cast(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0 if value in (None, '') else value\n",
    "\n",
    "        _reader._cast_number = _safe_cast_number\n",
    "    except Exception:\n",
    "        # If patch fails we continue; runtime reader will still attempt default behaviour\n",
    "        pass\n",
    "\n",
    "def load_special_sku_60(path):\n",
    "    print(f\"Loading Special SKU with 60 days target cover data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check file extension\n",
    "        file_ext = str(path).lower().split('.')[-1]\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Special SKU with 60 days target cover data with {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Special SKU with 60 days target cover data from {path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da39de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _patch_openpyxl_number_casting...\n",
      "Loading Special SKU with 60 days target cover data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/special_sku_60.csv...\n",
      "Successfully loaded Special SKU with 60 days target cover data with 40 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama Produk</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8999999595357</td>\n",
       "      <td>DOVE Perawatan Rambut Rontok Hair Tonic Spray ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8999999584207</td>\n",
       "      <td>DOVE Deep Cleanse Micellar Shampo Himalaya Sal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8999999526344</td>\n",
       "      <td>TRESEMME Shampoo Hair Fall Tresplex 170ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40200509458</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Pink 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40200509242</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Kuning 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40200509360</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Ungu 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8999999540159</td>\n",
       "      <td>VASELINE Repairing Jelly Aloevera 50ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8999999559588</td>\n",
       "      <td>VASELINE Body Lotion Serum Soft Glow 180ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8999999502942</td>\n",
       "      <td>VASELINE Repairing Jelly 50ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8999999035273</td>\n",
       "      <td>VASELINE Healthy Bright Spf 30 PA++ Gluta Vita...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8999999577421</td>\n",
       "      <td>VASELINE Healthy Bright Hijab Protect Gluta Vi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8999999502973</td>\n",
       "      <td>VASELINE Repairing Jelly 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8999999559595</td>\n",
       "      <td>VASELINE Body Serum Fiem Glow 180ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8999999577407</td>\n",
       "      <td>VASELINE Healthy Bright Radiant Gluta Vitamin ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30100244316</td>\n",
       "      <td>LUX Mood Library Peaceful Galaxy Body Scrub 360gr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30100167658</td>\n",
       "      <td>LUX Mood Library Peaceful Galaxy Shower Gel 470gr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10100507517</td>\n",
       "      <td>THE ORIGINOTE Micellar Cleansing Tissue 10pcs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10100724414</td>\n",
       "      <td>THE ORIGINOTE Hyalu BHA Acne Micellar Water 300ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20101207602</td>\n",
       "      <td>SEA MAKEUP Stayput Prime &amp; Set Acne Setting Sp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>725765154392</td>\n",
       "      <td>SEA MAKEUP Lock It Matte Acne Setting Spray 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>745604686181</td>\n",
       "      <td>GRACE AND GLOW Bightening Sun Body Serum 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30200424556</td>\n",
       "      <td>GRACE AND GLOW Whitw B-3 Bright Body Gel Serum...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40100204145</td>\n",
       "      <td>GRACE AND GLOW Velvet Breeze Dry Shampoo 150ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40100204024</td>\n",
       "      <td>GRACE AND GLOW Daisy Breeze Dry Shampoo 150ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30400519016</td>\n",
       "      <td>GRACE AND GLOW Invisible Smooth Antiperspirant...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8997230373085</td>\n",
       "      <td>DEAR ME BEAUTY Spf 50 + Skin Barrier Sunscreen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8997230373894</td>\n",
       "      <td>DEAR ME BEAUTY Serum Lip Tint Dear Vania 3.5ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8999908082800</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8999908266101</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8999908082701</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8999908805508</td>\n",
       "      <td>MARINA UV White Sunblock Spf 30 PA++ Hand &amp; Bo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8999908618702</td>\n",
       "      <td>MARINA Healty and Glow Body Scrub 200ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8999999581930</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Flawless Bri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8999999581947</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Dewy Radianc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9556126670654</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Flawless Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8999999719418</td>\n",
       "      <td>VASELINE Body Lotion Healty Bright Uv Extra Br...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8999999590055</td>\n",
       "      <td>VASELINE Healty Burst Lotion GLUTA-HYA Overnig...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8851932417464</td>\n",
       "      <td>VASELINE Daily Sun Refreshing SPF50+ 170ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8999999003043</td>\n",
       "      <td>VASELINE Healty Bright Sun Pollution Spf24 Pro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8999999596972</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Serum Burst ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SKU                                        Nama Produk  \\\n",
       "0   8999999595357  DOVE Perawatan Rambut Rontok Hair Tonic Spray ...   \n",
       "1   8999999584207  DOVE Deep Cleanse Micellar Shampo Himalaya Sal...   \n",
       "2   8999999526344          TRESEMME Shampoo Hair Fall Tresplex 170ml   \n",
       "3     40200509458       SUNSILK Multivitamin Hair Parfume Pink 100ml   \n",
       "4     40200509242     SUNSILK Multivitamin Hair Parfume Kuning 100ml   \n",
       "5     40200509360       SUNSILK Multivitamin Hair Parfume Ungu 100ml   \n",
       "6   8999999540159             VASELINE Repairing Jelly Aloevera 50ml   \n",
       "7   8999999559588         VASELINE Body Lotion Serum Soft Glow 180ml   \n",
       "8   8999999502942                      VASELINE Repairing Jelly 50ml   \n",
       "9   8999999035273  VASELINE Healthy Bright Spf 30 PA++ Gluta Vita...   \n",
       "10  8999999577421  VASELINE Healthy Bright Hijab Protect Gluta Vi...   \n",
       "11  8999999502973                     VASELINE Repairing Jelly 100ml   \n",
       "12  8999999559595                VASELINE Body Serum Fiem Glow 180ml   \n",
       "13  8999999577407  VASELINE Healthy Bright Radiant Gluta Vitamin ...   \n",
       "14    30100244316  LUX Mood Library Peaceful Galaxy Body Scrub 360gr   \n",
       "15    30100167658  LUX Mood Library Peaceful Galaxy Shower Gel 470gr   \n",
       "16    10100507517      THE ORIGINOTE Micellar Cleansing Tissue 10pcs   \n",
       "17    10100724414  THE ORIGINOTE Hyalu BHA Acne Micellar Water 300ml   \n",
       "18    20101207602  SEA MAKEUP Stayput Prime & Set Acne Setting Sp...   \n",
       "19   725765154392  SEA MAKEUP Lock It Matte Acne Setting Spray 100ml   \n",
       "20   745604686181     GRACE AND GLOW Bightening Sun Body Serum 100ml   \n",
       "21    30200424556  GRACE AND GLOW Whitw B-3 Bright Body Gel Serum...   \n",
       "22    40100204145     GRACE AND GLOW Velvet Breeze Dry Shampoo 150ml   \n",
       "23    40100204024      GRACE AND GLOW Daisy Breeze Dry Shampoo 150ml   \n",
       "24    30400519016  GRACE AND GLOW Invisible Smooth Antiperspirant...   \n",
       "25  8997230373085  DEAR ME BEAUTY Spf 50 + Skin Barrier Sunscreen...   \n",
       "26  8997230373894     DEAR ME BEAUTY Serum Lip Tint Dear Vania 3.5ml   \n",
       "27  8999908082800  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "28  8999908266101  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "29  8999908082701  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "30  8999908805508  MARINA UV White Sunblock Spf 30 PA++ Hand & Bo...   \n",
       "31  8999908618702            MARINA Healty and Glow Body Scrub 200ml   \n",
       "32  8999999581930  VASELINE Healthy Bright Gluta-Hya Flawless Bri...   \n",
       "33  8999999581947  VASELINE Healthy Bright Gluta-Hya Dewy Radianc...   \n",
       "34  9556126670654  VASELINE Healthy Bright Gluta-Hya Flawless Glo...   \n",
       "35  8999999719418  VASELINE Body Lotion Healty Bright Uv Extra Br...   \n",
       "36  8999999590055  VASELINE Healty Burst Lotion GLUTA-HYA Overnig...   \n",
       "37  8851932417464         VASELINE Daily Sun Refreshing SPF50+ 170ml   \n",
       "38  8999999003043  VASELINE Healty Bright Sun Pollution Spf24 Pro...   \n",
       "39  8999999596972  VASELINE Healthy Bright Gluta-Hya Serum Burst ...   \n",
       "\n",
       "    Unnamed: 2  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         NaN  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "18         NaN  \n",
       "19         NaN  \n",
       "20         NaN  \n",
       "21         NaN  \n",
       "22         NaN  \n",
       "23         NaN  \n",
       "24         NaN  \n",
       "25         NaN  \n",
       "26         NaN  \n",
       "27         NaN  \n",
       "28         NaN  \n",
       "29         NaN  \n",
       "30         NaN  \n",
       "31         NaN  \n",
       "32         NaN  \n",
       "33         NaN  \n",
       "34         NaN  \n",
       "35         NaN  \n",
       "36         NaN  \n",
       "37         NaN  \n",
       "38         NaN  \n",
       "39         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading supplier data: /Users/andresuchitra/dev/missglam/autopo/notebook/data/supplier.csv\n",
      "Loading Padang data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/rawpo/xlsx/1. Miss glam Padang.xlsx...\n",
      "Successfully loaded Padang data with 6741 rows\n",
      "\n",
      "Processing PO file: 1. Miss Glam Padang.xlsx ....\n",
      "  - Extracted location: PADANG\n",
      "\n",
      "Reading excel file: 1. Miss Glam Padang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 1. Miss Glam Padang.xlsx with 6741 rows\n",
      "Processing store: PADANG - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 135 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/1. Miss Glam Padang.csv\n",
      "  - Location: PADANG\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 6741\n",
      "  - 'Miss Glam Padang' suppliers: 6606 rows\n",
      "  - Other suppliers: 17 rows\n",
      "  - No supplier data: 118 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.xlsx\n",
      "\n",
      "Processing PO file: 10. Miss Glam Palembang.xlsx ....\n",
      "  - Extracted location: PALEMBANG\n",
      "\n",
      "Reading excel file: 10. Miss Glam Palembang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 10. Miss Glam Palembang.xlsx with 4848 rows\n",
      "Processing store: PALEMBANG - 26.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 113 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/10. Miss Glam Palembang.csv\n",
      "  - Location: PALEMBANG\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 4848\n",
      "  - 'Miss Glam Padang' suppliers: 10 rows\n",
      "  - Other suppliers: 4770 rows\n",
      "  - No supplier data: 68 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.xlsx\n",
      "\n",
      "Processing PO file: 11. Miss Glam Damar.xlsx ....\n",
      "  - Extracted location: DAMAR\n",
      "\n",
      "Reading excel file: 11. Miss Glam Damar.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 11. Miss Glam Damar.xlsx with 6635 rows\n",
      "Processing store: DAMAR - 91.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 136 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/11. Miss Glam Damar.csv\n",
      "  - Location: DAMAR\n",
      "  - Contribution: 91%\n",
      "  - Rows processed: 6635\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 6508 rows\n",
      "  - No supplier data: 127 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.xlsx\n",
      "\n",
      "Processing PO file: 12. Miss Glam Bangka.xlsx ....\n",
      "  - Extracted location: BANGKA\n",
      "\n",
      "Reading excel file: 12. Miss Glam Bangka.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 12. Miss Glam Bangka.xlsx with 4529 rows\n",
      "Processing store: BANGKA - 28.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 130 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/12. Miss Glam Bangka.csv\n",
      "  - Location: BANGKA\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 4529\n",
      "  - 'Miss Glam Padang' suppliers: 20 rows\n",
      "  - Other suppliers: 4430 rows\n",
      "  - No supplier data: 79 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.xlsx\n",
      "\n",
      "Processing PO file: 13. Miss Glam Payakumbuh.xlsx ....\n",
      "  - Extracted location: PAYAKUMBUH\n",
      "\n",
      "Reading excel file: 13. Miss Glam Payakumbuh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 13. Miss Glam Payakumbuh.xlsx with 5297 rows\n",
      "Processing store: PAYAKUMBUH - 47.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 83 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/13. Miss Glam Payakumbuh.csv\n",
      "  - Location: PAYAKUMBUH\n",
      "  - Contribution: 47%\n",
      "  - Rows processed: 5297\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 5217 rows\n",
      "  - No supplier data: 80 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.xlsx\n",
      "\n",
      "Processing PO file: 14. Miss Glam Solok.xlsx ....\n",
      "  - Extracted location: SOLOK\n",
      "\n",
      "Reading excel file: 14. Miss Glam Solok.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 14. Miss Glam Solok.xlsx with 4649 rows\n",
      "Processing store: SOLOK - 37.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 62 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/14. Miss Glam Solok.csv\n",
      "  - Location: SOLOK\n",
      "  - Contribution: 37%\n",
      "  - Rows processed: 4649\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 4591 rows\n",
      "  - No supplier data: 58 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.xlsx\n",
      "\n",
      "Processing PO file: 15. Miss Glam Tembilahan.xlsx ....\n",
      "  - Extracted location: TEMBILAHAN\n",
      "\n",
      "Reading excel file: 15. Miss Glam Tembilahan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 15. Miss Glam Tembilahan.xlsx with 4325 rows\n",
      "Processing store: TEMBILAHAN - 27.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 80 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/15. Miss Glam Tembilahan.csv\n",
      "  - Location: TEMBILAHAN\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 4325\n",
      "  - 'Miss Glam Padang' suppliers: 9 rows\n",
      "  - Other suppliers: 4251 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.xlsx\n",
      "\n",
      "Processing PO file: 16. Miss Glam Lubuk Linggau.xlsx ....\n",
      "  - Extracted location: LUBUK LINGGAU\n",
      "\n",
      "Reading excel file: 16. Miss Glam Lubuk Linggau.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 16. Miss Glam Lubuk Linggau.xlsx with 4399 rows\n",
      "Processing store: LUBUK LINGGAU - 26.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 178 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/16. Miss Glam Lubuk Linggau.csv\n",
      "  - Location: LUBUK LINGGAU\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 4399\n",
      "  - 'Miss Glam Padang' suppliers: 7 rows\n",
      "  - Other suppliers: 4332 rows\n",
      "  - No supplier data: 60 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.xlsx\n",
      "\n",
      "Processing PO file: 17. Miss Glam Dumai.xlsx ....\n",
      "  - Extracted location: DUMAI\n",
      "\n",
      "Reading excel file: 17. Miss Glam Dumai.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 17. Miss Glam Dumai.xlsx with 4754 rows\n",
      "Processing store: DUMAI - 36.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 68 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/17. Miss Glam Dumai.csv\n",
      "  - Location: DUMAI\n",
      "  - Contribution: 36%\n",
      "  - Rows processed: 4754\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 4689 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.xlsx\n",
      "\n",
      "Processing PO file: 18. Miss Glam Kedaton.xlsx ....\n",
      "  - Extracted location: KEDATON\n",
      "\n",
      "Reading excel file: 18. Miss Glam Kedaton.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 18. Miss Glam Kedaton.xlsx with 4210 rows\n",
      "Processing store: KEDATON - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 116 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/18. Miss Glam Kedaton.csv\n",
      "  - Location: KEDATON\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4210\n",
      "  - 'Miss Glam Padang' suppliers: 13 rows\n",
      "  - Other suppliers: 4132 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.xlsx\n",
      "\n",
      "Processing PO file: 19. Miss Glam Rantau Prapat.xlsx ....\n",
      "  - Extracted location: RANTAU PRAPAT\n",
      "\n",
      "Reading excel file: 19. Miss Glam Rantau Prapat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 19. Miss Glam Rantau Prapat.xlsx with 4256 rows\n",
      "Processing store: RANTAU PRAPAT - 27.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 97 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/19. Miss Glam Rantau Prapat.csv\n",
      "  - Location: RANTAU PRAPAT\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 4256\n",
      "  - 'Miss Glam Padang' suppliers: 17 rows\n",
      "  - Other suppliers: 4170 rows\n",
      "  - No supplier data: 69 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.xlsx\n",
      "\n",
      "Processing PO file: 2. Miss Glam Pekanbaru.xlsx ....\n",
      "  - Extracted location: PEKANBARU\n",
      "\n",
      "Reading excel file: 2. Miss Glam Pekanbaru.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 2. Miss Glam Pekanbaru.xlsx with 5935 rows\n",
      "Processing store: PEKANBARU - 60.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 120 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/2. Miss Glam Pekanbaru.csv\n",
      "  - Location: PEKANBARU\n",
      "  - Contribution: 60%\n",
      "  - Rows processed: 5935\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5824 rows\n",
      "  - No supplier data: 107 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.xlsx\n",
      "\n",
      "Processing PO file: 20. Miss Glam Tanjung Pinang.xlsx ....\n",
      "  - Extracted location: TANJUNG PINANG\n",
      "\n",
      "Reading excel file: 20. Miss Glam Tanjung Pinang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 20. Miss Glam Tanjung Pinang.xlsx with 4021 rows\n",
      "Processing store: TANJUNG PINANG - 19.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 99 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/20. Miss Glam Tanjung Pinang.csv\n",
      "  - Location: TANJUNG PINANG\n",
      "  - Contribution: 19%\n",
      "  - Rows processed: 4021\n",
      "  - 'Miss Glam Padang' suppliers: 8 rows\n",
      "  - Other suppliers: 3941 rows\n",
      "  - No supplier data: 72 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.xlsx\n",
      "\n",
      "Processing PO file: 21. Miss Glam Sutomo.xlsx ....\n",
      "  - Extracted location: SUTOMO\n",
      "\n",
      "Reading excel file: 21. Miss Glam Sutomo.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 21. Miss Glam Sutomo.xlsx with 5728 rows\n",
      "Processing store: SUTOMO - 49.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 90 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/21. Miss Glam Sutomo.csv\n",
      "  - Location: SUTOMO\n",
      "  - Contribution: 49%\n",
      "  - Rows processed: 5728\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5647 rows\n",
      "  - No supplier data: 77 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.xlsx\n",
      "\n",
      "Processing PO file: 22. Miss Glam Pasaman Barat.xlsx ....\n",
      "  - Extracted location: PASAMAN BARAT\n",
      "\n",
      "Reading excel file: 22. Miss Glam Pasaman Barat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 22. Miss Glam Pasaman Barat.xlsx with 3819 rows\n",
      "Processing store: PASAMAN BARAT - 17.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 61 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasaman Barat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasaman Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/22. Miss Glam Pasaman Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/22. Miss Glam Pasaman Barat.csv\n",
      "  - Location: PASAMAN BARAT\n",
      "  - Contribution: 17%\n",
      "  - Rows processed: 3819\n",
      "  - 'Miss Glam Padang' suppliers: 6 rows\n",
      "  - Other suppliers: 3771 rows\n",
      "  - No supplier data: 42 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasaman Barat.xlsx\n",
      "\n",
      "Processing PO file: 23. Miss Glam Halat.xlsx ....\n",
      "  - Extracted location: HALAT\n",
      "\n",
      "Reading excel file: 23. Miss Glam Halat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 23. Miss Glam Halat.xlsx with 4778 rows\n",
      "Processing store: HALAT - 31.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 114 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/23. Miss Glam Halat.csv\n",
      "  - Location: HALAT\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 4778\n",
      "  - 'Miss Glam Padang' suppliers: 15 rows\n",
      "  - Other suppliers: 4675 rows\n",
      "  - No supplier data: 88 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.xlsx\n",
      "\n",
      "Processing PO file: 24. Miss Glam Duri.xlsx ....\n",
      "  - Extracted location: DURI\n",
      "\n",
      "Reading excel file: 24. Miss Glam Duri.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 24. Miss Glam Duri.xlsx with 3971 rows\n",
      "Processing store: DURI - 28.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 57 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/24. Miss Glam Duri.csv\n",
      "  - Location: DURI\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 3971\n",
      "  - 'Miss Glam Padang' suppliers: 2 rows\n",
      "  - Other suppliers: 3917 rows\n",
      "  - No supplier data: 52 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.xlsx\n",
      "\n",
      "Processing PO file: 25. Miss Glam Sudirman.xlsx ....\n",
      "  - Extracted location: SUDIRMAN\n",
      "\n",
      "Reading excel file: 25. Miss Glam Sudirman.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 25. Miss Glam Sudirman.xlsx with 5622 rows\n",
      "Processing store: SUDIRMAN - 44.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 114 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/25. Miss Glam Sudirman.csv\n",
      "  - Location: SUDIRMAN\n",
      "  - Contribution: 44%\n",
      "  - Rows processed: 5622\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5520 rows\n",
      "  - No supplier data: 98 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.xlsx\n",
      "\n",
      "Processing PO file: 26. Miss Glam Dr. Mansyur.xlsx ....\n",
      "  - Extracted location: DR. MANSYUR\n",
      "\n",
      "Reading excel file: 26. Miss Glam Dr. Mansyur.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 26. Miss Glam Dr. Mansyur.xlsx with 4910 rows\n",
      "Processing store: DR. MANSYUR - 25.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 123 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/26. Miss Glam Dr. Mansyur.csv\n",
      "  - Location: DR. MANSYUR\n",
      "  - Contribution: 25%\n",
      "  - Rows processed: 4910\n",
      "  - 'Miss Glam Padang' suppliers: 14 rows\n",
      "  - Other suppliers: 4809 rows\n",
      "  - No supplier data: 87 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.xlsx\n",
      "\n",
      "Processing PO file: 27. Miss Glam P. Sidimpuan.xlsx ....\n",
      "  - Extracted location: P. SIDIMPUAN\n",
      "\n",
      "Reading excel file: 27. Miss Glam P. Sidimpuan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 27. Miss Glam P. Sidimpuan.xlsx with 3760 rows\n",
      "Processing store: P. SIDIMPUAN - 31.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 46 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/27. Miss Glam P. Sidimpuan.csv\n",
      "  - Location: P. SIDIMPUAN\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 3760\n",
      "  - 'Miss Glam Padang' suppliers: 5 rows\n",
      "  - Other suppliers: 3718 rows\n",
      "  - No supplier data: 37 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.xlsx\n",
      "\n",
      "Processing PO file: 28. Miss Glam Aceh.xlsx ....\n",
      "  - Extracted location: ACEH\n",
      "\n",
      "Reading excel file: 28. Miss Glam Aceh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 28. Miss Glam Aceh.xlsx with 3527 rows\n",
      "Processing store: ACEH - 15.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 71 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/28. Miss Glam Aceh.csv\n",
      "  - Location: ACEH\n",
      "  - Contribution: 15%\n",
      "  - Rows processed: 3527\n",
      "  - 'Miss Glam Padang' suppliers: 14 rows\n",
      "  - Other suppliers: 3468 rows\n",
      "  - No supplier data: 45 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.xlsx\n",
      "\n",
      "Processing PO file: 29. Miss Glam Marpoyan.xlsx ....\n",
      "  - Extracted location: MARPOYAN\n",
      "\n",
      "Reading excel file: 29. Miss Glam Marpoyan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 29. Miss Glam Marpoyan.xlsx with 4646 rows\n",
      "Processing store: MARPOYAN - 30.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 113 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/29. Miss Glam Marpoyan.csv\n",
      "  - Location: MARPOYAN\n",
      "  - Contribution: 30%\n",
      "  - Rows processed: 4646\n",
      "  - 'Miss Glam Padang' suppliers: 19 rows\n",
      "  - Other suppliers: 4560 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.xlsx\n",
      "\n",
      "Processing PO file: 3. Miss Glam Jambi.xlsx ....\n",
      "  - Extracted location: JAMBI\n",
      "\n",
      "Reading excel file: 3. Miss Glam Jambi.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 3. Miss Glam Jambi.xlsx with 5229 rows\n",
      "Processing store: JAMBI - 33.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 132 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/3. Miss Glam Jambi.csv\n",
      "  - Location: JAMBI\n",
      "  - Contribution: 33%\n",
      "  - Rows processed: 5229\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5137 rows\n",
      "  - No supplier data: 88 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.xlsx\n",
      "\n",
      "Processing PO file: 30. Miss Glam Sei Penuh.xlsx ....\n",
      "  - Extracted location: SEI PENUH\n",
      "\n",
      "Reading excel file: 30. Miss Glam Sei Penuh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 30. Miss Glam Sei Penuh.xlsx with 3591 rows\n",
      "Processing store: SEI PENUH - 21.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 75 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/30. Miss Glam Sei Penuh.csv\n",
      "  - Location: SEI PENUH\n",
      "  - Contribution: 21%\n",
      "  - Rows processed: 3591\n",
      "  - 'Miss Glam Padang' suppliers: 28 rows\n",
      "  - Other suppliers: 3534 rows\n",
      "  - No supplier data: 29 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.xlsx\n",
      "\n",
      "Processing PO file: 31. Miss Glam Mayang.xlsx ....\n",
      "  - Extracted location: MAYANG\n",
      "\n",
      "Reading excel file: 31. Miss Glam Mayang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 31. Miss Glam Mayang.xlsx with 4256 rows\n",
      "Processing store: MAYANG - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 232 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/31. Miss Glam Mayang.csv\n",
      "  - Location: MAYANG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4256\n",
      "  - 'Miss Glam Padang' suppliers: 31 rows\n",
      "  - Other suppliers: 4179 rows\n",
      "  - No supplier data: 46 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.xlsx\n",
      "\n",
      "Processing PO file: 32.  Miss Glam Soeta.xlsx ....\n",
      "  - Extracted location: SOETA\n",
      "Warning: No contribution percentage found for SOETA\n",
      "\n",
      "Reading excel file: 32.  Miss Glam Soeta.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 32.  Miss Glam Soeta.xlsx with 4527 rows\n",
      "Processing store: SOETA - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 2021 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32.  Miss Glam Soeta.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32.  Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/32.  Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/32.  Miss Glam Soeta.csv\n",
      "  - Location: SOETA\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 4527\n",
      "  - 'Miss Glam Padang' suppliers: 124 rows\n",
      "  - Other suppliers: 4362 rows\n",
      "  - No supplier data: 41 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32.  Miss Glam Soeta.xlsx\n",
      "\n",
      "Processing PO file: 33. Miss Glam Balikpapan.xlsx ....\n",
      "  - Extracted location: BALIKPAPAN\n",
      "Warning: No contribution percentage found for BALIKPAPAN\n",
      "\n",
      "Reading excel file: 33. Miss Glam Balikpapan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 33. Miss Glam Balikpapan.xlsx with 4588 rows\n",
      "Processing store: BALIKPAPAN - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 1631 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/33. Miss Glam Balikpapan.csv\n",
      "  - Location: BALIKPAPAN\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 4588\n",
      "  - 'Miss Glam Padang' suppliers: 85 rows\n",
      "  - Other suppliers: 4440 rows\n",
      "  - No supplier data: 63 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.xlsx\n",
      "\n",
      "Processing PO file: 4. Miss Glam Bukittinggi.xlsx ....\n",
      "  - Extracted location: BUKITTINGGI\n",
      "\n",
      "Reading excel file: 4. Miss Glam Bukittinggi.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 4. Miss Glam Bukittinggi.xlsx with 5286 rows\n",
      "Processing store: BUKITTINGGI - 45.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 72 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/4. Miss Glam Bukittinggi.csv\n",
      "  - Location: BUKITTINGGI\n",
      "  - Contribution: 45%\n",
      "  - Rows processed: 5286\n",
      "  - 'Miss Glam Padang' suppliers: 1 rows\n",
      "  - Other suppliers: 5220 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.xlsx\n",
      "\n",
      "Processing PO file: 5. Miss Glam Panam.xlsx ....\n",
      "  - Extracted location: PANAM\n",
      "\n",
      "Reading excel file: 5. Miss Glam Panam.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 5. Miss Glam Panam.xlsx with 5258 rows\n",
      "Processing store: PANAM - 46.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 98 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/5. Miss Glam Panam.csv\n",
      "  - Location: PANAM\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 5258\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 5164 rows\n",
      "  - No supplier data: 94 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.xlsx\n",
      "\n",
      "Processing PO file: 6. Miss Glam Muaro Bungo.xlsx ....\n",
      "  - Extracted location: MUARO BUNGO\n",
      "\n",
      "Reading excel file: 6. Miss Glam Muaro Bungo.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 6. Miss Glam Muaro Bungo.xlsx with 4913 rows\n",
      "Processing store: MUARO BUNGO - 42.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 95 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muaro Bungo.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muaro Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/6. Miss Glam Muaro Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/6. Miss Glam Muaro Bungo.csv\n",
      "  - Location: MUARO BUNGO\n",
      "  - Contribution: 42%\n",
      "  - Rows processed: 4913\n",
      "  - 'Miss Glam Padang' suppliers: 7 rows\n",
      "  - Other suppliers: 4839 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muaro Bungo.xlsx\n",
      "\n",
      "Processing PO file: 7. Miss Glam Lampung.xlsx ....\n",
      "  - Extracted location: LAMPUNG\n",
      "\n",
      "Reading excel file: 7. Miss Glam Lampung.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 7. Miss Glam Lampung.xlsx with 4437 rows\n",
      "Processing store: LAMPUNG - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 106 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/7. Miss Glam Lampung.csv\n",
      "  - Location: LAMPUNG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4437\n",
      "  - 'Miss Glam Padang' suppliers: 16 rows\n",
      "  - Other suppliers: 4357 rows\n",
      "  - No supplier data: 64 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.xlsx\n",
      "\n",
      "Processing PO file: 8. Miss Glam Bengkulu.xlsx ....\n",
      "  - Extracted location: BENGKULU\n",
      "\n",
      "Reading excel file: 8. Miss Glam Bengkulu.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 8. Miss Glam Bengkulu.xlsx with 3941 rows\n",
      "Processing store: BENGKULU - 14.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 107 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/8. Miss Glam Bengkulu.csv\n",
      "  - Location: BENGKULU\n",
      "  - Contribution: 14%\n",
      "  - Rows processed: 3941\n",
      "  - 'Miss Glam Padang' suppliers: 12 rows\n",
      "  - Other suppliers: 3856 rows\n",
      "  - No supplier data: 73 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.xlsx\n",
      "\n",
      "Processing PO file: 9. Miss Glam Medan.xlsx ....\n",
      "  - Extracted location: MEDAN\n",
      "\n",
      "Reading excel file: 9. Miss Glam Medan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_19206/3312070225.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 9. Miss Glam Medan.xlsx with 5734 rows\n",
      "Processing store: MEDAN - 46.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 151 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/9. Miss Glam Medan.csv\n",
      "  - Location: MEDAN\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 5734\n",
      "  - 'Miss Glam Padang' suppliers: 15 rows\n",
      "  - Other suppliers: 5610 rows\n",
      "  - No supplier data: 109 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.xlsx\n",
      "\n",
      "Processing complete! Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>location</th>\n",
       "      <th>contribution_pct</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>padang_suppliers</th>\n",
       "      <th>other_suppliers</th>\n",
       "      <th>no_supplier</th>\n",
       "      <th>status</th>\n",
       "      <th>output_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Miss Glam Padang.xlsx</td>\n",
       "      <td>PADANG</td>\n",
       "      <td>100</td>\n",
       "      <td>6741</td>\n",
       "      <td>6606</td>\n",
       "      <td>17</td>\n",
       "      <td>118</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10. Miss Glam Palembang.xlsx</td>\n",
       "      <td>PALEMBANG</td>\n",
       "      <td>26</td>\n",
       "      <td>4848</td>\n",
       "      <td>10</td>\n",
       "      <td>4770</td>\n",
       "      <td>68</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11. Miss Glam Damar.xlsx</td>\n",
       "      <td>DAMAR</td>\n",
       "      <td>91</td>\n",
       "      <td>6635</td>\n",
       "      <td>0</td>\n",
       "      <td>6508</td>\n",
       "      <td>127</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12. Miss Glam Bangka.xlsx</td>\n",
       "      <td>BANGKA</td>\n",
       "      <td>28</td>\n",
       "      <td>4529</td>\n",
       "      <td>20</td>\n",
       "      <td>4430</td>\n",
       "      <td>79</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13. Miss Glam Payakumbuh.xlsx</td>\n",
       "      <td>PAYAKUMBUH</td>\n",
       "      <td>47</td>\n",
       "      <td>5297</td>\n",
       "      <td>0</td>\n",
       "      <td>5217</td>\n",
       "      <td>80</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14. Miss Glam Solok.xlsx</td>\n",
       "      <td>SOLOK</td>\n",
       "      <td>37</td>\n",
       "      <td>4649</td>\n",
       "      <td>0</td>\n",
       "      <td>4591</td>\n",
       "      <td>58</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15. Miss Glam Tembilahan.xlsx</td>\n",
       "      <td>TEMBILAHAN</td>\n",
       "      <td>27</td>\n",
       "      <td>4325</td>\n",
       "      <td>9</td>\n",
       "      <td>4251</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16. Miss Glam Lubuk Linggau.xlsx</td>\n",
       "      <td>LUBUK LINGGAU</td>\n",
       "      <td>26</td>\n",
       "      <td>4399</td>\n",
       "      <td>7</td>\n",
       "      <td>4332</td>\n",
       "      <td>60</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17. Miss Glam Dumai.xlsx</td>\n",
       "      <td>DUMAI</td>\n",
       "      <td>36</td>\n",
       "      <td>4754</td>\n",
       "      <td>0</td>\n",
       "      <td>4689</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18. Miss Glam Kedaton.xlsx</td>\n",
       "      <td>KEDATON</td>\n",
       "      <td>18</td>\n",
       "      <td>4210</td>\n",
       "      <td>13</td>\n",
       "      <td>4132</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19. Miss Glam Rantau Prapat.xlsx</td>\n",
       "      <td>RANTAU PRAPAT</td>\n",
       "      <td>27</td>\n",
       "      <td>4256</td>\n",
       "      <td>17</td>\n",
       "      <td>4170</td>\n",
       "      <td>69</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2. Miss Glam Pekanbaru.xlsx</td>\n",
       "      <td>PEKANBARU</td>\n",
       "      <td>60</td>\n",
       "      <td>5935</td>\n",
       "      <td>4</td>\n",
       "      <td>5824</td>\n",
       "      <td>107</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20. Miss Glam Tanjung Pinang.xlsx</td>\n",
       "      <td>TANJUNG PINANG</td>\n",
       "      <td>19</td>\n",
       "      <td>4021</td>\n",
       "      <td>8</td>\n",
       "      <td>3941</td>\n",
       "      <td>72</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21. Miss Glam Sutomo.xlsx</td>\n",
       "      <td>SUTOMO</td>\n",
       "      <td>49</td>\n",
       "      <td>5728</td>\n",
       "      <td>4</td>\n",
       "      <td>5647</td>\n",
       "      <td>77</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22. Miss Glam Pasaman Barat.xlsx</td>\n",
       "      <td>PASAMAN BARAT</td>\n",
       "      <td>17</td>\n",
       "      <td>3819</td>\n",
       "      <td>6</td>\n",
       "      <td>3771</td>\n",
       "      <td>42</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23. Miss Glam Halat.xlsx</td>\n",
       "      <td>HALAT</td>\n",
       "      <td>31</td>\n",
       "      <td>4778</td>\n",
       "      <td>15</td>\n",
       "      <td>4675</td>\n",
       "      <td>88</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24. Miss Glam Duri.xlsx</td>\n",
       "      <td>DURI</td>\n",
       "      <td>28</td>\n",
       "      <td>3971</td>\n",
       "      <td>2</td>\n",
       "      <td>3917</td>\n",
       "      <td>52</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25. Miss Glam Sudirman.xlsx</td>\n",
       "      <td>SUDIRMAN</td>\n",
       "      <td>44</td>\n",
       "      <td>5622</td>\n",
       "      <td>4</td>\n",
       "      <td>5520</td>\n",
       "      <td>98</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26. Miss Glam Dr. Mansyur.xlsx</td>\n",
       "      <td>DR. MANSYUR</td>\n",
       "      <td>25</td>\n",
       "      <td>4910</td>\n",
       "      <td>14</td>\n",
       "      <td>4809</td>\n",
       "      <td>87</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27. Miss Glam P. Sidimpuan.xlsx</td>\n",
       "      <td>P. SIDIMPUAN</td>\n",
       "      <td>31</td>\n",
       "      <td>3760</td>\n",
       "      <td>5</td>\n",
       "      <td>3718</td>\n",
       "      <td>37</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28. Miss Glam Aceh.xlsx</td>\n",
       "      <td>ACEH</td>\n",
       "      <td>15</td>\n",
       "      <td>3527</td>\n",
       "      <td>14</td>\n",
       "      <td>3468</td>\n",
       "      <td>45</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29. Miss Glam Marpoyan.xlsx</td>\n",
       "      <td>MARPOYAN</td>\n",
       "      <td>30</td>\n",
       "      <td>4646</td>\n",
       "      <td>19</td>\n",
       "      <td>4560</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3. Miss Glam Jambi.xlsx</td>\n",
       "      <td>JAMBI</td>\n",
       "      <td>33</td>\n",
       "      <td>5229</td>\n",
       "      <td>4</td>\n",
       "      <td>5137</td>\n",
       "      <td>88</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30. Miss Glam Sei Penuh.xlsx</td>\n",
       "      <td>SEI PENUH</td>\n",
       "      <td>21</td>\n",
       "      <td>3591</td>\n",
       "      <td>28</td>\n",
       "      <td>3534</td>\n",
       "      <td>29</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31. Miss Glam Mayang.xlsx</td>\n",
       "      <td>MAYANG</td>\n",
       "      <td>18</td>\n",
       "      <td>4256</td>\n",
       "      <td>31</td>\n",
       "      <td>4179</td>\n",
       "      <td>46</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32.  Miss Glam Soeta.xlsx</td>\n",
       "      <td>SOETA</td>\n",
       "      <td>100</td>\n",
       "      <td>4527</td>\n",
       "      <td>124</td>\n",
       "      <td>4362</td>\n",
       "      <td>41</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33. Miss Glam Balikpapan.xlsx</td>\n",
       "      <td>BALIKPAPAN</td>\n",
       "      <td>100</td>\n",
       "      <td>4588</td>\n",
       "      <td>85</td>\n",
       "      <td>4440</td>\n",
       "      <td>63</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4. Miss Glam Bukittinggi.xlsx</td>\n",
       "      <td>BUKITTINGGI</td>\n",
       "      <td>45</td>\n",
       "      <td>5286</td>\n",
       "      <td>1</td>\n",
       "      <td>5220</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5. Miss Glam Panam.xlsx</td>\n",
       "      <td>PANAM</td>\n",
       "      <td>46</td>\n",
       "      <td>5258</td>\n",
       "      <td>0</td>\n",
       "      <td>5164</td>\n",
       "      <td>94</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6. Miss Glam Muaro Bungo.xlsx</td>\n",
       "      <td>MUARO BUNGO</td>\n",
       "      <td>42</td>\n",
       "      <td>4913</td>\n",
       "      <td>7</td>\n",
       "      <td>4839</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7. Miss Glam Lampung.xlsx</td>\n",
       "      <td>LAMPUNG</td>\n",
       "      <td>18</td>\n",
       "      <td>4437</td>\n",
       "      <td>16</td>\n",
       "      <td>4357</td>\n",
       "      <td>64</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8. Miss Glam Bengkulu.xlsx</td>\n",
       "      <td>BENGKULU</td>\n",
       "      <td>14</td>\n",
       "      <td>3941</td>\n",
       "      <td>12</td>\n",
       "      <td>3856</td>\n",
       "      <td>73</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9. Miss Glam Medan.xlsx</td>\n",
       "      <td>MEDAN</td>\n",
       "      <td>46</td>\n",
       "      <td>5734</td>\n",
       "      <td>15</td>\n",
       "      <td>5610</td>\n",
       "      <td>109</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file        location contribution_pct  \\\n",
       "0            1. Miss Glam Padang.xlsx          PADANG              100   \n",
       "1        10. Miss Glam Palembang.xlsx       PALEMBANG               26   \n",
       "2            11. Miss Glam Damar.xlsx           DAMAR               91   \n",
       "3           12. Miss Glam Bangka.xlsx          BANGKA               28   \n",
       "4       13. Miss Glam Payakumbuh.xlsx      PAYAKUMBUH               47   \n",
       "5            14. Miss Glam Solok.xlsx           SOLOK               37   \n",
       "6       15. Miss Glam Tembilahan.xlsx      TEMBILAHAN               27   \n",
       "7    16. Miss Glam Lubuk Linggau.xlsx   LUBUK LINGGAU               26   \n",
       "8            17. Miss Glam Dumai.xlsx           DUMAI               36   \n",
       "9          18. Miss Glam Kedaton.xlsx         KEDATON               18   \n",
       "10   19. Miss Glam Rantau Prapat.xlsx   RANTAU PRAPAT               27   \n",
       "11        2. Miss Glam Pekanbaru.xlsx       PEKANBARU               60   \n",
       "12  20. Miss Glam Tanjung Pinang.xlsx  TANJUNG PINANG               19   \n",
       "13          21. Miss Glam Sutomo.xlsx          SUTOMO               49   \n",
       "14   22. Miss Glam Pasaman Barat.xlsx   PASAMAN BARAT               17   \n",
       "15           23. Miss Glam Halat.xlsx           HALAT               31   \n",
       "16            24. Miss Glam Duri.xlsx            DURI               28   \n",
       "17        25. Miss Glam Sudirman.xlsx        SUDIRMAN               44   \n",
       "18     26. Miss Glam Dr. Mansyur.xlsx     DR. MANSYUR               25   \n",
       "19    27. Miss Glam P. Sidimpuan.xlsx    P. SIDIMPUAN               31   \n",
       "20            28. Miss Glam Aceh.xlsx            ACEH               15   \n",
       "21        29. Miss Glam Marpoyan.xlsx        MARPOYAN               30   \n",
       "22            3. Miss Glam Jambi.xlsx           JAMBI               33   \n",
       "23       30. Miss Glam Sei Penuh.xlsx       SEI PENUH               21   \n",
       "24          31. Miss Glam Mayang.xlsx          MAYANG               18   \n",
       "25          32.  Miss Glam Soeta.xlsx           SOETA              100   \n",
       "26      33. Miss Glam Balikpapan.xlsx      BALIKPAPAN              100   \n",
       "27      4. Miss Glam Bukittinggi.xlsx     BUKITTINGGI               45   \n",
       "28            5. Miss Glam Panam.xlsx           PANAM               46   \n",
       "29      6. Miss Glam Muaro Bungo.xlsx     MUARO BUNGO               42   \n",
       "30          7. Miss Glam Lampung.xlsx         LAMPUNG               18   \n",
       "31         8. Miss Glam Bengkulu.xlsx        BENGKULU               14   \n",
       "32            9. Miss Glam Medan.xlsx           MEDAN               46   \n",
       "\n",
       "    total_rows  padang_suppliers  other_suppliers  no_supplier   status  \\\n",
       "0         6741              6606               17          118  Success   \n",
       "1         4848                10             4770           68  Success   \n",
       "2         6635                 0             6508          127  Success   \n",
       "3         4529                20             4430           79  Success   \n",
       "4         5297                 0             5217           80  Success   \n",
       "5         4649                 0             4591           58  Success   \n",
       "6         4325                 9             4251           65  Success   \n",
       "7         4399                 7             4332           60  Success   \n",
       "8         4754                 0             4689           65  Success   \n",
       "9         4210                13             4132           65  Success   \n",
       "10        4256                17             4170           69  Success   \n",
       "11        5935                 4             5824          107  Success   \n",
       "12        4021                 8             3941           72  Success   \n",
       "13        5728                 4             5647           77  Success   \n",
       "14        3819                 6             3771           42  Success   \n",
       "15        4778                15             4675           88  Success   \n",
       "16        3971                 2             3917           52  Success   \n",
       "17        5622                 4             5520           98  Success   \n",
       "18        4910                14             4809           87  Success   \n",
       "19        3760                 5             3718           37  Success   \n",
       "20        3527                14             3468           45  Success   \n",
       "21        4646                19             4560           67  Success   \n",
       "22        5229                 4             5137           88  Success   \n",
       "23        3591                28             3534           29  Success   \n",
       "24        4256                31             4179           46  Success   \n",
       "25        4527               124             4362           41  Success   \n",
       "26        4588                85             4440           63  Success   \n",
       "27        5286                 1             5220           65  Success   \n",
       "28        5258                 0             5164           94  Success   \n",
       "29        4913                 7             4839           67  Success   \n",
       "30        4437                16             4357           64  Success   \n",
       "31        3941                12             3856           73  Success   \n",
       "32        5734                15             5610          109  Success   \n",
       "\n",
       "                                          output_path  \n",
       "0   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "1   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "2   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "3   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "4   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "5   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "6   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "7   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "8   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "9   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "10  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "11  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "12  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "13  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "14  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "15  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "16  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "17  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "18  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "19  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "20  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "21  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "22  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "23  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "24  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "25  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "26  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "27  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "28  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "29  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "30  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "31  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "32  /Users/andresuchitra/dev/missglam/autopo/noteb...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the last processed file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama</th>\n",
       "      <th>Toko</th>\n",
       "      <th>Stok</th>\n",
       "      <th>Daily Sales</th>\n",
       "      <th>Max. Daily Sales</th>\n",
       "      <th>Lead Time</th>\n",
       "      <th>Max. Lead Time</th>\n",
       "      <th>Min. Order</th>\n",
       "      <th>...</th>\n",
       "      <th>Nama Supplier</th>\n",
       "      <th>ID Brand</th>\n",
       "      <th>Nama Brand</th>\n",
       "      <th>ID Store</th>\n",
       "      <th>Nama Store</th>\n",
       "      <th>Hari Order</th>\n",
       "      <th>Min. Purchase</th>\n",
       "      <th>Trading Term</th>\n",
       "      <th>Promo Factor</th>\n",
       "      <th>Delay Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>10400614911</td>\n",
       "      <td>ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>10400517459</td>\n",
       "      <td>ACNAWAY Mugwort Daily Sunscreen Only For Acne ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>101001107647</td>\n",
       "      <td>ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACNES</td>\n",
       "      <td>8992821102372</td>\n",
       "      <td>ACNES Complete White Face Wash 100gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. MENSA BINASUKSES - PPN (MDN)</td>\n",
       "      <td>33.00</td>\n",
       "      <td>ACNES</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>3.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACNES</td>\n",
       "      <td>8992821102365</td>\n",
       "      <td>ACNES Complete White Face Wash 50gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.30</td>\n",
       "      <td>5.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. MENSA BINASUKSES - PPN (MDN)</td>\n",
       "      <td>33.00</td>\n",
       "      <td>ACNES</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>3.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>YOUVIT</td>\n",
       "      <td>60100406456</td>\n",
       "      <td>YOUVIT Ezzleep 7 Gummies 28gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. ANUGERAH ARGON MEDICA - PPN (PKU)</td>\n",
       "      <td>1019.00</td>\n",
       "      <td>YOUVIT</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>4.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5730</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402932</td>\n",
       "      <td>YU CHUN Mei Cordyceps Brightening Cleanser 100ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5731</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402703</td>\n",
       "      <td>YU CHUN Mei Cordyceps Lightening Day Cream 30gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5732</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402710</td>\n",
       "      <td>YU CHUN Mei Cordyceps Lightening Night Cream 30g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402918</td>\n",
       "      <td>YU CHUN Mei Serum Whitening Essence 30ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5734 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand            SKU  \\\n",
       "0     ACNAWAY    10400614911   \n",
       "1     ACNAWAY    10400517459   \n",
       "2     ACNAWAY   101001107647   \n",
       "3       ACNES  8992821102372   \n",
       "4       ACNES  8992821102365   \n",
       "...       ...            ...   \n",
       "5729   YOUVIT    60100406456   \n",
       "5730  YU CHUN  8997014402932   \n",
       "5731  YU CHUN  8997014402703   \n",
       "5732  YU CHUN  8997014402710   \n",
       "5733  YU CHUN  8997014402918   \n",
       "\n",
       "                                                   Nama             Toko  \\\n",
       "0     ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...  Miss Glam Medan   \n",
       "1     ACNAWAY Mugwort Daily Sunscreen Only For Acne ...  Miss Glam Medan   \n",
       "2     ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...  Miss Glam Medan   \n",
       "3                  ACNES Complete White Face Wash 100gr  Miss Glam Medan   \n",
       "4                   ACNES Complete White Face Wash 50gr  Miss Glam Medan   \n",
       "...                                                 ...              ...   \n",
       "5729                      YOUVIT Ezzleep 7 Gummies 28gr  Miss Glam Medan   \n",
       "5730   YU CHUN Mei Cordyceps Brightening Cleanser 100ml  Miss Glam Medan   \n",
       "5731    YU CHUN Mei Cordyceps Lightening Day Cream 30gr  Miss Glam Medan   \n",
       "5732   YU CHUN Mei Cordyceps Lightening Night Cream 30g  Miss Glam Medan   \n",
       "5733           YU CHUN Mei Serum Whitening Essence 30ml  Miss Glam Medan   \n",
       "\n",
       "      Stok  Daily Sales  Max. Daily Sales  Lead Time  Max. Lead Time  \\\n",
       "0     6.00         0.03              1.00       5.00           28.00   \n",
       "1    12.00         0.27              3.00       5.00           28.00   \n",
       "2     0.00         0.35              1.84       5.00           28.00   \n",
       "3     8.00         0.39              1.84       5.00           12.00   \n",
       "4     6.00         0.61              2.30       5.00           12.00   \n",
       "...    ...          ...               ...        ...             ...   \n",
       "5729  0.00         0.03              1.00       1.00            2.00   \n",
       "5730  2.00         0.02              0.46       1.00            2.00   \n",
       "5731  4.00         0.06              0.46       1.00            2.00   \n",
       "5732  0.00         0.04              0.46       1.00            2.00   \n",
       "5733  5.00         0.02              0.46       1.00            2.00   \n",
       "\n",
       "      Min. Order  ...                                   Nama Supplier  \\\n",
       "0           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "1           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "2           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "3           3.00  ...                PT. MENSA BINASUKSES - PPN (MDN)   \n",
       "4           3.00  ...                PT. MENSA BINASUKSES - PPN (MDN)   \n",
       "...          ...  ...                                             ...   \n",
       "5729        1.00  ...           PT. ANUGERAH ARGON MEDICA - PPN (PKU)   \n",
       "5730        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "5731        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "5732        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "5733        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "\n",
       "      ID Brand  Nama Brand  ID Store           Nama Store  Hari Order  \\\n",
       "0      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "1      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "2      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "3        33.00       ACNES     19.00      Miss Glam Medan        3.00   \n",
       "4        33.00       ACNES     19.00      Miss Glam Medan        3.00   \n",
       "...        ...         ...       ...                  ...         ...   \n",
       "5729   1019.00      YOUVIT      8.00  Miss Glam Pekanbaru        4.00   \n",
       "5730    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "5731    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "5732    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "5733    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "\n",
       "      Min. Purchase  Trading Term  Promo Factor  Delay Factor  \n",
       "0         500000.00          0.00                              \n",
       "1         500000.00          0.00                              \n",
       "2         500000.00          0.00                              \n",
       "3         500000.00          0.00                              \n",
       "4         500000.00          0.00                              \n",
       "...             ...           ...           ...           ...  \n",
       "5729      500000.00          0.00                              \n",
       "5730      500000.00          0.00                              \n",
       "5731      500000.00          0.00                              \n",
       "5732      500000.00          0.00                              \n",
       "5733      500000.00          0.00                              \n",
       "\n",
       "[5734 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import display\n",
    "from locale import atof\n",
    "import numpy as np\n",
    "from openpyxl.styles import numbers\n",
    "\n",
    "_patch_openpyxl_number_casting()\n",
    "\n",
    "# Apply the formatting to numeric columns in your final output\n",
    "def format_dataframe_display(df):\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # Apply formatting to numeric columns\n",
    "    for col in df_display.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: format_id_number(x, 2) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/Users/andresuchitra/dev/missglam/autopo/notebook')\n",
    "SUPPLIER_PATH = BASE_DIR / 'data/supplier.csv'\n",
    "RAWPO_DIR = BASE_DIR / 'data/rawpo/csv'\n",
    "RAWPO_XLSX_DIR = BASE_DIR / 'data/rawpo/xlsx'\n",
    "STORE_CONTRIBUTION_PATH = BASE_DIR / 'data/store_contribution.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'output/complete'\n",
    "OUTPUT_EXCEL_DIR = BASE_DIR / 'output/excel'\n",
    "OUTPUT_M2_DIR = BASE_DIR / 'output/m2'\n",
    "OUTPUT_EMERGENCY_DIR = BASE_DIR / 'output/emergency'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EXCEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_M2_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EMERGENCY_DIR, exist_ok=True)\n",
    "\n",
    "df_special_60 = load_special_sku_60(BASE_DIR / 'data/special_sku_60.csv')\n",
    "\n",
    "display(df_special_60)\n",
    "\n",
    "def load_store_contribution(store_contribution_path):\n",
    "    \"\"\"Load and prepare store contribution data.\"\"\"\n",
    "    store_contrib = pd.read_csv(store_contribution_path, header=None, \n",
    "                              names=['store', 'contribution_pct'])\n",
    "    # Convert store names to lowercase for case-insensitive matching\n",
    "    store_contrib['store_lower'] = store_contrib['store'].str.lower()\n",
    "    return store_contrib\n",
    "\n",
    "def get_contribution_pct(location, store_contrib):\n",
    "    \"\"\"Get contribution percentage for a given location.\"\"\"\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    contrib_row = store_contrib[store_contrib['store_lower'] == location_lower]\n",
    "    if not contrib_row.empty:\n",
    "        return contrib_row['contribution_pct'].values[0]\n",
    "    print(f\"Warning: No contribution percentage found for {location}\")\n",
    "\n",
    "    return 100  # Default to 100% if not found\n",
    "\n",
    "def load_supplier_data(supplier_path):\n",
    "    \"\"\"Load and clean supplier data.\"\"\"\n",
    "    print(f\"Loading supplier data: {supplier_path}\")\n",
    "    df = pd.read_csv(supplier_path, sep=';', decimal=',').fillna('')\n",
    "    df['Nama Brand'] = df['Nama Brand'].str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier information.\"\"\"\n",
    "    print(\"Merging with suppliers...\")\n",
    "    \n",
    "    # Clean supplier data\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    \n",
    "    # Deduplicate to prevent row explosion - Unique Brand+Store\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "    \n",
    "    # Ensure PO data has clean columns for merging\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "    \n",
    "    # 1. Primary Merge: Match on Brand AND Store (Toko)\n",
    "    # This prioritizes the specific supplier for that store\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "    \n",
    "    # 2. Fallback: For unmatched rows, try to find ANY supplier for that Brand\n",
    "    # Identify rows where merge failed (Nama Brand is NaN)\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    \n",
    "    if unmatched_mask.any():\n",
    "        print(f\"Found {unmatched_mask.sum()} rows without direct store match. Attempting fallback...\")\n",
    "        \n",
    "        # Get the unmatched rows and drop the empty supplier columns\n",
    "        unmatched_rows = merged_df[unmatched_mask].copy()\n",
    "        supplier_cols = [col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand']\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols)\n",
    "        \n",
    "        # Create fallback supplier list (one per brand)\n",
    "        # We take the first one found for each brand\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        \n",
    "        # Merge unmatched rows with fallback suppliers\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "        \n",
    "        # Combine the initially matched rows with the fallback-matched rows\n",
    "        matched_initial = merged_df[~unmatched_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "    \n",
    "    # Clean up supplier columns\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def calculate_inventory_metrics(df_clean, df_special_60):\n",
    "    \"\"\"\n",
    "    Calculate various inventory metrics including safety stock, reorder points, and PO quantities.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Input dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added calculated columns\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "    df = df_clean.copy()\n",
    "    \n",
    "    # Set display options\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "    # Normalise stock column name\n",
    "    stock_col = 'Stok' if 'Stok' in df.columns else 'Stock'\n",
    "\n",
    "    # Force the columns we need into numeric form\n",
    "    numeric_cols = [\n",
    "        stock_col, 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "        'Max. Lead Time', 'Sedang PO', 'HPP', 'Harga', 'sales_contribution'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    try:\n",
    "        # 1. Safety stock calculation\n",
    "        df['Safety stock'] = (df['Max. Daily Sales'] * df['Max. Lead Time']) - (df['Daily Sales'] * df['Lead Time'])\n",
    "        df['Safety stock'] = df['Safety stock'].apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 2. Reorder point calculation\n",
    "        df['Reorder point'] = np.ceil((df['Daily Sales'] * df['Lead Time']) + df['Safety stock']).fillna(0).astype(int)\n",
    "        \n",
    "        # 3. Stock cover for 30 or 60 days based on special SKUs\n",
    "        # Default to 30 days for all SKUs\n",
    "        df['target_days'] = 30\n",
    "        \n",
    "        # 4. Check if we have special SKUs and update their target days to 60\n",
    "        if df_special_60 is not None and not df_special_60.empty:\n",
    "            # Find the SKU column in the main dataframe (case-insensitive)\n",
    "            sku_col = next((col for col in df.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "            # Find the SKU column in the special SKU dataframe (case-insensitive)\n",
    "            special_sku_col = next((col for col in df_special_60.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "            if sku_col and special_sku_col:\n",
    "                # Convert both to string and strip whitespace for matching\n",
    "                df[sku_col] = df[sku_col].astype(str).str.strip()\n",
    "                df_special_60[special_sku_col] = df_special_60[special_sku_col].astype(str).str.strip()\n",
    "                \n",
    "                # Update target_days to 60 for special SKUs\n",
    "                special_skus = set(df_special_60[special_sku_col].unique())\n",
    "                df.loc[df[sku_col].isin(special_skus), 'target_days'] = 60\n",
    "            else:\n",
    "                print(\"Warning: Could not find 'SKU' column in one of the dataframes\")\n",
    "        \n",
    "        # Calculate target days cover based on the determined days\n",
    "        df['target_days_cover'] = (df['Daily Sales'] * df['target_days']).apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        df['current_stock_days_cover'] = np.where(\n",
    "            df['Daily Sales'] > 0,\n",
    "            df[stock_col] / df['Daily Sales'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # 5. Is open PO flag\n",
    "        df['is_open_po'] = np.where(\n",
    "            (df['current_stock_days_cover'] < df['target_days']) & \n",
    "            (df['Stok'] <= df['Reorder point']), 1, 0\n",
    "        )\n",
    "        \n",
    "        # 6. Initial PO quantity\n",
    "        df['initial_qty_po'] = df['target_days_cover'] - df[stock_col] - df.get('Sedang PO', 0)\n",
    "        df['initial_qty_po'] = (\n",
    "            pd.Series(\n",
    "                np.where(df['is_open_po'] == 1, df['initial_qty_po'], 0),\n",
    "                index=df.index\n",
    "            )\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 7. Emergency PO quantity\n",
    "        df['emergency_po_qty'] = np.where(\n",
    "            df.get('Sedang PO', 0) > 0,\n",
    "            np.maximum(0, (df['Max. Lead Time'] - df['current_stock_days_cover']) * df['Daily Sales']),\n",
    "            np.ceil((df['Max. Lead Time'] - df['current_stock_days_cover']) * df['Daily Sales'])\n",
    "        )\n",
    "        \n",
    "        # Clean up emergency PO quantities\n",
    "        df['emergency_po_qty'] = (\n",
    "            df['emergency_po_qty']\n",
    "            .replace([np.inf, -np.inf], 0)\n",
    "            .fillna(0)\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 8. Updated regular PO quantity\n",
    "        df['updated_regular_po_qty'] = (df['initial_qty_po'] - df['emergency_po_qty']).clip(lower=0).astype(int)\n",
    "        \n",
    "        # 9. Final updated regular PO quantity (enforce minimum order)\n",
    "        df['final_updated_regular_po_qty'] = np.where(\n",
    "            (df['updated_regular_po_qty'] > 0) & \n",
    "            (df['updated_regular_po_qty'] < df['Min. Order']),\n",
    "            df['Min. Order'],\n",
    "            df['updated_regular_po_qty']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 10. Calculate costs if by multiplying with contribution percentage\n",
    "        df['emergency_po_cost'] = (df['emergency_po_qty'] * df['HPP']).round(2)\n",
    "        df['final_updated_regular_po_cost'] = (df['final_updated_regular_po_qty'] * df['HPP']).round(2)\n",
    "        \n",
    "        # Clean up any remaining NaN or infinite values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_inventory_metrics: {str(e)}\")\n",
    "        return df_clean\n",
    "\n",
    "def clean_po_data(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP', 'Harga'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP', 'Harga']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP', 'Harga'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert SKU to string and clean it\n",
    "        if 'SKU' in df.columns:\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP', 'Min. Order', 'Harga'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "        df['contribution_ratio'] = contribution_pct / 100\n",
    "\n",
    "\n",
    "        location_upper = location.upper()\n",
    "        exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        print(f\"Processing store: {location} - {contribution_pct}%\")\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        if padang_sales is not None:\n",
    "            # Ensure padang_sales has the required columns\n",
    "            padang_sales = padang_sales.copy()\n",
    "            padang_sales.columns = padang_sales.columns.str.strip()\n",
    "            \n",
    "            # Convert SKU to string in both dataframes\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "            padang_sales['SKU'] = padang_sales['SKU'].astype(str).str.strip()\n",
    "            \n",
    "            padang_skus = set(padang_sales['SKU'].unique())\n",
    "            df['Is in Padang'] = df['SKU'].isin(padang_skus).astype(int)\n",
    "        else:\n",
    "            print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "            df['Is in Padang'] = 0\n",
    "\n",
    "        if not needs_padang_override:\n",
    "            return df\n",
    "\n",
    "        if padang_sales is None:\n",
    "            raise ValueError(\n",
    "                \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "                \"or any store with contribution < 100%.\"\n",
    "            )\n",
    "\n",
    "        # Process Padang sales data\n",
    "        padang_df = padang_sales.copy()\n",
    "        padang_df.columns = padang_df.columns.str.strip()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        missing_cols = [col for col in required_cols if col not in padang_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in Padang sales data: {missing_cols}\")\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        if 'Daily Sales' in df.columns:\n",
    "            df['Orig Daily Sales'] = df['Daily Sales']\n",
    "        if 'Max. Daily Sales' in df.columns:\n",
    "            df['Orig Max. Daily Sales'] = df['Max. Daily Sales']\n",
    "\n",
    "        print(\"Overriding with Padang sales data...\")\n",
    "        \n",
    "        # Ensure SKU is string in both dataframes before merge\n",
    "        df['SKU'] = df['SKU'].astype(str)\n",
    "        padang_df['SKU'] = padang_df['SKU'].astype(str)\n",
    "        \n",
    "        # Merge with Padang's sales data\n",
    "        df = df.merge(\n",
    "            padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "                'Daily Sales': 'Padang Daily Sales',\n",
    "                'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "            }),\n",
    "            on='SKU',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "            df['Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Daily Sales']\n",
    "            )\n",
    "            \n",
    "        if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "            df['Max. Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Max Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Max. Daily Sales']\n",
    "            )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        columns_to_drop = [\n",
    "            'Padang Daily Sales', 'Padang Max Daily Sales',\n",
    "        ]\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        # remove duplicate SKU\n",
    "        df = df.drop_duplicates(subset=['SKU'], keep='first')\n",
    "\n",
    "        # calculate sales contribution\n",
    "        df['sales_contribution'] = df['Daily Sales'] * df['Harga']\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_store_name_from_filename(filename):\n",
    "    \"\"\"Extract store name from filename, handling different patterns.\"\"\"\n",
    "    # Remove file extension and split by spaces\n",
    "    name_parts = Path(filename).stem.split()\n",
    "    \n",
    "    # Handle cases like \"002 Miss Glam Pekanbaru.csv\" -> \"Pekanbaru\"\n",
    "    # or \"01 Miss Glam Padang.csv\" -> \"Padang\"\n",
    "    if len(name_parts) >= 3 and name_parts[1].lower() == 'miss' and name_parts[2].lower() == 'glam':\n",
    "        return ' '.join(name_parts[3:]).strip().upper()\n",
    "    elif len(name_parts) >= 2 and name_parts[0].lower() == 'miss' and name_parts[1].lower() == 'glam':\n",
    "        return ' '.join(name_parts[2:]).strip().upper()\n",
    "    # Fallback: take everything after the first space\n",
    "    elif ' ' in filename:\n",
    "        return ' '.join(name_parts[1:]).strip().upper()\n",
    "    return name_parts[0].upper()\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # List of (separator, encoding) combinations to try\n",
    "    formats_to_try = [\n",
    "        (',', 'utf-8'),      # Standard CSV with comma\n",
    "        (';', 'utf-8'),      # Semicolon with UTF-8\n",
    "        (',', 'latin1'),     # Comma with Latin1\n",
    "        (';', 'latin1'),     # Semicolon with Latin1\n",
    "        (',', 'cp1252'),     # Windows-1252 encoding\n",
    "        (';', 'cp1252')\n",
    "    ]\n",
    "    \n",
    "    for sep, enc in formats_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=sep,\n",
    "                decimal=',',\n",
    "                thousands='.',\n",
    "                encoding=enc,\n",
    "                engine='python'  # More consistent behavior with Python engine\n",
    "            )\n",
    "            # If we get here, the file was read successfully\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError, pd.errors.EmptyDataError) as e:\n",
    "            continue  # Try next format\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading {file_path} with sep='{sep}', encoding='{enc}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we get here, all attempts failed\n",
    "    print(f\"Failed to read {file_path} with any known format\")\n",
    "    return None\n",
    "\n",
    "def process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=False):\n",
    "    \"\"\"Process a single PO file and return merged data and summary.\"\"\"\n",
    "    print(f\"\\nProcessing PO file: {file_path.name} ....\")\n",
    "    \n",
    "    try:\n",
    "        # Extract location from filename using the new function\n",
    "        location = get_store_name_from_filename(file_path.name)\n",
    "        print(f\"  - Extracted location: {location}\")  # Debug print\n",
    "        \n",
    "        contribution_pct = get_contribution_pct(location, store_contrib)\n",
    "        \n",
    "        # Read the CSV with error handling\n",
    "        try:\n",
    "            # Try reading with different encodings if needed\n",
    "            if is_excel_folder:\n",
    "                df = read_excel_file(file_path)\n",
    "            else:\n",
    "                df = read_csv_file(file_path)\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "                \n",
    "            # Clean the data\n",
    "            df_clean = clean_po_data(df,location, contribution_pct, df_padang)\n",
    "\n",
    "            # update sku \n",
    "            \n",
    "            # Skip if cleaning failed\n",
    "            if df_clean.empty:\n",
    "                raise ValueError(\"Data cleaning failed\")\n",
    "        \n",
    "            # calculate metrics PO\n",
    "            df_clean = calculate_inventory_metrics(df_clean, df_special_60)\n",
    "            \n",
    "            # Merge with suppliers\n",
    "            merged_df = merge_with_suppliers(df_clean, supplier_df)\n",
    "\n",
    "            # Generate summary\n",
    "            padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "            other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                                  (merged_df['Nama Store'] != '')).sum()\n",
    "            \n",
    "            summary = {\n",
    "                'file': file_path.name,\n",
    "                'location': location,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'total_rows': len(merged_df),\n",
    "                'padang_suppliers': int(padang_count),\n",
    "                'other_suppliers': int(other_supplier_count),\n",
    "                'no_supplier': int((merged_df['Nama Store'] == '').sum()),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            return merged_df, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file data: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {file_path.name}: {str(e)}\"\n",
    "        print(f\"  - {error_msg}\")\n",
    "        return None, {\n",
    "            'file': file_path.name,\n",
    "            'location': location if 'location' in locals() else 'Unknown',\n",
    "            'contribution_pct': contribution_pct if 'contribution_pct' in locals() else 0,\n",
    "            'total_rows': 0,\n",
    "            'padang_suppliers': 0,\n",
    "            'other_suppliers': 0,\n",
    "            'no_supplier': 0,\n",
    "            'status': f\"Error: {str(e)[:100]}\"  # Truncate long error messages\n",
    "        }\n",
    "\n",
    "def load_padang_data(padang_path):\n",
    "    \"\"\"Load Padang data from either CSV or Excel file.\n",
    "    \n",
    "    Args:\n",
    "        padang_path: Path to the input file (CSV or XLSX)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and cleaned Padang data\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported or file cannot be read\n",
    "    \"\"\"\n",
    "    print(f\"Loading Padang data from {padang_path}...\")\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = str(padang_path).lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(padang_path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(padang_path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Padang data with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Padang data from {padang_path}: {str(e)}\")\n",
    "\n",
    "def format_number_for_csv(x):\n",
    "    \"\"\"Format numbers for CSV output with Indonesian locale (comma as decimal, dot as thousand)\"\"\"\n",
    "    if pd.isna(x) or x == '':\n",
    "        return x\n",
    "    try:\n",
    "        if isinstance(x, (int, float)):\n",
    "            if x == int(x):  # Whole number\n",
    "                return f\"{int(x):,d}\".replace(\",\", \".\")\n",
    "            else:  # Decimal number\n",
    "                return f\"{x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def clean_and_convert(df):\n",
    "    \"\"\"Clean and convert DataFrame columns to appropriate types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string first to handle NaN/None consistently\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Define NA values that should be treated as empty/missing\n",
    "    na_values = list(NA_VALUES)\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        # Replace NA values with empty string first (treating them as literals, not regex)\n",
    "        df[col] = df[col].replace(na_values, '', regex=False)\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if df[col].empty:\n",
    "            continue\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if col in NUMERIC_COLUMNS:\n",
    "            # Convert to numeric, coercing errors to NaN, then fill with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            # For non-numeric columns, ensure they're strings and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace empty strings with NaN and then fill with empty string\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('').infer_objects(copy=False)\n",
    "            df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            df[col] = df[col].infer_objects(copy=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file with robust error handling for problematic values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nReading excel file: {file_path.name}...\")\n",
    "        \n",
    "        # First, read the file with openpyxl directly to handle the data more carefully\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = load_workbook(\n",
    "            filename=file_path,\n",
    "            read_only=True,    # Read-only mode is faster and uses less memory\n",
    "            data_only=True,    # Get the stored value instead of the formula\n",
    "            keep_links=False   # Don't load external links\n",
    "        )\n",
    "        \n",
    "        # Get the first sheet\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Get headers from the first row\n",
    "        headers = []\n",
    "        for idx, cell in enumerate(next(ws.iter_rows(values_only=True))):\n",
    "            header = str(cell).strip() if cell not in (None, '') else f\"Column_{idx + 1}\"\n",
    "            headers.append(header)\n",
    "        \n",
    "        # Initialize data rows\n",
    "        data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for row in ws.iter_rows(min_row=2, values_only=True):  # Skip header row\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                if cell is None:\n",
    "                    row_data.append('')\n",
    "                    continue\n",
    "\n",
    "                cell_str = str(cell).strip()\n",
    "                if cell_str.upper() in NA_VALUES:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    row_data.append(cell_str)\n",
    "            \n",
    "            # Only add row if it has data\n",
    "            if any(cell != '' for cell in row_data):\n",
    "                data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Normalize column data types\n",
    "        df = clean_and_convert(df)\n",
    "        \n",
    "        print(f\"✅ Successfully processed {file_path.name} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path.name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_file(df, file_path, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame to file with consistent extension and content type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path object or string for the output file\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments to pass to to_csv or to_excel\n",
    "        \n",
    "    Returns:\n",
    "        Path: The path where the file was saved\n",
    "    \"\"\"\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the correct file extension\n",
    "    if not file_path.suffix.lower() == f'.{file_format}':\n",
    "        file_path = file_path.with_suffix(f'.{file_format}')\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_output = df.copy()\n",
    "    \n",
    "    # Common preprocessing\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].astype(str).str.strip()\n",
    "        if file_format == 'xlsx':\n",
    "            # For Excel, wrap SKU in =\"...\" to preserve leading zeros\n",
    "            df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Format numbers for CSV if needed\n",
    "    if file_format == 'csv':\n",
    "        numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "    \n",
    "    # Save based on format\n",
    "    if file_format == 'csv':\n",
    "        df_output.to_csv(\n",
    "            file_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig',\n",
    "            **kwargs\n",
    "        )\n",
    "    elif file_format == 'xlsx':\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "            df_output.to_excel(writer, index=False, **kwargs)\n",
    "            \n",
    "            # Format SKU column as text in Excel\n",
    "            if 'SKU' in df_output.columns:\n",
    "                ws = writer.sheets[list(writer.sheets.keys())[0]]\n",
    "                sku_col_idx = df_output.columns.get_loc(\"SKU\") + 1\n",
    "                for row in ws.iter_rows(\n",
    "                    min_row=2,  # Skip header\n",
    "                    max_row=ws.max_row,\n",
    "                    min_col=sku_col_idx,\n",
    "                    max_col=sku_col_idx\n",
    "                ):\n",
    "                    for cell in row:\n",
    "                        cell.number_format = numbers.FORMAT_TEXT\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "    \n",
    "    print(f\"File saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_to_complete_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save Complete format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "\n",
    "    return save_file(df, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_m2_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save M2 format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_filtered = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[['Toko', 'SKU', 'HPP', 'final_updated_regular_po_qty']]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_M2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_M2_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_emergency_po_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save emergency PO format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with emergency PO qty > 0\n",
    "    df_filtered = df[df['emergency_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[[\n",
    "        'Brand', 'SKU', 'Nama', 'Toko', 'HPP', \n",
    "        'emergency_po_qty', 'emergency_po_cost'\n",
    "    ]]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_EMERGENCY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_EMERGENCY_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    supplier_df = load_supplier_data(SUPPLIER_PATH)\n",
    "    store_contrib = load_store_contribution(STORE_CONTRIBUTION_PATH)\n",
    "    all_summaries = []\n",
    "\n",
    "    # get padang df first\n",
    "    df_padang = load_padang_data(BASE_DIR / 'data/rawpo/xlsx/1. Miss glam Padang.xlsx')\n",
    "\n",
    "    # test_xlsx_convert()\n",
    "\n",
    "    # Process each PO file\n",
    "    for file_path in sorted(RAWPO_XLSX_DIR.glob('*.xlsx')):\n",
    "        try:\n",
    "            merged_df, summary = process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=True)\n",
    "\n",
    "            save_to_complete_format(merged_df, file_path.name, file_format='xlsx')\n",
    "            save_to_complete_format(merged_df, file_path.name)\n",
    "            save_to_m2_format(merged_df, file_path.name)\n",
    "            save_to_emergency_po_format(merged_df, file_path.name)\n",
    "\n",
    "            # summary['output_path'] = str(output_path)\n",
    "            output_path = OUTPUT_DIR / file_path.name\n",
    "            summary['output_path'] = str(output_path)\n",
    "\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  - Location: {summary['location']}\")\n",
    "            print(f\"  - Contribution: {summary['contribution_pct']}%\")\n",
    "            print(f\"  - Rows processed: {summary['total_rows']}\")\n",
    "            print(f\"  - 'Miss Glam Padang' suppliers: {summary['padang_suppliers']} rows\")\n",
    "            print(f\"  - Other suppliers: {summary['other_suppliers']} rows\")\n",
    "            print(f\"  - No supplier data: {summary['no_supplier']} rows\")\n",
    "            print(f\"  - Saved to: {output_path}\")\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Display final summary\n",
    "    if all_summaries:\n",
    "        print(\"\\nProcessing complete! Summary:\")\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Show sample of last processed file\n",
    "        print(\"\\nSample of the last processed file:\")\n",
    "        display(merged_df)\n",
    "    else:\n",
    "        print(\"\\nNo files were processed successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
