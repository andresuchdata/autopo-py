{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26400c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e2257",
   "metadata": {},
   "source": [
    "### Add Supply Chain params for AutoPO\n",
    "\n",
    "- Safety stock - (max sales x max lead time) - (avg sales x avg lead time)\n",
    "- Reorder point - avg sales x avg lead time + safety stock\n",
    "- Stock cover days (for 21 days) - avg sales x 21\n",
    "- RoP_Reference (1 -> RoP > Stock cover days, 0 -> RoP < Stock cover days)\n",
    "- Current stock days cover -> Current stock / avg sales\n",
    "- Is_open_po (1 -> Current Stock < Reorder point, 0 -> otherwise)\n",
    "- Initial_Qty_PO - Reorder point - Current stock\n",
    "\n",
    "- Is_emergency_PO - 1 -> Current stock days cover <= max lead time\n",
    "\n",
    "- Emergency_PO_Qty - (max lead time - Current stock days cover) x Avg sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178c94d",
   "metadata": {},
   "source": [
    "### Mapping Brand and SKU with supplier (add supplier column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89776ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make copies to avoid modifying originals\n",
    "df_clean_trimmed = df_clean.copy()\n",
    "raw_supplier_trimmed = raw_supplier_df.copy()\n",
    "\n",
    "# Trim whitespace from brand names\n",
    "df_clean_trimmed['Brand'] = df_clean_trimmed['Brand'].str.strip()\n",
    "raw_supplier_trimmed['Nama Brand'] = raw_supplier_trimmed['Nama Brand'].str.strip()\n",
    "\n",
    "# First, get all Padang suppliers\n",
    "padang_suppliers = raw_supplier_trimmed[\n",
    "    raw_supplier_trimmed['Nama Store'] == 'Miss Glam Padang'\n",
    "]\n",
    "\n",
    "# Then get all other suppliers (non-Padang)\n",
    "other_suppliers = raw_supplier_trimmed[\n",
    "    raw_supplier_trimmed['Nama Store'] != 'Miss Glam Padang'\n",
    "]\n",
    "\n",
    "# Step 1: Left join with Padang suppliers first (priority)\n",
    "merged_df = pd.merge(\n",
    "    df_clean_trimmed,\n",
    "    padang_suppliers,\n",
    "    left_on='Brand',\n",
    "    right_on='Nama Brand',\n",
    "    how='left',\n",
    "    suffixes=('_clean', '_supplier')\n",
    ")\n",
    "\n",
    "# Step 2: For rows without Padang supplier, try to find other suppliers\n",
    "# Get the indices of rows that didn't get a match with Padang suppliers\n",
    "no_padang_match = merged_df[merged_df['Nama Brand'].isna()].index\n",
    "\n",
    "if len(no_padang_match) > 0:\n",
    "    # Get the brands that need non-Padang suppliers\n",
    "    brands_needing_suppliers = merged_df.loc[no_padang_match, 'Brand'].unique()\n",
    "    \n",
    "    # Get the first matching supplier for each brand (you can change this logic if needed)\n",
    "    first_supplier_per_brand = other_suppliers.drop_duplicates(subset='Nama Brand')\n",
    "    \n",
    "    # Update the rows that didn't have Padang suppliers\n",
    "    for brand in brands_needing_suppliers:\n",
    "        supplier_data = first_supplier_per_brand[first_supplier_per_brand['Nama Brand'] == brand]\n",
    "        if not supplier_data.empty:\n",
    "            # Update the corresponding rows in merged_df\n",
    "            brand_mask = (merged_df['Brand'] == brand) & (merged_df['Nama Brand'].isna())\n",
    "            for col in supplier_data.columns:\n",
    "                if col in merged_df.columns and col != 'Brand':  # Don't overwrite the Brand column\n",
    "                    merged_df.loc[brand_mask, col] = supplier_data[col].values[0]\n",
    "\n",
    "# Clean up: For any remaining NaN values in supplier columns, fill with empty string or as needed\n",
    "supplier_columns = [\n",
    "    'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "    'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "    'Promo Factor', 'Delay Factor'\n",
    "]\n",
    "\n",
    "for col in supplier_columns:\n",
    "    if col in merged_df.columns:\n",
    "        if merged_df[col].dtype == 'object':\n",
    "            merged_df[col] = merged_df[col].fillna('')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Total rows in df_clean: {len(df_clean_trimmed)}\")\n",
    "print(f\"Total rows after merge: {len(merged_df)}\")\n",
    "\n",
    "# Count how many rows got Padang suppliers vs other suppliers vs no suppliers\n",
    "padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                       (merged_df['Nama Store'] != '')).sum()\n",
    "no_supplier = (merged_df['Nama Store'] == '').sum()\n",
    "\n",
    "print(f\"\\nSuppliers matched:\")\n",
    "print(f\"- 'Miss Glam Padang' suppliers: {padang_count} rows\")\n",
    "print(f\"- Other suppliers: {other_supplier_count} rows\")\n",
    "print(f\"- No supplier data: {no_supplier} rows\")\n",
    "\n",
    "# Save the result\n",
    "os.makedirs('output', exist_ok=True)\n",
    "output_path = 'output/merged_with_suppliers.csv'\n",
    "merged_df.to_csv(output_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "\n",
    "# Show a sample of the results\n",
    "print(\"\\nSample of merged data (first 5 rows):\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_clean with raw_supplier_df to see all supplier matches\n",
    "all_suppliers_merge = pd.merge(\n",
    "    df_clean_trimmed,\n",
    "    raw_supplier_trimmed,\n",
    "    left_on='Brand',\n",
    "    right_on='Nama Brand',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Group by Brand and SKU to count unique suppliers\n",
    "supplier_counts = all_suppliers_merge.groupby(['Brand', 'SKU'])['Nama Supplier'].nunique().reset_index()\n",
    "supplier_counts.columns = ['Brand', 'SKU', 'Supplier_Count']\n",
    "\n",
    "# Filter for brands/SKUs with multiple suppliers\n",
    "multi_supplier_items = supplier_counts[supplier_counts['Supplier_Count'] > 1]\n",
    "\n",
    "print(f\"Found {len(multi_supplier_items)} brand/SKU combinations with multiple suppliers\")\n",
    "print(\"\\nSample of items with multiple suppliers:\")\n",
    "display(multi_supplier_items.head())\n",
    "\n",
    "# If you want to see the actual supplier details for these items\n",
    "if not multi_supplier_items.empty:\n",
    "    print(\"\\nDetailed supplier information for multi-supplier items:\")\n",
    "    multi_supplier_details = all_suppliers_merge.merge(\n",
    "        multi_supplier_items[['Brand', 'SKU']],\n",
    "        on=['Brand', 'SKU']\n",
    "    )\n",
    "    display(multi_supplier_details[['Brand', 'SKU', 'Nama Supplier', 'Nama Store']].drop_duplicates().sort_values(['Brand', 'SKU']))\n",
    "\n",
    "    # List of SKUs to check\n",
    "skus_to_check = [\n",
    "    '8995232702124',  # ACNEMED\n",
    "    '8992821100293',  # ACNES\n",
    "    '8992821100309',  # ACNES\n",
    "    '8992821100323',  # ACNES\n",
    "    '8992821100354'   # ACNES\n",
    "]\n",
    "\n",
    "# Convert SKUs to integers (since they appear as integers in df_clean)\n",
    "skus_to_check = [int(sku) for sku in skus_to_check]\n",
    "\n",
    "# Check if these SKUs exist in df_clean\n",
    "found_skus = merged_df[merged_df['SKU'].isin(skus_to_check)]\n",
    "\n",
    "if not found_skus.empty:\n",
    "    print(\"Found matching SKUs in df_clean:\")\n",
    "    display(found_skus[['Brand', 'SKU', 'Nama']])\n",
    "else:\n",
    "    print(\"None of these SKUs were found in df_clean.\")\n",
    "    print(\"\\nChecking if there are any similar SKUs...\")\n",
    "    \n",
    "    # Check for any SKUs that contain these numbers\n",
    "    for sku in skus_to_check:\n",
    "        similar = merged_df[merged_df['SKU'].astype(str).str.contains(str(sku)[:8])]\n",
    "        if not similar.empty:\n",
    "            print(f\"\\nSKUs similar to {sku}:\")\n",
    "            display(similar[['Brand', 'SKU', 'Nama']])\n",
    "    \n",
    "    # Check the data types to ensure we're comparing correctly\n",
    "    print(\"\\nData type of SKU column:\", merged_df['SKU'].dtype)\n",
    "    print(\"Sample SKUs from df_clean:\", merged_df['SKU'].head().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a03",
   "metadata": {},
   "source": [
    "### Find brands who are missing suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find brands in df_clean that don't have a match in raw_supplier_df\n",
    "missing_brands = set(df_clean['Brand']) - set(raw_supplier_df['Nama Brand'].dropna().unique())\n",
    "\n",
    "print(f\"Number of brands in df_clean: {len(df_clean['Brand'].unique())}\")\n",
    "print(f\"Number of brands in raw_supplier_df: {len(raw_supplier_df['Nama Brand'].unique())}\")\n",
    "print(f\"\\nNumber of brands missing supplier data: {len(missing_brands)}\")\n",
    "print(\"\\nFirst 20 missing brands (alphabetical order):\")\n",
    "print(sorted(list(missing_brands))[:20])\n",
    "\n",
    "# Count how many rows are affected per missing brand\n",
    "missing_brand_counts = df_clean[df_clean['Brand'].isin(missing_brands)]['Brand'].value_counts()\n",
    "print(\"\\nTop 20 missing brands by row count:\")\n",
    "print(missing_brand_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6ba5",
   "metadata": {},
   "source": [
    "# Final batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce491334",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = [\n",
    "    'HPP', 'Harga', 'Ranking', 'Grade', 'Terjual', 'Stok', 'Lost Days',\n",
    "    'Velocity Capped', 'Daily Sales', 'Lead Time', 'Max. Daily Sales',\n",
    "    'Max. Lead Time', 'Min. Order', 'Safety Stok', 'ROP', '3W Cover',\n",
    "    'Sedang PO', 'Suggested', 'Amount', 'Promo Factor', 'Delay Factor',\n",
    "    'Stock Cover', 'Days to Backup', 'Qty to Backup'\n",
    "]\n",
    "\n",
    "NA_VALUES = {\n",
    "    'NAN', 'NA', '#N/A', 'NULL', 'NONE', '', '?', '-', 'INF', '-INF',\n",
    "    '+INF', 'INFINITY', '-INFINITY', '1.#INF', '-1.#INF', '1.#QNAN'\n",
    "}\n",
    "\n",
    "def _patch_openpyxl_number_casting():\n",
    "    \"\"\"Ensure openpyxl won't crash when encountering NAN/INF in numeric cells.\"\"\"\n",
    "    print(\"Calling _patch_openpyxl_number_casting...\")\n",
    "\n",
    "    try:\n",
    "        from openpyxl.worksheet import _reader\n",
    "\n",
    "        original_cast = _reader._cast_number\n",
    "\n",
    "        def _safe_cast_number(value):  # pragma: no cover - monkey patch\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().upper() in NA_VALUES:\n",
    "                    return 0\n",
    "            try:\n",
    "                return original_cast(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0 if value in (None, '') else value\n",
    "\n",
    "        _reader._cast_number = _safe_cast_number\n",
    "    except Exception:\n",
    "        # If patch fails we continue; runtime reader will still attempt default behaviour\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_special_sku_60(path):\n",
    "    print(f\"Loading Special SKU with 60 days target cover data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check file extension\n",
    "        file_ext = str(path).lower().split('.')[-1]\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Special SKU with 60 days target cover data with {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Special SKU with 60 days target cover data from {path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da39de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _patch_openpyxl_number_casting...\n",
      "Loading Special SKU with 60 days target cover data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/special_sku_60.csv...\n",
      "Successfully loaded Special SKU with 60 days target cover data with 40 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama Produk</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8999999595357</td>\n",
       "      <td>DOVE Perawatan Rambut Rontok Hair Tonic Spray ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8999999584207</td>\n",
       "      <td>DOVE Deep Cleanse Micellar Shampo Himalaya Sal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8999999526344</td>\n",
       "      <td>TRESEMME Shampoo Hair Fall Tresplex 170ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40200509458</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Pink 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40200509242</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Kuning 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40200509360</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Ungu 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8999999540159</td>\n",
       "      <td>VASELINE Repairing Jelly Aloevera 50ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8999999559588</td>\n",
       "      <td>VASELINE Body Lotion Serum Soft Glow 180ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8999999502942</td>\n",
       "      <td>VASELINE Repairing Jelly 50ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8999999035273</td>\n",
       "      <td>VASELINE Healthy Bright Spf 30 PA++ Gluta Vita...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8999999577421</td>\n",
       "      <td>VASELINE Healthy Bright Hijab Protect Gluta Vi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8999999502973</td>\n",
       "      <td>VASELINE Repairing Jelly 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8999999559595</td>\n",
       "      <td>VASELINE Body Serum Fiem Glow 180ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8999999577407</td>\n",
       "      <td>VASELINE Healthy Bright Radiant Gluta Vitamin ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30100244316</td>\n",
       "      <td>LUX Mood Library Peaceful Galaxy Body Scrub 360gr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30100167658</td>\n",
       "      <td>LUX Mood Library Peaceful Galaxy Shower Gel 470gr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10100507517</td>\n",
       "      <td>THE ORIGINOTE Micellar Cleansing Tissue 10pcs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10100724414</td>\n",
       "      <td>THE ORIGINOTE Hyalu BHA Acne Micellar Water 300ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20101207602</td>\n",
       "      <td>SEA MAKEUP Stayput Prime &amp; Set Acne Setting Sp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>725765154392</td>\n",
       "      <td>SEA MAKEUP Lock It Matte Acne Setting Spray 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>745604686181</td>\n",
       "      <td>GRACE AND GLOW Bightening Sun Body Serum 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30200424556</td>\n",
       "      <td>GRACE AND GLOW Whitw B-3 Bright Body Gel Serum...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40100204145</td>\n",
       "      <td>GRACE AND GLOW Velvet Breeze Dry Shampoo 150ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40100204024</td>\n",
       "      <td>GRACE AND GLOW Daisy Breeze Dry Shampoo 150ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30400519016</td>\n",
       "      <td>GRACE AND GLOW Invisible Smooth Antiperspirant...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8997230373085</td>\n",
       "      <td>DEAR ME BEAUTY Spf 50 + Skin Barrier Sunscreen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8997230373894</td>\n",
       "      <td>DEAR ME BEAUTY Serum Lip Tint Dear Vania 3.5ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8999908082800</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8999908266101</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8999908082701</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8999908805508</td>\n",
       "      <td>MARINA UV White Sunblock Spf 30 PA++ Hand &amp; Bo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8999908618702</td>\n",
       "      <td>MARINA Healty and Glow Body Scrub 200ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8999999581930</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Flawless Bri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8999999581947</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Dewy Radianc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9556126670654</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Flawless Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8999999719418</td>\n",
       "      <td>VASELINE Body Lotion Healty Bright Uv Extra Br...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8999999590055</td>\n",
       "      <td>VASELINE Healty Burst Lotion GLUTA-HYA Overnig...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8851932417464</td>\n",
       "      <td>VASELINE Daily Sun Refreshing SPF50+ 170ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8999999003043</td>\n",
       "      <td>VASELINE Healty Bright Sun Pollution Spf24 Pro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8999999596972</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Serum Burst ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SKU                                        Nama Produk  \\\n",
       "0   8999999595357  DOVE Perawatan Rambut Rontok Hair Tonic Spray ...   \n",
       "1   8999999584207  DOVE Deep Cleanse Micellar Shampo Himalaya Sal...   \n",
       "2   8999999526344          TRESEMME Shampoo Hair Fall Tresplex 170ml   \n",
       "3     40200509458       SUNSILK Multivitamin Hair Parfume Pink 100ml   \n",
       "4     40200509242     SUNSILK Multivitamin Hair Parfume Kuning 100ml   \n",
       "5     40200509360       SUNSILK Multivitamin Hair Parfume Ungu 100ml   \n",
       "6   8999999540159             VASELINE Repairing Jelly Aloevera 50ml   \n",
       "7   8999999559588         VASELINE Body Lotion Serum Soft Glow 180ml   \n",
       "8   8999999502942                      VASELINE Repairing Jelly 50ml   \n",
       "9   8999999035273  VASELINE Healthy Bright Spf 30 PA++ Gluta Vita...   \n",
       "10  8999999577421  VASELINE Healthy Bright Hijab Protect Gluta Vi...   \n",
       "11  8999999502973                     VASELINE Repairing Jelly 100ml   \n",
       "12  8999999559595                VASELINE Body Serum Fiem Glow 180ml   \n",
       "13  8999999577407  VASELINE Healthy Bright Radiant Gluta Vitamin ...   \n",
       "14    30100244316  LUX Mood Library Peaceful Galaxy Body Scrub 360gr   \n",
       "15    30100167658  LUX Mood Library Peaceful Galaxy Shower Gel 470gr   \n",
       "16    10100507517      THE ORIGINOTE Micellar Cleansing Tissue 10pcs   \n",
       "17    10100724414  THE ORIGINOTE Hyalu BHA Acne Micellar Water 300ml   \n",
       "18    20101207602  SEA MAKEUP Stayput Prime & Set Acne Setting Sp...   \n",
       "19   725765154392  SEA MAKEUP Lock It Matte Acne Setting Spray 100ml   \n",
       "20   745604686181     GRACE AND GLOW Bightening Sun Body Serum 100ml   \n",
       "21    30200424556  GRACE AND GLOW Whitw B-3 Bright Body Gel Serum...   \n",
       "22    40100204145     GRACE AND GLOW Velvet Breeze Dry Shampoo 150ml   \n",
       "23    40100204024      GRACE AND GLOW Daisy Breeze Dry Shampoo 150ml   \n",
       "24    30400519016  GRACE AND GLOW Invisible Smooth Antiperspirant...   \n",
       "25  8997230373085  DEAR ME BEAUTY Spf 50 + Skin Barrier Sunscreen...   \n",
       "26  8997230373894     DEAR ME BEAUTY Serum Lip Tint Dear Vania 3.5ml   \n",
       "27  8999908082800  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "28  8999908266101  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "29  8999908082701  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "30  8999908805508  MARINA UV White Sunblock Spf 30 PA++ Hand & Bo...   \n",
       "31  8999908618702            MARINA Healty and Glow Body Scrub 200ml   \n",
       "32  8999999581930  VASELINE Healthy Bright Gluta-Hya Flawless Bri...   \n",
       "33  8999999581947  VASELINE Healthy Bright Gluta-Hya Dewy Radianc...   \n",
       "34  9556126670654  VASELINE Healthy Bright Gluta-Hya Flawless Glo...   \n",
       "35  8999999719418  VASELINE Body Lotion Healty Bright Uv Extra Br...   \n",
       "36  8999999590055  VASELINE Healty Burst Lotion GLUTA-HYA Overnig...   \n",
       "37  8851932417464         VASELINE Daily Sun Refreshing SPF50+ 170ml   \n",
       "38  8999999003043  VASELINE Healty Bright Sun Pollution Spf24 Pro...   \n",
       "39  8999999596972  VASELINE Healthy Bright Gluta-Hya Serum Burst ...   \n",
       "\n",
       "    Unnamed: 2  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         NaN  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "18         NaN  \n",
       "19         NaN  \n",
       "20         NaN  \n",
       "21         NaN  \n",
       "22         NaN  \n",
       "23         NaN  \n",
       "24         NaN  \n",
       "25         NaN  \n",
       "26         NaN  \n",
       "27         NaN  \n",
       "28         NaN  \n",
       "29         NaN  \n",
       "30         NaN  \n",
       "31         NaN  \n",
       "32         NaN  \n",
       "33         NaN  \n",
       "34         NaN  \n",
       "35         NaN  \n",
       "36         NaN  \n",
       "37         NaN  \n",
       "38         NaN  \n",
       "39         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading supplier data: /Users/andresuchitra/dev/missglam/autopo/notebook/data/supplier.csv\n",
      "Loading Padang data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/rawpo/xlsx/1. Miss glam Padang.xlsx...\n",
      "Successfully loaded Padang data with 6761 rows\n",
      "\n",
      "Processing PO file: 1. Miss Glam Padang.xlsx ....\n",
      "  - Extracted location: PADANG\n",
      "\n",
      "Reading excel file: 1. Miss Glam Padang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 1. Miss Glam Padang.xlsx with 6761 rows\n",
      "Processing store: PADANG - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 138 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/1. Miss Glam Padang.csv\n",
      "  - Location: PADANG\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 6761\n",
      "  - 'Miss Glam Padang' suppliers: 6623 rows\n",
      "  - Other suppliers: 17 rows\n",
      "  - No supplier data: 121 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.xlsx\n",
      "\n",
      "Processing PO file: 10. Miss Glam Palembang.xlsx ....\n",
      "  - Extracted location: PALEMBANG\n",
      "\n",
      "Reading excel file: 10. Miss Glam Palembang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 10. Miss Glam Palembang.xlsx with 4873 rows\n",
      "Processing store: PALEMBANG - 26.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 116 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/10. Miss Glam Palembang.csv\n",
      "  - Location: PALEMBANG\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 4873\n",
      "  - 'Miss Glam Padang' suppliers: 10 rows\n",
      "  - Other suppliers: 4796 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.xlsx\n",
      "\n",
      "Processing PO file: 11. Miss Glam Damar.xlsx ....\n",
      "  - Extracted location: DAMAR\n",
      "\n",
      "Reading excel file: 11. Miss Glam Damar.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 11. Miss Glam Damar.xlsx with 6656 rows\n",
      "Processing store: DAMAR - 91.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 134 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/11. Miss Glam Damar.csv\n",
      "  - Location: DAMAR\n",
      "  - Contribution: 91%\n",
      "  - Rows processed: 6656\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 6531 rows\n",
      "  - No supplier data: 125 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.xlsx\n",
      "\n",
      "Processing PO file: 12. Miss Glam Bangka.xlsx ....\n",
      "  - Extracted location: BANGKA\n",
      "\n",
      "Reading excel file: 12. Miss Glam Bangka.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 12. Miss Glam Bangka.xlsx with 4542 rows\n",
      "Processing store: BANGKA - 28.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 129 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/12. Miss Glam Bangka.csv\n",
      "  - Location: BANGKA\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 4542\n",
      "  - 'Miss Glam Padang' suppliers: 21 rows\n",
      "  - Other suppliers: 4443 rows\n",
      "  - No supplier data: 78 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.xlsx\n",
      "\n",
      "Processing PO file: 13. Miss Glam Payakumbuh.xlsx ....\n",
      "  - Extracted location: PAYAKUMBUH\n",
      "\n",
      "Reading excel file: 13. Miss Glam Payakumbuh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 13. Miss Glam Payakumbuh.xlsx with 5304 rows\n",
      "Processing store: PAYAKUMBUH - 47.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 83 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/13. Miss Glam Payakumbuh.csv\n",
      "  - Location: PAYAKUMBUH\n",
      "  - Contribution: 47%\n",
      "  - Rows processed: 5304\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 5224 rows\n",
      "  - No supplier data: 80 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.xlsx\n",
      "\n",
      "Processing PO file: 14. Miss Glam Solok.xlsx ....\n",
      "  - Extracted location: SOLOK\n",
      "\n",
      "Reading excel file: 14. Miss Glam Solok.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 14. Miss Glam Solok.xlsx with 4657 rows\n",
      "Processing store: SOLOK - 37.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 64 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/14. Miss Glam Solok.csv\n",
      "  - Location: SOLOK\n",
      "  - Contribution: 37%\n",
      "  - Rows processed: 4657\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 4597 rows\n",
      "  - No supplier data: 60 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.xlsx\n",
      "\n",
      "Processing PO file: 15. Miss Glam Tembilahan.xlsx ....\n",
      "  - Extracted location: TEMBILAHAN\n",
      "\n",
      "Reading excel file: 15. Miss Glam Tembilahan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 15. Miss Glam Tembilahan.xlsx with 4351 rows\n",
      "Processing store: TEMBILAHAN - 27.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 80 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/15. Miss Glam Tembilahan.csv\n",
      "  - Location: TEMBILAHAN\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 4351\n",
      "  - 'Miss Glam Padang' suppliers: 9 rows\n",
      "  - Other suppliers: 4277 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.xlsx\n",
      "\n",
      "Processing PO file: 16. Miss Glam Lubuk Linggau.xlsx ....\n",
      "  - Extracted location: LUBUK LINGGAU\n",
      "\n",
      "Reading excel file: 16. Miss Glam Lubuk Linggau.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 16. Miss Glam Lubuk Linggau.xlsx with 4410 rows\n",
      "Processing store: LUBUK LINGGAU - 26.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 175 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/16. Miss Glam Lubuk Linggau.csv\n",
      "  - Location: LUBUK LINGGAU\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 4410\n",
      "  - 'Miss Glam Padang' suppliers: 7 rows\n",
      "  - Other suppliers: 4345 rows\n",
      "  - No supplier data: 58 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.xlsx\n",
      "\n",
      "Processing PO file: 17. Miss Glam Dumai.xlsx ....\n",
      "  - Extracted location: DUMAI\n",
      "\n",
      "Reading excel file: 17. Miss Glam Dumai.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 17. Miss Glam Dumai.xlsx with 4778 rows\n",
      "Processing store: DUMAI - 36.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 67 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/17. Miss Glam Dumai.csv\n",
      "  - Location: DUMAI\n",
      "  - Contribution: 36%\n",
      "  - Rows processed: 4778\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 4713 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.xlsx\n",
      "\n",
      "Processing PO file: 18. Miss Glam Kedaton.xlsx ....\n",
      "  - Extracted location: KEDATON\n",
      "\n",
      "Reading excel file: 18. Miss Glam Kedaton.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 18. Miss Glam Kedaton.xlsx with 4212 rows\n",
      "Processing store: KEDATON - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 118 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/18. Miss Glam Kedaton.csv\n",
      "  - Location: KEDATON\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4212\n",
      "  - 'Miss Glam Padang' suppliers: 13 rows\n",
      "  - Other suppliers: 4132 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.xlsx\n",
      "\n",
      "Processing PO file: 19. Miss Glam Rantau Prapat.xlsx ....\n",
      "  - Extracted location: RANTAU PRAPAT\n",
      "\n",
      "Reading excel file: 19. Miss Glam Rantau Prapat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 19. Miss Glam Rantau Prapat.xlsx with 4278 rows\n",
      "Processing store: RANTAU PRAPAT - 27.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 98 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/19. Miss Glam Rantau Prapat.csv\n",
      "  - Location: RANTAU PRAPAT\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 4278\n",
      "  - 'Miss Glam Padang' suppliers: 19 rows\n",
      "  - Other suppliers: 4191 rows\n",
      "  - No supplier data: 68 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.xlsx\n",
      "\n",
      "Processing PO file: 2. Miss Glam Pekanbaru.xlsx ....\n",
      "  - Extracted location: PEKANBARU\n",
      "\n",
      "Reading excel file: 2. Miss Glam Pekanbaru.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 2. Miss Glam Pekanbaru.xlsx with 5957 rows\n",
      "Processing store: PEKANBARU - 60.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 119 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/2. Miss Glam Pekanbaru.csv\n",
      "  - Location: PEKANBARU\n",
      "  - Contribution: 60%\n",
      "  - Rows processed: 5957\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5847 rows\n",
      "  - No supplier data: 106 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.xlsx\n",
      "\n",
      "Processing PO file: 20. Miss Glam Tanjung Pinang.xlsx ....\n",
      "  - Extracted location: TANJUNG PINANG\n",
      "\n",
      "Reading excel file: 20. Miss Glam Tanjung Pinang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 20. Miss Glam Tanjung Pinang.xlsx with 4037 rows\n",
      "Processing store: TANJUNG PINANG - 19.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 95 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/20. Miss Glam Tanjung Pinang.csv\n",
      "  - Location: TANJUNG PINANG\n",
      "  - Contribution: 19%\n",
      "  - Rows processed: 4037\n",
      "  - 'Miss Glam Padang' suppliers: 8 rows\n",
      "  - Other suppliers: 3961 rows\n",
      "  - No supplier data: 68 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.xlsx\n",
      "\n",
      "Processing PO file: 21. Miss Glam Sutomo.xlsx ....\n",
      "  - Extracted location: SUTOMO\n",
      "\n",
      "Reading excel file: 21. Miss Glam Sutomo.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 21. Miss Glam Sutomo.xlsx with 5706 rows\n",
      "Processing store: SUTOMO - 49.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 90 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/21. Miss Glam Sutomo.csv\n",
      "  - Location: SUTOMO\n",
      "  - Contribution: 49%\n",
      "  - Rows processed: 5706\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5625 rows\n",
      "  - No supplier data: 77 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.xlsx\n",
      "\n",
      "Processing PO file: 22. Miss Glam Pasaman Barat.xlsx ....\n",
      "  - Extracted location: PASAMAN BARAT\n",
      "\n",
      "Reading excel file: 22. Miss Glam Pasaman Barat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 22. Miss Glam Pasaman Barat.xlsx with 3842 rows\n",
      "Processing store: PASAMAN BARAT - 17.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 61 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasaman Barat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/22. Miss Glam Pasaman Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/22. Miss Glam Pasaman Barat.csv\n",
      "  - Location: PASAMAN BARAT\n",
      "  - Contribution: 17%\n",
      "  - Rows processed: 3842\n",
      "  - 'Miss Glam Padang' suppliers: 6 rows\n",
      "  - Other suppliers: 3794 rows\n",
      "  - No supplier data: 42 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasaman Barat.xlsx\n",
      "\n",
      "Processing PO file: 23. Miss Glam Halat.xlsx ....\n",
      "  - Extracted location: HALAT\n",
      "\n",
      "Reading excel file: 23. Miss Glam Halat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 23. Miss Glam Halat.xlsx with 4805 rows\n",
      "Processing store: HALAT - 31.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 121 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/23. Miss Glam Halat.csv\n",
      "  - Location: HALAT\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 4805\n",
      "  - 'Miss Glam Padang' suppliers: 15 rows\n",
      "  - Other suppliers: 4696 rows\n",
      "  - No supplier data: 94 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.xlsx\n",
      "\n",
      "Processing PO file: 24. Miss Glam Duri.xlsx ....\n",
      "  - Extracted location: DURI\n",
      "\n",
      "Reading excel file: 24. Miss Glam Duri.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 24. Miss Glam Duri.xlsx with 3968 rows\n",
      "Processing store: DURI - 28.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 57 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/24. Miss Glam Duri.csv\n",
      "  - Location: DURI\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 3968\n",
      "  - 'Miss Glam Padang' suppliers: 2 rows\n",
      "  - Other suppliers: 3914 rows\n",
      "  - No supplier data: 52 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.xlsx\n",
      "\n",
      "Processing PO file: 25. Miss Glam Sudirman.xlsx ....\n",
      "  - Extracted location: SUDIRMAN\n",
      "\n",
      "Reading excel file: 25. Miss Glam Sudirman.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 25. Miss Glam Sudirman.xlsx with 5632 rows\n",
      "Processing store: SUDIRMAN - 44.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 112 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/25. Miss Glam Sudirman.csv\n",
      "  - Location: SUDIRMAN\n",
      "  - Contribution: 44%\n",
      "  - Rows processed: 5632\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5530 rows\n",
      "  - No supplier data: 98 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.xlsx\n",
      "\n",
      "Processing PO file: 26. Miss Glam Dr. Mansyur.xlsx ....\n",
      "  - Extracted location: DR. MANSYUR\n",
      "\n",
      "Reading excel file: 26. Miss Glam Dr. Mansyur.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 26. Miss Glam Dr. Mansyur.xlsx with 4963 rows\n",
      "Processing store: DR. MANSYUR - 25.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 126 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/26. Miss Glam Dr. Mansyur.csv\n",
      "  - Location: DR. MANSYUR\n",
      "  - Contribution: 25%\n",
      "  - Rows processed: 4963\n",
      "  - 'Miss Glam Padang' suppliers: 16 rows\n",
      "  - Other suppliers: 4860 rows\n",
      "  - No supplier data: 87 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.xlsx\n",
      "\n",
      "Processing PO file: 27. Miss Glam P. Sidimpuan.xlsx ....\n",
      "  - Extracted location: P. SIDIMPUAN\n",
      "\n",
      "Reading excel file: 27. Miss Glam P. Sidimpuan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 27. Miss Glam P. Sidimpuan.xlsx with 3771 rows\n",
      "Processing store: P. SIDIMPUAN - 31.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 46 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/27. Miss Glam P. Sidimpuan.csv\n",
      "  - Location: P. SIDIMPUAN\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 3771\n",
      "  - 'Miss Glam Padang' suppliers: 5 rows\n",
      "  - Other suppliers: 3729 rows\n",
      "  - No supplier data: 37 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.xlsx\n",
      "\n",
      "Processing PO file: 28. Miss Glam Aceh.xlsx ....\n",
      "  - Extracted location: ACEH\n",
      "\n",
      "Reading excel file: 28. Miss Glam Aceh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 28. Miss Glam Aceh.xlsx with 3552 rows\n",
      "Processing store: ACEH - 15.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 72 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/28. Miss Glam Aceh.csv\n",
      "  - Location: ACEH\n",
      "  - Contribution: 15%\n",
      "  - Rows processed: 3552\n",
      "  - 'Miss Glam Padang' suppliers: 14 rows\n",
      "  - Other suppliers: 3493 rows\n",
      "  - No supplier data: 45 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.xlsx\n",
      "\n",
      "Processing PO file: 29. Miss Glam Marpoyan.xlsx ....\n",
      "  - Extracted location: MARPOYAN\n",
      "\n",
      "Reading excel file: 29. Miss Glam Marpoyan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 29. Miss Glam Marpoyan.xlsx with 4621 rows\n",
      "Processing store: MARPOYAN - 30.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 110 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/29. Miss Glam Marpoyan.csv\n",
      "  - Location: MARPOYAN\n",
      "  - Contribution: 30%\n",
      "  - Rows processed: 4621\n",
      "  - 'Miss Glam Padang' suppliers: 19 rows\n",
      "  - Other suppliers: 4537 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.xlsx\n",
      "\n",
      "Processing PO file: 3. Miss Glam Jambi.xlsx ....\n",
      "  - Extracted location: JAMBI\n",
      "\n",
      "Reading excel file: 3. Miss Glam Jambi.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 3. Miss Glam Jambi.xlsx with 5236 rows\n",
      "Processing store: JAMBI - 33.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 131 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/3. Miss Glam Jambi.csv\n",
      "  - Location: JAMBI\n",
      "  - Contribution: 33%\n",
      "  - Rows processed: 5236\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5145 rows\n",
      "  - No supplier data: 87 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.xlsx\n",
      "\n",
      "Processing PO file: 30. Miss Glam Sei Penuh.xlsx ....\n",
      "  - Extracted location: SEI PENUH\n",
      "\n",
      "Reading excel file: 30. Miss Glam Sei Penuh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 30. Miss Glam Sei Penuh.xlsx with 3581 rows\n",
      "Processing store: SEI PENUH - 21.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 72 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/30. Miss Glam Sei Penuh.csv\n",
      "  - Location: SEI PENUH\n",
      "  - Contribution: 21%\n",
      "  - Rows processed: 3581\n",
      "  - 'Miss Glam Padang' suppliers: 27 rows\n",
      "  - Other suppliers: 3526 rows\n",
      "  - No supplier data: 28 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.xlsx\n",
      "\n",
      "Processing PO file: 31. Miss Glam Mayang.xlsx ....\n",
      "  - Extracted location: MAYANG\n",
      "\n",
      "Reading excel file: 31. Miss Glam Mayang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 31. Miss Glam Mayang.xlsx with 4267 rows\n",
      "Processing store: MAYANG - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 235 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/31. Miss Glam Mayang.csv\n",
      "  - Location: MAYANG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4267\n",
      "  - 'Miss Glam Padang' suppliers: 30 rows\n",
      "  - Other suppliers: 4190 rows\n",
      "  - No supplier data: 47 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.xlsx\n",
      "\n",
      "Processing PO file: 32. Miss Glam Soeta.xlsx ....\n",
      "  - Extracted location: SOETA\n",
      "Warning: No contribution percentage found for SOETA\n",
      "\n",
      "Reading excel file: 32. Miss Glam Soeta.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 32. Miss Glam Soeta.xlsx with 4490 rows\n",
      "Processing store: SOETA - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 2008 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32. Miss Glam Soeta.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/32. Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/32. Miss Glam Soeta.csv\n",
      "  - Location: SOETA\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 4490\n",
      "  - 'Miss Glam Padang' suppliers: 123 rows\n",
      "  - Other suppliers: 4327 rows\n",
      "  - No supplier data: 40 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32. Miss Glam Soeta.xlsx\n",
      "\n",
      "Processing PO file: 33. Miss Glam Balikpapan.xlsx ....\n",
      "  - Extracted location: BALIKPAPAN\n",
      "Warning: No contribution percentage found for BALIKPAPAN\n",
      "\n",
      "Reading excel file: 33. Miss Glam Balikpapan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 33. Miss Glam Balikpapan.xlsx with 4491 rows\n",
      "Processing store: BALIKPAPAN - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 1586 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/33. Miss Glam Balikpapan.csv\n",
      "  - Location: BALIKPAPAN\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 4491\n",
      "  - 'Miss Glam Padang' suppliers: 84 rows\n",
      "  - Other suppliers: 4346 rows\n",
      "  - No supplier data: 61 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.xlsx\n",
      "\n",
      "Processing PO file: 4. Miss Glam Bukittinggi.xlsx ....\n",
      "  - Extracted location: BUKITTINGGI\n",
      "\n",
      "Reading excel file: 4. Miss Glam Bukittinggi.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 4. Miss Glam Bukittinggi.xlsx with 5315 rows\n",
      "Processing store: BUKITTINGGI - 45.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 71 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/4. Miss Glam Bukittinggi.csv\n",
      "  - Location: BUKITTINGGI\n",
      "  - Contribution: 45%\n",
      "  - Rows processed: 5315\n",
      "  - 'Miss Glam Padang' suppliers: 1 rows\n",
      "  - Other suppliers: 5250 rows\n",
      "  - No supplier data: 64 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.xlsx\n",
      "\n",
      "Processing PO file: 5. Miss Glam Panam.xlsx ....\n",
      "  - Extracted location: PANAM\n",
      "\n",
      "Reading excel file: 5. Miss Glam Panam.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 5. Miss Glam Panam.xlsx with 5272 rows\n",
      "Processing store: PANAM - 46.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 100 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/5. Miss Glam Panam.csv\n",
      "  - Location: PANAM\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 5272\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 5176 rows\n",
      "  - No supplier data: 96 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.xlsx\n",
      "\n",
      "Processing PO file: 6. Miss Glam Muaro Bungo.xlsx ....\n",
      "  - Extracted location: MUARO BUNGO\n",
      "\n",
      "Reading excel file: 6. Miss Glam Muaro Bungo.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 6. Miss Glam Muaro Bungo.xlsx with 4909 rows\n",
      "Processing store: MUARO BUNGO - 42.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 95 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muaro Bungo.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/6. Miss Glam Muaro Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/6. Miss Glam Muaro Bungo.csv\n",
      "  - Location: MUARO BUNGO\n",
      "  - Contribution: 42%\n",
      "  - Rows processed: 4909\n",
      "  - 'Miss Glam Padang' suppliers: 7 rows\n",
      "  - Other suppliers: 4835 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muaro Bungo.xlsx\n",
      "\n",
      "Processing PO file: 7. Miss Glam Lampung.xlsx ....\n",
      "  - Extracted location: LAMPUNG\n",
      "\n",
      "Reading excel file: 7. Miss Glam Lampung.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 7. Miss Glam Lampung.xlsx with 4437 rows\n",
      "Processing store: LAMPUNG - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 106 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/7. Miss Glam Lampung.csv\n",
      "  - Location: LAMPUNG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4437\n",
      "  - 'Miss Glam Padang' suppliers: 17 rows\n",
      "  - Other suppliers: 4358 rows\n",
      "  - No supplier data: 62 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.xlsx\n",
      "\n",
      "Processing PO file: 8. Miss Glam Bengkulu.xlsx ....\n",
      "  - Extracted location: BENGKULU\n",
      "\n",
      "Reading excel file: 8. Miss Glam Bengkulu.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 8. Miss Glam Bengkulu.xlsx with 3961 rows\n",
      "Processing store: BENGKULU - 14.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 107 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/8. Miss Glam Bengkulu.csv\n",
      "  - Location: BENGKULU\n",
      "  - Contribution: 14%\n",
      "  - Rows processed: 3961\n",
      "  - 'Miss Glam Padang' suppliers: 13 rows\n",
      "  - Other suppliers: 3876 rows\n",
      "  - No supplier data: 72 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.xlsx\n",
      "\n",
      "Processing PO file: 9. Miss Glam Medan.xlsx ....\n",
      "  - Extracted location: MEDAN\n",
      "\n",
      "Reading excel file: 9. Miss Glam Medan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_35135/1449918981.py:665: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 9. Miss Glam Medan.xlsx with 5763 rows\n",
      "Processing store: MEDAN - 46.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 151 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/9. Miss Glam Medan.csv\n",
      "  - Location: MEDAN\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 5763\n",
      "  - 'Miss Glam Padang' suppliers: 14 rows\n",
      "  - Other suppliers: 5640 rows\n",
      "  - No supplier data: 109 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.xlsx\n",
      "\n",
      "Processing complete! Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>location</th>\n",
       "      <th>contribution_pct</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>padang_suppliers</th>\n",
       "      <th>other_suppliers</th>\n",
       "      <th>no_supplier</th>\n",
       "      <th>status</th>\n",
       "      <th>output_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Miss Glam Padang.xlsx</td>\n",
       "      <td>PADANG</td>\n",
       "      <td>100</td>\n",
       "      <td>6761</td>\n",
       "      <td>6623</td>\n",
       "      <td>17</td>\n",
       "      <td>121</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10. Miss Glam Palembang.xlsx</td>\n",
       "      <td>PALEMBANG</td>\n",
       "      <td>26</td>\n",
       "      <td>4873</td>\n",
       "      <td>10</td>\n",
       "      <td>4796</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11. Miss Glam Damar.xlsx</td>\n",
       "      <td>DAMAR</td>\n",
       "      <td>91</td>\n",
       "      <td>6656</td>\n",
       "      <td>0</td>\n",
       "      <td>6531</td>\n",
       "      <td>125</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12. Miss Glam Bangka.xlsx</td>\n",
       "      <td>BANGKA</td>\n",
       "      <td>28</td>\n",
       "      <td>4542</td>\n",
       "      <td>21</td>\n",
       "      <td>4443</td>\n",
       "      <td>78</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13. Miss Glam Payakumbuh.xlsx</td>\n",
       "      <td>PAYAKUMBUH</td>\n",
       "      <td>47</td>\n",
       "      <td>5304</td>\n",
       "      <td>0</td>\n",
       "      <td>5224</td>\n",
       "      <td>80</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14. Miss Glam Solok.xlsx</td>\n",
       "      <td>SOLOK</td>\n",
       "      <td>37</td>\n",
       "      <td>4657</td>\n",
       "      <td>0</td>\n",
       "      <td>4597</td>\n",
       "      <td>60</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15. Miss Glam Tembilahan.xlsx</td>\n",
       "      <td>TEMBILAHAN</td>\n",
       "      <td>27</td>\n",
       "      <td>4351</td>\n",
       "      <td>9</td>\n",
       "      <td>4277</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16. Miss Glam Lubuk Linggau.xlsx</td>\n",
       "      <td>LUBUK LINGGAU</td>\n",
       "      <td>26</td>\n",
       "      <td>4410</td>\n",
       "      <td>7</td>\n",
       "      <td>4345</td>\n",
       "      <td>58</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17. Miss Glam Dumai.xlsx</td>\n",
       "      <td>DUMAI</td>\n",
       "      <td>36</td>\n",
       "      <td>4778</td>\n",
       "      <td>0</td>\n",
       "      <td>4713</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18. Miss Glam Kedaton.xlsx</td>\n",
       "      <td>KEDATON</td>\n",
       "      <td>18</td>\n",
       "      <td>4212</td>\n",
       "      <td>13</td>\n",
       "      <td>4132</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19. Miss Glam Rantau Prapat.xlsx</td>\n",
       "      <td>RANTAU PRAPAT</td>\n",
       "      <td>27</td>\n",
       "      <td>4278</td>\n",
       "      <td>19</td>\n",
       "      <td>4191</td>\n",
       "      <td>68</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2. Miss Glam Pekanbaru.xlsx</td>\n",
       "      <td>PEKANBARU</td>\n",
       "      <td>60</td>\n",
       "      <td>5957</td>\n",
       "      <td>4</td>\n",
       "      <td>5847</td>\n",
       "      <td>106</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20. Miss Glam Tanjung Pinang.xlsx</td>\n",
       "      <td>TANJUNG PINANG</td>\n",
       "      <td>19</td>\n",
       "      <td>4037</td>\n",
       "      <td>8</td>\n",
       "      <td>3961</td>\n",
       "      <td>68</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21. Miss Glam Sutomo.xlsx</td>\n",
       "      <td>SUTOMO</td>\n",
       "      <td>49</td>\n",
       "      <td>5706</td>\n",
       "      <td>4</td>\n",
       "      <td>5625</td>\n",
       "      <td>77</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22. Miss Glam Pasaman Barat.xlsx</td>\n",
       "      <td>PASAMAN BARAT</td>\n",
       "      <td>17</td>\n",
       "      <td>3842</td>\n",
       "      <td>6</td>\n",
       "      <td>3794</td>\n",
       "      <td>42</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23. Miss Glam Halat.xlsx</td>\n",
       "      <td>HALAT</td>\n",
       "      <td>31</td>\n",
       "      <td>4805</td>\n",
       "      <td>15</td>\n",
       "      <td>4696</td>\n",
       "      <td>94</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24. Miss Glam Duri.xlsx</td>\n",
       "      <td>DURI</td>\n",
       "      <td>28</td>\n",
       "      <td>3968</td>\n",
       "      <td>2</td>\n",
       "      <td>3914</td>\n",
       "      <td>52</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25. Miss Glam Sudirman.xlsx</td>\n",
       "      <td>SUDIRMAN</td>\n",
       "      <td>44</td>\n",
       "      <td>5632</td>\n",
       "      <td>4</td>\n",
       "      <td>5530</td>\n",
       "      <td>98</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26. Miss Glam Dr. Mansyur.xlsx</td>\n",
       "      <td>DR. MANSYUR</td>\n",
       "      <td>25</td>\n",
       "      <td>4963</td>\n",
       "      <td>16</td>\n",
       "      <td>4860</td>\n",
       "      <td>87</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27. Miss Glam P. Sidimpuan.xlsx</td>\n",
       "      <td>P. SIDIMPUAN</td>\n",
       "      <td>31</td>\n",
       "      <td>3771</td>\n",
       "      <td>5</td>\n",
       "      <td>3729</td>\n",
       "      <td>37</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28. Miss Glam Aceh.xlsx</td>\n",
       "      <td>ACEH</td>\n",
       "      <td>15</td>\n",
       "      <td>3552</td>\n",
       "      <td>14</td>\n",
       "      <td>3493</td>\n",
       "      <td>45</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29. Miss Glam Marpoyan.xlsx</td>\n",
       "      <td>MARPOYAN</td>\n",
       "      <td>30</td>\n",
       "      <td>4621</td>\n",
       "      <td>19</td>\n",
       "      <td>4537</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3. Miss Glam Jambi.xlsx</td>\n",
       "      <td>JAMBI</td>\n",
       "      <td>33</td>\n",
       "      <td>5236</td>\n",
       "      <td>4</td>\n",
       "      <td>5145</td>\n",
       "      <td>87</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30. Miss Glam Sei Penuh.xlsx</td>\n",
       "      <td>SEI PENUH</td>\n",
       "      <td>21</td>\n",
       "      <td>3581</td>\n",
       "      <td>27</td>\n",
       "      <td>3526</td>\n",
       "      <td>28</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31. Miss Glam Mayang.xlsx</td>\n",
       "      <td>MAYANG</td>\n",
       "      <td>18</td>\n",
       "      <td>4267</td>\n",
       "      <td>30</td>\n",
       "      <td>4190</td>\n",
       "      <td>47</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32. Miss Glam Soeta.xlsx</td>\n",
       "      <td>SOETA</td>\n",
       "      <td>100</td>\n",
       "      <td>4490</td>\n",
       "      <td>123</td>\n",
       "      <td>4327</td>\n",
       "      <td>40</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33. Miss Glam Balikpapan.xlsx</td>\n",
       "      <td>BALIKPAPAN</td>\n",
       "      <td>100</td>\n",
       "      <td>4491</td>\n",
       "      <td>84</td>\n",
       "      <td>4346</td>\n",
       "      <td>61</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4. Miss Glam Bukittinggi.xlsx</td>\n",
       "      <td>BUKITTINGGI</td>\n",
       "      <td>45</td>\n",
       "      <td>5315</td>\n",
       "      <td>1</td>\n",
       "      <td>5250</td>\n",
       "      <td>64</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5. Miss Glam Panam.xlsx</td>\n",
       "      <td>PANAM</td>\n",
       "      <td>46</td>\n",
       "      <td>5272</td>\n",
       "      <td>0</td>\n",
       "      <td>5176</td>\n",
       "      <td>96</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6. Miss Glam Muaro Bungo.xlsx</td>\n",
       "      <td>MUARO BUNGO</td>\n",
       "      <td>42</td>\n",
       "      <td>4909</td>\n",
       "      <td>7</td>\n",
       "      <td>4835</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7. Miss Glam Lampung.xlsx</td>\n",
       "      <td>LAMPUNG</td>\n",
       "      <td>18</td>\n",
       "      <td>4437</td>\n",
       "      <td>17</td>\n",
       "      <td>4358</td>\n",
       "      <td>62</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8. Miss Glam Bengkulu.xlsx</td>\n",
       "      <td>BENGKULU</td>\n",
       "      <td>14</td>\n",
       "      <td>3961</td>\n",
       "      <td>13</td>\n",
       "      <td>3876</td>\n",
       "      <td>72</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9. Miss Glam Medan.xlsx</td>\n",
       "      <td>MEDAN</td>\n",
       "      <td>46</td>\n",
       "      <td>5763</td>\n",
       "      <td>14</td>\n",
       "      <td>5640</td>\n",
       "      <td>109</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file        location contribution_pct  \\\n",
       "0            1. Miss Glam Padang.xlsx          PADANG              100   \n",
       "1        10. Miss Glam Palembang.xlsx       PALEMBANG               26   \n",
       "2            11. Miss Glam Damar.xlsx           DAMAR               91   \n",
       "3           12. Miss Glam Bangka.xlsx          BANGKA               28   \n",
       "4       13. Miss Glam Payakumbuh.xlsx      PAYAKUMBUH               47   \n",
       "5            14. Miss Glam Solok.xlsx           SOLOK               37   \n",
       "6       15. Miss Glam Tembilahan.xlsx      TEMBILAHAN               27   \n",
       "7    16. Miss Glam Lubuk Linggau.xlsx   LUBUK LINGGAU               26   \n",
       "8            17. Miss Glam Dumai.xlsx           DUMAI               36   \n",
       "9          18. Miss Glam Kedaton.xlsx         KEDATON               18   \n",
       "10   19. Miss Glam Rantau Prapat.xlsx   RANTAU PRAPAT               27   \n",
       "11        2. Miss Glam Pekanbaru.xlsx       PEKANBARU               60   \n",
       "12  20. Miss Glam Tanjung Pinang.xlsx  TANJUNG PINANG               19   \n",
       "13          21. Miss Glam Sutomo.xlsx          SUTOMO               49   \n",
       "14   22. Miss Glam Pasaman Barat.xlsx   PASAMAN BARAT               17   \n",
       "15           23. Miss Glam Halat.xlsx           HALAT               31   \n",
       "16            24. Miss Glam Duri.xlsx            DURI               28   \n",
       "17        25. Miss Glam Sudirman.xlsx        SUDIRMAN               44   \n",
       "18     26. Miss Glam Dr. Mansyur.xlsx     DR. MANSYUR               25   \n",
       "19    27. Miss Glam P. Sidimpuan.xlsx    P. SIDIMPUAN               31   \n",
       "20            28. Miss Glam Aceh.xlsx            ACEH               15   \n",
       "21        29. Miss Glam Marpoyan.xlsx        MARPOYAN               30   \n",
       "22            3. Miss Glam Jambi.xlsx           JAMBI               33   \n",
       "23       30. Miss Glam Sei Penuh.xlsx       SEI PENUH               21   \n",
       "24          31. Miss Glam Mayang.xlsx          MAYANG               18   \n",
       "25           32. Miss Glam Soeta.xlsx           SOETA              100   \n",
       "26      33. Miss Glam Balikpapan.xlsx      BALIKPAPAN              100   \n",
       "27      4. Miss Glam Bukittinggi.xlsx     BUKITTINGGI               45   \n",
       "28            5. Miss Glam Panam.xlsx           PANAM               46   \n",
       "29      6. Miss Glam Muaro Bungo.xlsx     MUARO BUNGO               42   \n",
       "30          7. Miss Glam Lampung.xlsx         LAMPUNG               18   \n",
       "31         8. Miss Glam Bengkulu.xlsx        BENGKULU               14   \n",
       "32            9. Miss Glam Medan.xlsx           MEDAN               46   \n",
       "\n",
       "    total_rows  padang_suppliers  other_suppliers  no_supplier   status  \\\n",
       "0         6761              6623               17          121  Success   \n",
       "1         4873                10             4796           67  Success   \n",
       "2         6656                 0             6531          125  Success   \n",
       "3         4542                21             4443           78  Success   \n",
       "4         5304                 0             5224           80  Success   \n",
       "5         4657                 0             4597           60  Success   \n",
       "6         4351                 9             4277           65  Success   \n",
       "7         4410                 7             4345           58  Success   \n",
       "8         4778                 0             4713           65  Success   \n",
       "9         4212                13             4132           67  Success   \n",
       "10        4278                19             4191           68  Success   \n",
       "11        5957                 4             5847          106  Success   \n",
       "12        4037                 8             3961           68  Success   \n",
       "13        5706                 4             5625           77  Success   \n",
       "14        3842                 6             3794           42  Success   \n",
       "15        4805                15             4696           94  Success   \n",
       "16        3968                 2             3914           52  Success   \n",
       "17        5632                 4             5530           98  Success   \n",
       "18        4963                16             4860           87  Success   \n",
       "19        3771                 5             3729           37  Success   \n",
       "20        3552                14             3493           45  Success   \n",
       "21        4621                19             4537           65  Success   \n",
       "22        5236                 4             5145           87  Success   \n",
       "23        3581                27             3526           28  Success   \n",
       "24        4267                30             4190           47  Success   \n",
       "25        4490               123             4327           40  Success   \n",
       "26        4491                84             4346           61  Success   \n",
       "27        5315                 1             5250           64  Success   \n",
       "28        5272                 0             5176           96  Success   \n",
       "29        4909                 7             4835           67  Success   \n",
       "30        4437                17             4358           62  Success   \n",
       "31        3961                13             3876           72  Success   \n",
       "32        5763                14             5640          109  Success   \n",
       "\n",
       "                                          output_path  \n",
       "0   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "1   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "2   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "3   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "4   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "5   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "6   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "7   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "8   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "9   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "10  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "11  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "12  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "13  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "14  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "15  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "16  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "17  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "18  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "19  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "20  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "21  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "22  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "23  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "24  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "25  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "26  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "27  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "28  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "29  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "30  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "31  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "32  /Users/andresuchitra/dev/missglam/autopo/noteb...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the last processed file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama</th>\n",
       "      <th>Toko</th>\n",
       "      <th>Stok</th>\n",
       "      <th>Daily Sales</th>\n",
       "      <th>Max. Daily Sales</th>\n",
       "      <th>Lead Time</th>\n",
       "      <th>Max. Lead Time</th>\n",
       "      <th>Min. Order</th>\n",
       "      <th>...</th>\n",
       "      <th>Nama Supplier</th>\n",
       "      <th>ID Brand</th>\n",
       "      <th>Nama Brand</th>\n",
       "      <th>ID Store</th>\n",
       "      <th>Nama Store</th>\n",
       "      <th>Hari Order</th>\n",
       "      <th>Min. Purchase</th>\n",
       "      <th>Trading Term</th>\n",
       "      <th>Promo Factor</th>\n",
       "      <th>Delay Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>10400614911</td>\n",
       "      <td>ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>11200219810</td>\n",
       "      <td>ACNAWAY Mugwort Blackhead Treatment Step 1 17ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>11200219943</td>\n",
       "      <td>ACNAWAY Mugwort Blackhead Treatment Step 2 17ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>10400517459</td>\n",
       "      <td>ACNAWAY Mugwort Daily Sunscreen Only For Acne ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>101001107647</td>\n",
       "      <td>ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>YOUVIT</td>\n",
       "      <td>60100406456</td>\n",
       "      <td>YOUVIT Ezzleep 7 Gummies 28gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. ANUGERAH ARGON MEDICA - PPN (PKU)</td>\n",
       "      <td>1019.00</td>\n",
       "      <td>YOUVIT</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>4.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5759</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402932</td>\n",
       "      <td>YU CHUN Mei Cordyceps Brightening Cleanser 100ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402703</td>\n",
       "      <td>YU CHUN Mei Cordyceps Lightening Day Cream 30gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402710</td>\n",
       "      <td>YU CHUN Mei Cordyceps Lightening Night Cream 30g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402918</td>\n",
       "      <td>YU CHUN Mei Serum Whitening Essence 30ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CV. SEMANGAT JAYA - PPN (PKU)</td>\n",
       "      <td>489.00</td>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Miss Glam Pekanbaru</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5763 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand            SKU  \\\n",
       "0     ACNAWAY    10400614911   \n",
       "1     ACNAWAY    11200219810   \n",
       "2     ACNAWAY    11200219943   \n",
       "3     ACNAWAY    10400517459   \n",
       "4     ACNAWAY   101001107647   \n",
       "...       ...            ...   \n",
       "5758   YOUVIT    60100406456   \n",
       "5759  YU CHUN  8997014402932   \n",
       "5760  YU CHUN  8997014402703   \n",
       "5761  YU CHUN  8997014402710   \n",
       "5762  YU CHUN  8997014402918   \n",
       "\n",
       "                                                   Nama             Toko  \\\n",
       "0     ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...  Miss Glam Medan   \n",
       "1       ACNAWAY Mugwort Blackhead Treatment Step 1 17ml  Miss Glam Medan   \n",
       "2       ACNAWAY Mugwort Blackhead Treatment Step 2 17ml  Miss Glam Medan   \n",
       "3     ACNAWAY Mugwort Daily Sunscreen Only For Acne ...  Miss Glam Medan   \n",
       "4     ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...  Miss Glam Medan   \n",
       "...                                                 ...              ...   \n",
       "5758                      YOUVIT Ezzleep 7 Gummies 28gr  Miss Glam Medan   \n",
       "5759   YU CHUN Mei Cordyceps Brightening Cleanser 100ml  Miss Glam Medan   \n",
       "5760    YU CHUN Mei Cordyceps Lightening Day Cream 30gr  Miss Glam Medan   \n",
       "5761   YU CHUN Mei Cordyceps Lightening Night Cream 30g  Miss Glam Medan   \n",
       "5762           YU CHUN Mei Serum Whitening Essence 30ml  Miss Glam Medan   \n",
       "\n",
       "      Stok  Daily Sales  Max. Daily Sales  Lead Time  Max. Lead Time  \\\n",
       "0     6.00         0.03              1.00       5.00           28.00   \n",
       "1     0.00         0.00              0.00       0.00            0.00   \n",
       "2     0.00         0.00              0.00       0.00            0.00   \n",
       "3    12.00         0.27              3.00       5.00           28.00   \n",
       "4     0.00         0.35              1.84       5.00           28.00   \n",
       "...    ...          ...               ...        ...             ...   \n",
       "5758  0.00         0.03              1.00       1.00            2.00   \n",
       "5759  2.00         0.02              0.46       1.00            2.00   \n",
       "5760  4.00         0.06              0.46       1.00            2.00   \n",
       "5761  0.00         0.04              0.46       1.00            2.00   \n",
       "5762  5.00         0.02              0.46       1.00            2.00   \n",
       "\n",
       "      Min. Order  ...                                   Nama Supplier  \\\n",
       "0           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "1           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "2           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "3           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "4           1.00  ...  PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   \n",
       "...          ...  ...                                             ...   \n",
       "5758        1.00  ...           PT. ANUGERAH ARGON MEDICA - PPN (PKU)   \n",
       "5759        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "5760        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "5761        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "5762        3.00  ...                   CV. SEMANGAT JAYA - PPN (PKU)   \n",
       "\n",
       "      ID Brand  Nama Brand  ID Store           Nama Store  Hari Order  \\\n",
       "0      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "1      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "2      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "3      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "4      1480.00     ACNAWAY     19.00      Miss Glam Medan        2.00   \n",
       "...        ...         ...       ...                  ...         ...   \n",
       "5758   1019.00      YOUVIT      8.00  Miss Glam Pekanbaru        4.00   \n",
       "5759    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "5760    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "5761    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "5762    489.00     YU CHUN      8.00  Miss Glam Pekanbaru        0.00   \n",
       "\n",
       "      Min. Purchase  Trading Term  Promo Factor  Delay Factor  \n",
       "0         500000.00          0.00                              \n",
       "1         500000.00          0.00                              \n",
       "2         500000.00          0.00                              \n",
       "3         500000.00          0.00                              \n",
       "4         500000.00          0.00                              \n",
       "...             ...           ...           ...           ...  \n",
       "5758      500000.00          0.00                              \n",
       "5759      500000.00          0.00                              \n",
       "5760      500000.00          0.00                              \n",
       "5761      500000.00          0.00                              \n",
       "5762      500000.00          0.00                              \n",
       "\n",
       "[5763 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import display\n",
    "from locale import atof\n",
    "import numpy as np\n",
    "from openpyxl.styles import numbers\n",
    "\n",
    "_patch_openpyxl_number_casting()\n",
    "\n",
    "# Apply the formatting to numeric columns in your final output\n",
    "def format_dataframe_display(df):\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # Apply formatting to numeric columns\n",
    "    for col in df_display.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: format_id_number(x, 2) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/Users/andresuchitra/dev/missglam/autopo/notebook')\n",
    "SUPPLIER_PATH = BASE_DIR / 'data/supplier.csv'\n",
    "RAWPO_DIR = BASE_DIR / 'data/rawpo/csv'\n",
    "RAWPO_XLSX_DIR = BASE_DIR / 'data/rawpo/xlsx'\n",
    "STORE_CONTRIBUTION_PATH = BASE_DIR / 'data/store_contribution.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'output/complete'\n",
    "OUTPUT_EXCEL_DIR = BASE_DIR / 'output/excel'\n",
    "OUTPUT_M2_DIR = BASE_DIR / 'output/m2'\n",
    "OUTPUT_EMERGENCY_DIR = BASE_DIR / 'output/emergency'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EXCEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_M2_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EMERGENCY_DIR, exist_ok=True)\n",
    "\n",
    "df_special_60 = load_special_sku_60(BASE_DIR / 'data/special_sku_60.csv')\n",
    "\n",
    "display(df_special_60)\n",
    "\n",
    "def load_store_contribution(store_contribution_path):\n",
    "    \"\"\"Load and prepare store contribution data.\"\"\"\n",
    "    store_contrib = pd.read_csv(store_contribution_path, header=None, \n",
    "                              names=['store', 'contribution_pct'])\n",
    "    # Convert store names to lowercase for case-insensitive matching\n",
    "    store_contrib['store_lower'] = store_contrib['store'].str.lower()\n",
    "    return store_contrib\n",
    "\n",
    "def get_contribution_pct(location, store_contrib):\n",
    "    \"\"\"Get contribution percentage for a given location.\"\"\"\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    contrib_row = store_contrib[store_contrib['store_lower'] == location_lower]\n",
    "    if not contrib_row.empty:\n",
    "        return contrib_row['contribution_pct'].values[0]\n",
    "    print(f\"Warning: No contribution percentage found for {location}\")\n",
    "\n",
    "    return 100  # Default to 100% if not found\n",
    "\n",
    "def load_supplier_data(supplier_path):\n",
    "    \"\"\"Load and clean supplier data.\"\"\"\n",
    "    print(f\"Loading supplier data: {supplier_path}\")\n",
    "    df = pd.read_csv(supplier_path, sep=';', decimal=',').fillna('')\n",
    "    df['Nama Brand'] = df['Nama Brand'].str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier information.\"\"\"\n",
    "    print(\"Merging with suppliers...\")\n",
    "    \n",
    "    # Clean supplier data\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    \n",
    "    # Deduplicate to prevent row explosion - Unique Brand+Store\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "    \n",
    "    # Ensure PO data has clean columns for merging\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "    \n",
    "    # 1. Primary Merge: Match on Brand AND Store (Toko)\n",
    "    # This prioritizes the specific supplier for that store\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "    \n",
    "    # 2. Fallback: For unmatched rows, try to find ANY supplier for that Brand\n",
    "    # Identify rows where merge failed (Nama Brand is NaN)\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    \n",
    "    if unmatched_mask.any():\n",
    "        print(f\"Found {unmatched_mask.sum()} rows without direct store match. Attempting fallback...\")\n",
    "        \n",
    "        # Get the unmatched rows and drop the empty supplier columns\n",
    "        unmatched_rows = merged_df[unmatched_mask].copy()\n",
    "        supplier_cols = [col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand']\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols)\n",
    "        \n",
    "        # Create fallback supplier list (one per brand)\n",
    "        # We take the first one found for each brand\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        \n",
    "        # Merge unmatched rows with fallback suppliers\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "        \n",
    "        # Combine the initially matched rows with the fallback-matched rows\n",
    "        matched_initial = merged_df[~unmatched_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "    \n",
    "    # Clean up supplier columns\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def calculate_inventory_metrics(df_clean, df_special_60):\n",
    "    \"\"\"\n",
    "    Calculate various inventory metrics including safety stock, reorder points, and PO quantities.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Input dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added calculated columns\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "    df = df_clean.copy()\n",
    "    \n",
    "    # Set display options\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "    # Normalise stock column name\n",
    "    stock_col = 'Stok' if 'Stok' in df.columns else 'Stock'\n",
    "\n",
    "    # Force the columns we need into numeric form\n",
    "    numeric_cols = [\n",
    "        stock_col, 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "        'Max. Lead Time', 'Sedang PO', 'HPP', 'Harga', 'sales_contribution'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    try:\n",
    "        # 1. Safety stock calculation\n",
    "        df['Safety stock'] = (df['Max. Daily Sales'] * df['Max. Lead Time']) - (df['Daily Sales'] * df['Lead Time'])\n",
    "        df['Safety stock'] = df['Safety stock'].apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 2. Reorder point calculation\n",
    "        df['Reorder point'] = np.ceil((df['Daily Sales'] * df['Lead Time']) + df['Safety stock']).fillna(0).astype(int)\n",
    "        \n",
    "        # 3. Stock cover for 30 or 60 days based on special SKUs\n",
    "        # Default to 30 days for all SKUs\n",
    "        df['target_days'] = 30\n",
    "        \n",
    "        # 4. Check if we have special SKUs and update their target days to 60\n",
    "        # if df_special_60 is not None and not df_special_60.empty:\n",
    "        #     # Find the SKU column in the main dataframe (case-insensitive)\n",
    "        #     sku_col = next((col for col in df.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "        #     # Find the SKU column in the special SKU dataframe (case-insensitive)\n",
    "        #     special_sku_col = next((col for col in df_special_60.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "        #     if sku_col and special_sku_col:\n",
    "        #         # Convert both to string and strip whitespace for matching\n",
    "        #         df[sku_col] = df[sku_col].astype(str).str.strip()\n",
    "        #         df_special_60[special_sku_col] = df_special_60[special_sku_col].astype(str).str.strip()\n",
    "                \n",
    "        #         # Update target_days to 60 for special SKUs\n",
    "        #         special_skus = set(df_special_60[special_sku_col].unique())\n",
    "        #         df.loc[df[sku_col].isin(special_skus), 'target_days'] = 60\n",
    "        #     else:\n",
    "        #         print(\"Warning: Could not find 'SKU' column in one of the dataframes\")\n",
    "        \n",
    "        # Calculate target days cover based on the determined days\n",
    "        df['target_days_cover'] = (df['Daily Sales'] * df['target_days']).apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        df['current_stock_days_cover'] = np.where(\n",
    "            df['Daily Sales'] > 0,\n",
    "            df[stock_col] / df['Daily Sales'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # 5. Is open PO flag\n",
    "        df['is_open_po'] = np.where(\n",
    "            (df['current_stock_days_cover'] < df['target_days']) & \n",
    "            (df['Stok'] <= df['Reorder point']), 1, 0\n",
    "        )\n",
    "        \n",
    "        # 6. Initial PO quantity\n",
    "        df['initial_qty_po'] = df['target_days_cover'] - df[stock_col] - df.get('Sedang PO', 0)\n",
    "        df['initial_qty_po'] = (\n",
    "            pd.Series(\n",
    "                np.where(df['is_open_po'] == 1, df['initial_qty_po'], 0),\n",
    "                index=df.index\n",
    "            )\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 7. Emergency PO quantity\n",
    "        df['emergency_po_qty'] = np.where(\n",
    "            df.get('Sedang PO', 0) > 0,\n",
    "            np.maximum(0, (df['Max. Lead Time'] - df['current_stock_days_cover']) * df['Daily Sales']),\n",
    "            np.ceil((df['Max. Lead Time'] - df['current_stock_days_cover']) * df['Daily Sales'])\n",
    "        )\n",
    "        \n",
    "        # Clean up emergency PO quantities\n",
    "        df['emergency_po_qty'] = (\n",
    "            df['emergency_po_qty']\n",
    "            .replace([np.inf, -np.inf], 0)\n",
    "            .fillna(0)\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 8. Updated regular PO quantity\n",
    "        df['updated_regular_po_qty'] = (df['initial_qty_po'] - df['emergency_po_qty']).clip(lower=0).astype(int)\n",
    "        \n",
    "        # 9. Final updated regular PO quantity (enforce minimum order)\n",
    "        df['final_updated_regular_po_qty'] = np.where(\n",
    "            (df['updated_regular_po_qty'] > 0) & \n",
    "            (df['updated_regular_po_qty'] < df['Min. Order']),\n",
    "            df['Min. Order'],\n",
    "            df['updated_regular_po_qty']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 10. Calculate costs if by multiplying with contribution percentage\n",
    "        df['emergency_po_cost'] = (df['emergency_po_qty'] * df['HPP']).round(2)\n",
    "        df['final_updated_regular_po_cost'] = (df['final_updated_regular_po_qty'] * df['HPP']).round(2)\n",
    "        \n",
    "        # Clean up any remaining NaN or infinite values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_inventory_metrics: {str(e)}\")\n",
    "        return df_clean\n",
    "\n",
    "def clean_po_data(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP', 'Harga'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP', 'Harga']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP', 'Harga'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert SKU to string and clean it\n",
    "        if 'SKU' in df.columns:\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP', 'Min. Order', 'Harga'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "        df['contribution_ratio'] = contribution_pct / 100\n",
    "\n",
    "\n",
    "        location_upper = location.upper()\n",
    "        exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        print(f\"Processing store: {location} - {contribution_pct}%\")\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        if padang_sales is not None:\n",
    "            # Ensure padang_sales has the required columns\n",
    "            padang_sales = padang_sales.copy()\n",
    "            padang_sales.columns = padang_sales.columns.str.strip()\n",
    "            \n",
    "            # Convert SKU to string in both dataframes\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "            padang_sales['SKU'] = padang_sales['SKU'].astype(str).str.strip()\n",
    "            \n",
    "            padang_skus = set(padang_sales['SKU'].unique())\n",
    "            df['Is in Padang'] = df['SKU'].isin(padang_skus).astype(int)\n",
    "        else:\n",
    "            print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "            df['Is in Padang'] = 0\n",
    "\n",
    "        if not needs_padang_override:\n",
    "            return df\n",
    "\n",
    "        if padang_sales is None:\n",
    "            raise ValueError(\n",
    "                \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "                \"or any store with contribution < 100%.\"\n",
    "            )\n",
    "\n",
    "        # Process Padang sales data\n",
    "        padang_df = padang_sales.copy()\n",
    "        padang_df.columns = padang_df.columns.str.strip()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        missing_cols = [col for col in required_cols if col not in padang_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in Padang sales data: {missing_cols}\")\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        if 'Daily Sales' in df.columns:\n",
    "            df['Orig Daily Sales'] = df['Daily Sales']\n",
    "        if 'Max. Daily Sales' in df.columns:\n",
    "            df['Orig Max. Daily Sales'] = df['Max. Daily Sales']\n",
    "\n",
    "        print(\"Overriding with Padang sales data...\")\n",
    "        \n",
    "        # Ensure SKU is string in both dataframes before merge\n",
    "        df['SKU'] = df['SKU'].astype(str)\n",
    "        padang_df['SKU'] = padang_df['SKU'].astype(str)\n",
    "        \n",
    "        # Merge with Padang's sales data\n",
    "        df = df.merge(\n",
    "            padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "                'Daily Sales': 'Padang Daily Sales',\n",
    "                'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "            }),\n",
    "            on='SKU',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "            df['Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Daily Sales']\n",
    "            )\n",
    "            \n",
    "        if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "            df['Max. Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Max Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Max. Daily Sales']\n",
    "            )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        columns_to_drop = [\n",
    "            'Padang Daily Sales', 'Padang Max Daily Sales',\n",
    "        ]\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        # remove duplicate SKU\n",
    "        df = df.drop_duplicates(subset=['SKU'], keep='first')\n",
    "\n",
    "        # calculate sales contribution\n",
    "        df['sales_contribution'] = df['Daily Sales'] * df['Harga']\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_store_name_from_filename(filename):\n",
    "    \"\"\"Extract store name from filename, handling different patterns.\"\"\"\n",
    "    # Remove file extension and split by spaces\n",
    "    name_parts = Path(filename).stem.split()\n",
    "    \n",
    "    # Handle cases like \"002 Miss Glam Pekanbaru.csv\" -> \"Pekanbaru\"\n",
    "    # or \"01 Miss Glam Padang.csv\" -> \"Padang\"\n",
    "    if len(name_parts) >= 3 and name_parts[1].lower() == 'miss' and name_parts[2].lower() == 'glam':\n",
    "        return ' '.join(name_parts[3:]).strip().upper()\n",
    "    elif len(name_parts) >= 2 and name_parts[0].lower() == 'miss' and name_parts[1].lower() == 'glam':\n",
    "        return ' '.join(name_parts[2:]).strip().upper()\n",
    "    # Fallback: take everything after the first space\n",
    "    elif ' ' in filename:\n",
    "        return ' '.join(name_parts[1:]).strip().upper()\n",
    "    return name_parts[0].upper()\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # List of (separator, encoding) combinations to try\n",
    "    formats_to_try = [\n",
    "        (',', 'utf-8'),      # Standard CSV with comma\n",
    "        (';', 'utf-8'),      # Semicolon with UTF-8\n",
    "        (',', 'latin1'),     # Comma with Latin1\n",
    "        (';', 'latin1'),     # Semicolon with Latin1\n",
    "        (',', 'cp1252'),     # Windows-1252 encoding\n",
    "        (';', 'cp1252')\n",
    "    ]\n",
    "    \n",
    "    for sep, enc in formats_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=sep,\n",
    "                decimal=',',\n",
    "                thousands='.',\n",
    "                encoding=enc,\n",
    "                engine='python'  # More consistent behavior with Python engine\n",
    "            )\n",
    "            # If we get here, the file was read successfully\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError, pd.errors.EmptyDataError) as e:\n",
    "            continue  # Try next format\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading {file_path} with sep='{sep}', encoding='{enc}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we get here, all attempts failed\n",
    "    print(f\"Failed to read {file_path} with any known format\")\n",
    "    return None\n",
    "\n",
    "def process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=False):\n",
    "    \"\"\"Process a single PO file and return merged data and summary.\"\"\"\n",
    "    print(f\"\\nProcessing PO file: {file_path.name} ....\")\n",
    "    \n",
    "    try:\n",
    "        # Extract location from filename using the new function\n",
    "        location = get_store_name_from_filename(file_path.name)\n",
    "        print(f\"  - Extracted location: {location}\")  # Debug print\n",
    "        \n",
    "        contribution_pct = get_contribution_pct(location, store_contrib)\n",
    "        \n",
    "        # Read the CSV with error handling\n",
    "        try:\n",
    "            # Try reading with different encodings if needed\n",
    "            if is_excel_folder:\n",
    "                df = read_excel_file(file_path)\n",
    "            else:\n",
    "                df = read_csv_file(file_path)\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "                \n",
    "            # Clean the data\n",
    "            df_clean = clean_po_data(df,location, contribution_pct, df_padang)\n",
    "\n",
    "            # update sku \n",
    "            \n",
    "            # Skip if cleaning failed\n",
    "            if df_clean.empty:\n",
    "                raise ValueError(\"Data cleaning failed\")\n",
    "        \n",
    "            # calculate metrics PO\n",
    "            df_clean = calculate_inventory_metrics(df_clean, df_special_60)\n",
    "            \n",
    "            # Merge with suppliers\n",
    "            merged_df = merge_with_suppliers(df_clean, supplier_df)\n",
    "\n",
    "            # Generate summary\n",
    "            padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "            other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                                  (merged_df['Nama Store'] != '')).sum()\n",
    "            \n",
    "            summary = {\n",
    "                'file': file_path.name,\n",
    "                'location': location,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'total_rows': len(merged_df),\n",
    "                'padang_suppliers': int(padang_count),\n",
    "                'other_suppliers': int(other_supplier_count),\n",
    "                'no_supplier': int((merged_df['Nama Store'] == '').sum()),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            return merged_df, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file data: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {file_path.name}: {str(e)}\"\n",
    "        print(f\"  - {error_msg}\")\n",
    "        return None, {\n",
    "            'file': file_path.name,\n",
    "            'location': location if 'location' in locals() else 'Unknown',\n",
    "            'contribution_pct': contribution_pct if 'contribution_pct' in locals() else 0,\n",
    "            'total_rows': 0,\n",
    "            'padang_suppliers': 0,\n",
    "            'other_suppliers': 0,\n",
    "            'no_supplier': 0,\n",
    "            'status': f\"Error: {str(e)[:100]}\"  # Truncate long error messages\n",
    "        }\n",
    "\n",
    "def load_padang_data(padang_path):\n",
    "    \"\"\"Load Padang data from either CSV or Excel file.\n",
    "    \n",
    "    Args:\n",
    "        padang_path: Path to the input file (CSV or XLSX)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and cleaned Padang data\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported or file cannot be read\n",
    "    \"\"\"\n",
    "    print(f\"Loading Padang data from {padang_path}...\")\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = str(padang_path).lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(padang_path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(padang_path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Padang data with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Padang data from {padang_path}: {str(e)}\")\n",
    "\n",
    "def format_number_for_csv(x):\n",
    "    \"\"\"Format numbers for CSV output with Indonesian locale (comma as decimal, dot as thousand)\"\"\"\n",
    "    if pd.isna(x) or x == '':\n",
    "        return x\n",
    "    try:\n",
    "        if isinstance(x, (int, float)):\n",
    "            if x == int(x):  # Whole number\n",
    "                return f\"{int(x):,d}\".replace(\",\", \".\")\n",
    "            else:  # Decimal number\n",
    "                return f\"{x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def clean_and_convert(df):\n",
    "    \"\"\"Clean and convert DataFrame columns to appropriate types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string first to handle NaN/None consistently\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Define NA values that should be treated as empty/missing\n",
    "    na_values = list(NA_VALUES)\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        # Replace NA values with empty string first (treating them as literals, not regex)\n",
    "        df[col] = df[col].replace(na_values, '', regex=False)\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if df[col].empty:\n",
    "            continue\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if col in NUMERIC_COLUMNS:\n",
    "            # Convert to numeric, coercing errors to NaN, then fill with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            # For non-numeric columns, ensure they're strings and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace empty strings with NaN and then fill with empty string\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('').infer_objects(copy=False)\n",
    "            df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            df[col] = df[col].infer_objects(copy=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file with robust error handling for problematic values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nReading excel file: {file_path.name}...\")\n",
    "        \n",
    "        # First, read the file with openpyxl directly to handle the data more carefully\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = load_workbook(\n",
    "            filename=file_path,\n",
    "            read_only=True,    # Read-only mode is faster and uses less memory\n",
    "            data_only=True,    # Get the stored value instead of the formula\n",
    "            keep_links=False   # Don't load external links\n",
    "        )\n",
    "        \n",
    "        # Get the first sheet\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Get headers from the first row\n",
    "        headers = []\n",
    "        for idx, cell in enumerate(next(ws.iter_rows(values_only=True))):\n",
    "            header = str(cell).strip() if cell not in (None, '') else f\"Column_{idx + 1}\"\n",
    "            headers.append(header)\n",
    "        \n",
    "        # Initialize data rows\n",
    "        data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for row in ws.iter_rows(min_row=2, values_only=True):  # Skip header row\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                if cell is None:\n",
    "                    row_data.append('')\n",
    "                    continue\n",
    "\n",
    "                cell_str = str(cell).strip()\n",
    "                if cell_str.upper() in NA_VALUES:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    row_data.append(cell_str)\n",
    "            \n",
    "            # Only add row if it has data\n",
    "            if any(cell != '' for cell in row_data):\n",
    "                data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Normalize column data types\n",
    "        df = clean_and_convert(df)\n",
    "        \n",
    "        print(f\"✅ Successfully processed {file_path.name} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path.name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_file(df, file_path, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame to file with consistent extension and content type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path object or string for the output file\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments to pass to to_csv or to_excel\n",
    "        \n",
    "    Returns:\n",
    "        Path: The path where the file was saved\n",
    "    \"\"\"\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the correct file extension\n",
    "    if not file_path.suffix.lower() == f'.{file_format}':\n",
    "        file_path = file_path.with_suffix(f'.{file_format}')\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_output = df.copy()\n",
    "    \n",
    "    # Common preprocessing\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].astype(str).str.strip()\n",
    "        if file_format == 'xlsx':\n",
    "            # For Excel, wrap SKU in =\"...\" to preserve leading zeros\n",
    "            df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Format numbers for CSV if needed\n",
    "    if file_format == 'csv':\n",
    "        numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "    \n",
    "    # Save based on format\n",
    "    if file_format == 'csv':\n",
    "        df_output.to_csv(\n",
    "            file_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig',\n",
    "            **kwargs\n",
    "        )\n",
    "    elif file_format == 'xlsx':\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "            df_output.to_excel(writer, index=False, **kwargs)\n",
    "            \n",
    "            # Format SKU column as text in Excel\n",
    "            if 'SKU' in df_output.columns:\n",
    "                ws = writer.sheets[list(writer.sheets.keys())[0]]\n",
    "                sku_col_idx = df_output.columns.get_loc(\"SKU\") + 1\n",
    "                for row in ws.iter_rows(\n",
    "                    min_row=2,  # Skip header\n",
    "                    max_row=ws.max_row,\n",
    "                    min_col=sku_col_idx,\n",
    "                    max_col=sku_col_idx\n",
    "                ):\n",
    "                    for cell in row:\n",
    "                        cell.number_format = numbers.FORMAT_TEXT\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "    \n",
    "    print(f\"File saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_to_complete_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save Complete format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "\n",
    "    return save_file(df, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_m2_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save M2 format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_filtered = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[['Toko', 'SKU', 'HPP', 'final_updated_regular_po_qty']]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_M2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_M2_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_emergency_po_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save emergency PO format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with emergency PO qty > 0\n",
    "    df_filtered = df[df['emergency_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[[\n",
    "        'Brand', 'SKU', 'Nama', 'Toko', 'HPP', \n",
    "        'emergency_po_qty', 'emergency_po_cost'\n",
    "    ]]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_EMERGENCY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_EMERGENCY_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    supplier_df = load_supplier_data(SUPPLIER_PATH)\n",
    "    store_contrib = load_store_contribution(STORE_CONTRIBUTION_PATH)\n",
    "    all_summaries = []\n",
    "\n",
    "    # get padang df first\n",
    "    df_padang = load_padang_data(BASE_DIR / 'data/rawpo/xlsx/1. Miss glam Padang.xlsx')\n",
    "\n",
    "    # test_xlsx_convert()\n",
    "\n",
    "    # Process each PO file\n",
    "    for file_path in sorted(RAWPO_XLSX_DIR.glob('*.xlsx')):\n",
    "        try:\n",
    "            merged_df, summary = process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=True)\n",
    "\n",
    "            save_to_complete_format(merged_df, file_path.name, file_format='xlsx')\n",
    "            save_to_m2_format(merged_df, file_path.name)\n",
    "            save_to_emergency_po_format(merged_df, file_path.name)\n",
    "\n",
    "            # summary['output_path'] = str(output_path)\n",
    "            output_path = OUTPUT_DIR / file_path.name\n",
    "            summary['output_path'] = str(output_path)\n",
    "\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  - Location: {summary['location']}\")\n",
    "            print(f\"  - Contribution: {summary['contribution_pct']}%\")\n",
    "            print(f\"  - Rows processed: {summary['total_rows']}\")\n",
    "            print(f\"  - 'Miss Glam Padang' suppliers: {summary['padang_suppliers']} rows\")\n",
    "            print(f\"  - Other suppliers: {summary['other_suppliers']} rows\")\n",
    "            print(f\"  - No supplier data: {summary['no_supplier']} rows\")\n",
    "            print(f\"  - Saved to: {output_path}\")\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Display final summary\n",
    "    if all_summaries:\n",
    "        print(\"\\nProcessing complete! Summary:\")\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Show sample of last processed file\n",
    "        print(\"\\nSample of the last processed file:\")\n",
    "        display(merged_df)\n",
    "    else:\n",
    "        print(\"\\nNo files were processed successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
