{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26400c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468e9d6",
   "metadata": {},
   "source": [
    "# Stock health Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6ba5",
   "metadata": {},
   "source": [
    "# Final batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce491334",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = [\n",
    "    'HPP', 'Harga', 'Ranking', 'Grade', 'Terjual', 'Stok', 'Lost Days',\n",
    "    'Velocity Capped', 'Daily Sales', 'Lead Time', 'Max. Daily Sales',\n",
    "    'Max. Lead Time', 'Min. Order', 'Safety Stok', 'ROP', '3W Cover',\n",
    "    'Sedang PO', 'Suggested', 'Amount', 'Promo Factor', 'Delay Factor',\n",
    "    'Stock Cover', 'Days to Backup', 'Qty to Backup'\n",
    "]\n",
    "\n",
    "NA_VALUES = {\n",
    "    'NAN', 'NA', '#N/A', 'NULL', 'NONE', '', '?', '-', 'INF', '-INF',\n",
    "    '+INF', 'INFINITY', '-INFINITY', '1.#INF', '-1.#INF', '1.#QNAN'\n",
    "}\n",
    "\n",
    "def _patch_openpyxl_number_casting():\n",
    "    \"\"\"Ensure openpyxl won't crash when encountering NAN/INF in numeric cells.\"\"\"\n",
    "    print(\"Calling _patch_openpyxl_number_casting...\")\n",
    "\n",
    "    try:\n",
    "        from openpyxl.worksheet import _reader\n",
    "\n",
    "        original_cast = _reader._cast_number\n",
    "\n",
    "        def _safe_cast_number(value):  # pragma: no cover - monkey patch\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().upper() in NA_VALUES:\n",
    "                    return 0\n",
    "            try:\n",
    "                return original_cast(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0 if value in (None, '') else value\n",
    "\n",
    "        _reader._cast_number = _safe_cast_number\n",
    "    except Exception:\n",
    "        # If patch fails we continue; runtime reader will still attempt default behaviour\n",
    "        pass\n",
    "\n",
    "def load_special_sku_60(path):\n",
    "    print(f\"Loading Special SKU with 60 days target cover data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check file extension\n",
    "        file_ext = str(path).lower().split('.')[-1]\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Special SKU with 60 days target cover data with {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Special SKU with 60 days target cover data from {path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x104035e40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py\", line 1980, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py\", line 1997, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _patch_openpyxl_number_casting...\n",
      "Loading Special SKU with 60 days target cover data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/special_sku_60.csv...\n",
      "Successfully loaded Special SKU with 60 days target cover data with 40 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama Produk</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8999999595357</td>\n",
       "      <td>DOVE Perawatan Rambut Rontok Hair Tonic Spray ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8999999584207</td>\n",
       "      <td>DOVE Deep Cleanse Micellar Shampo Himalaya Sal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8999999526344</td>\n",
       "      <td>TRESEMME Shampoo Hair Fall Tresplex 170ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40200509458</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Pink 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40200509242</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Kuning 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40200509360</td>\n",
       "      <td>SUNSILK Multivitamin Hair Parfume Ungu 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8999999540159</td>\n",
       "      <td>VASELINE Repairing Jelly Aloevera 50ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8999999559588</td>\n",
       "      <td>VASELINE Body Lotion Serum Soft Glow 180ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8999999502942</td>\n",
       "      <td>VASELINE Repairing Jelly 50ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8999999035273</td>\n",
       "      <td>VASELINE Healthy Bright Spf 30 PA++ Gluta Vita...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8999999577421</td>\n",
       "      <td>VASELINE Healthy Bright Hijab Protect Gluta Vi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8999999502973</td>\n",
       "      <td>VASELINE Repairing Jelly 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8999999559595</td>\n",
       "      <td>VASELINE Body Serum Fiem Glow 180ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8999999577407</td>\n",
       "      <td>VASELINE Healthy Bright Radiant Gluta Vitamin ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30100244316</td>\n",
       "      <td>LUX Mood Library Peaceful Galaxy Body Scrub 360gr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30100167658</td>\n",
       "      <td>LUX Mood Library Peaceful Galaxy Shower Gel 470gr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10100507517</td>\n",
       "      <td>THE ORIGINOTE Micellar Cleansing Tissue 10pcs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10100724414</td>\n",
       "      <td>THE ORIGINOTE Hyalu BHA Acne Micellar Water 300ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20101207602</td>\n",
       "      <td>SEA MAKEUP Stayput Prime &amp; Set Acne Setting Sp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>725765154392</td>\n",
       "      <td>SEA MAKEUP Lock It Matte Acne Setting Spray 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>745604686181</td>\n",
       "      <td>GRACE AND GLOW Bightening Sun Body Serum 100ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30200424556</td>\n",
       "      <td>GRACE AND GLOW Whitw B-3 Bright Body Gel Serum...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40100204145</td>\n",
       "      <td>GRACE AND GLOW Velvet Breeze Dry Shampoo 150ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40100204024</td>\n",
       "      <td>GRACE AND GLOW Daisy Breeze Dry Shampoo 150ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30400519016</td>\n",
       "      <td>GRACE AND GLOW Invisible Smooth Antiperspirant...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8997230373085</td>\n",
       "      <td>DEAR ME BEAUTY Spf 50 + Skin Barrier Sunscreen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8997230373894</td>\n",
       "      <td>DEAR ME BEAUTY Serum Lip Tint Dear Vania 3.5ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8999908082800</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8999908266101</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8999908082701</td>\n",
       "      <td>MARINA Hand Body Lotion Uv White Healthy &amp; Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8999908805508</td>\n",
       "      <td>MARINA UV White Sunblock Spf 30 PA++ Hand &amp; Bo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8999908618702</td>\n",
       "      <td>MARINA Healty and Glow Body Scrub 200ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8999999581930</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Flawless Bri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8999999581947</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Dewy Radianc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9556126670654</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Flawless Glo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8999999719418</td>\n",
       "      <td>VASELINE Body Lotion Healty Bright Uv Extra Br...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8999999590055</td>\n",
       "      <td>VASELINE Healty Burst Lotion GLUTA-HYA Overnig...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8851932417464</td>\n",
       "      <td>VASELINE Daily Sun Refreshing SPF50+ 170ml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8999999003043</td>\n",
       "      <td>VASELINE Healty Bright Sun Pollution Spf24 Pro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8999999596972</td>\n",
       "      <td>VASELINE Healthy Bright Gluta-Hya Serum Burst ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SKU                                        Nama Produk  \\\n",
       "0   8999999595357  DOVE Perawatan Rambut Rontok Hair Tonic Spray ...   \n",
       "1   8999999584207  DOVE Deep Cleanse Micellar Shampo Himalaya Sal...   \n",
       "2   8999999526344          TRESEMME Shampoo Hair Fall Tresplex 170ml   \n",
       "3     40200509458       SUNSILK Multivitamin Hair Parfume Pink 100ml   \n",
       "4     40200509242     SUNSILK Multivitamin Hair Parfume Kuning 100ml   \n",
       "5     40200509360       SUNSILK Multivitamin Hair Parfume Ungu 100ml   \n",
       "6   8999999540159             VASELINE Repairing Jelly Aloevera 50ml   \n",
       "7   8999999559588         VASELINE Body Lotion Serum Soft Glow 180ml   \n",
       "8   8999999502942                      VASELINE Repairing Jelly 50ml   \n",
       "9   8999999035273  VASELINE Healthy Bright Spf 30 PA++ Gluta Vita...   \n",
       "10  8999999577421  VASELINE Healthy Bright Hijab Protect Gluta Vi...   \n",
       "11  8999999502973                     VASELINE Repairing Jelly 100ml   \n",
       "12  8999999559595                VASELINE Body Serum Fiem Glow 180ml   \n",
       "13  8999999577407  VASELINE Healthy Bright Radiant Gluta Vitamin ...   \n",
       "14    30100244316  LUX Mood Library Peaceful Galaxy Body Scrub 360gr   \n",
       "15    30100167658  LUX Mood Library Peaceful Galaxy Shower Gel 470gr   \n",
       "16    10100507517      THE ORIGINOTE Micellar Cleansing Tissue 10pcs   \n",
       "17    10100724414  THE ORIGINOTE Hyalu BHA Acne Micellar Water 300ml   \n",
       "18    20101207602  SEA MAKEUP Stayput Prime & Set Acne Setting Sp...   \n",
       "19   725765154392  SEA MAKEUP Lock It Matte Acne Setting Spray 100ml   \n",
       "20   745604686181     GRACE AND GLOW Bightening Sun Body Serum 100ml   \n",
       "21    30200424556  GRACE AND GLOW Whitw B-3 Bright Body Gel Serum...   \n",
       "22    40100204145     GRACE AND GLOW Velvet Breeze Dry Shampoo 150ml   \n",
       "23    40100204024      GRACE AND GLOW Daisy Breeze Dry Shampoo 150ml   \n",
       "24    30400519016  GRACE AND GLOW Invisible Smooth Antiperspirant...   \n",
       "25  8997230373085  DEAR ME BEAUTY Spf 50 + Skin Barrier Sunscreen...   \n",
       "26  8997230373894     DEAR ME BEAUTY Serum Lip Tint Dear Vania 3.5ml   \n",
       "27  8999908082800  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "28  8999908266101  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "29  8999908082701  MARINA Hand Body Lotion Uv White Healthy & Glo...   \n",
       "30  8999908805508  MARINA UV White Sunblock Spf 30 PA++ Hand & Bo...   \n",
       "31  8999908618702            MARINA Healty and Glow Body Scrub 200ml   \n",
       "32  8999999581930  VASELINE Healthy Bright Gluta-Hya Flawless Bri...   \n",
       "33  8999999581947  VASELINE Healthy Bright Gluta-Hya Dewy Radianc...   \n",
       "34  9556126670654  VASELINE Healthy Bright Gluta-Hya Flawless Glo...   \n",
       "35  8999999719418  VASELINE Body Lotion Healty Bright Uv Extra Br...   \n",
       "36  8999999590055  VASELINE Healty Burst Lotion GLUTA-HYA Overnig...   \n",
       "37  8851932417464         VASELINE Daily Sun Refreshing SPF50+ 170ml   \n",
       "38  8999999003043  VASELINE Healty Bright Sun Pollution Spf24 Pro...   \n",
       "39  8999999596972  VASELINE Healthy Bright Gluta-Hya Serum Burst ...   \n",
       "\n",
       "    Unnamed: 2  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         NaN  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "18         NaN  \n",
       "19         NaN  \n",
       "20         NaN  \n",
       "21         NaN  \n",
       "22         NaN  \n",
       "23         NaN  \n",
       "24         NaN  \n",
       "25         NaN  \n",
       "26         NaN  \n",
       "27         NaN  \n",
       "28         NaN  \n",
       "29         NaN  \n",
       "30         NaN  \n",
       "31         NaN  \n",
       "32         NaN  \n",
       "33         NaN  \n",
       "34         NaN  \n",
       "35         NaN  \n",
       "36         NaN  \n",
       "37         NaN  \n",
       "38         NaN  \n",
       "39         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading supplier data: /Users/andresuchitra/dev/missglam/autopo/notebook/data/supplier.csv\n",
      "Loading Padang data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251204/1. MG Padang.xlsx...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error loading Padang data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251204/1. MG Padang.xlsx: [Errno 2] No such file or directory: '/Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251204/1. MG Padang.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 597\u001b[39m, in \u001b[36mload_padang_data\u001b[39m\u001b[34m(padang_path)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file_ext \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mxlsx\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    596\u001b[39m     \u001b[38;5;66;03m# Read Excel file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadang_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenpyxl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/pandas/io/excel/_base.py:1567\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1565\u001b[39m \u001b[38;5;28mself\u001b[39m.storage_options = storage_options\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/pandas/io/excel/_openpyxl.py:553\u001b[39m, in \u001b[36mOpenpyxlReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mopenpyxl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/pandas/io/excel/_base.py:563\u001b[39m, in \u001b[36mBaseExcelReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, (ExcelFile, \u001b[38;5;28mself\u001b[39m._workbook_class)):\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handles.handle, \u001b[38;5;28mself\u001b[39m._workbook_class):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/missglam/.venv/lib/python3.13/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m handles.append(handle)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251204/1. MG Padang.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 928\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 880\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    877\u001b[39m CURRENT_DIR = INPUT_DIR / current_date\n\u001b[32m    879\u001b[39m \u001b[38;5;66;03m# get padang df first\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m df_padang = \u001b[43mload_padang_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCURRENT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1. MG Padang.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# test_xlsx_convert()\u001b[39;00m\n\u001b[32m    883\u001b[39m \n\u001b[32m    884\u001b[39m \u001b[38;5;66;03m# Process each PO file\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CURRENT_DIR.glob(\u001b[33m'\u001b[39m\u001b[33m*.xlsx\u001b[39m\u001b[33m'\u001b[39m)):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 617\u001b[39m, in \u001b[36mload_padang_data\u001b[39m\u001b[34m(padang_path)\u001b[39m\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading Padang data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadang_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Error loading Padang data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251204/1. MG Padang.xlsx: [Errno 2] No such file or directory: '/Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251204/1. MG Padang.xlsx'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import display\n",
    "from locale import atof\n",
    "import numpy as np\n",
    "from openpyxl.styles import numbers\n",
    "from datetime import datetime\n",
    "\n",
    "_patch_openpyxl_number_casting()\n",
    "\n",
    "# Apply the formatting to numeric columns in your final output\n",
    "def format_dataframe_display(df):\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # Apply formatting to numeric columns\n",
    "    for col in df_display.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: format_id_number(x, 2) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/Users/andresuchitra/dev/missglam/autopo/notebook')\n",
    "SUPPLIER_PATH = BASE_DIR / 'data/supplier.csv'\n",
    "RAWPO_DIR = BASE_DIR / 'data/rawpo/csv'\n",
    "INPUT_DIR = BASE_DIR / 'data/input'\n",
    "RAWPO_XLSX_DIR = BASE_DIR / 'data/rawpo/xlsx'\n",
    "STORE_CONTRIBUTION_PATH = BASE_DIR / 'data/store_contribution.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'output/complete'\n",
    "OUTPUT_EXCEL_DIR = BASE_DIR / 'output/excel'\n",
    "OUTPUT_M2_DIR = BASE_DIR / 'output/m2'\n",
    "OUTPUT_EMERGENCY_DIR = BASE_DIR / 'output/emergency'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EXCEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_M2_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EMERGENCY_DIR, exist_ok=True)\n",
    "\n",
    "df_special_60 = load_special_sku_60(BASE_DIR / 'data/special_sku_60.csv')\n",
    "\n",
    "display(df_special_60)\n",
    "\n",
    "def load_store_contribution(store_contribution_path):\n",
    "    \"\"\"Load and prepare store contribution data.\"\"\"\n",
    "    store_contrib = pd.read_csv(store_contribution_path, header=None, \n",
    "                              names=['store', 'contribution_pct'])\n",
    "    # Convert store names to lowercase for case-insensitive matching\n",
    "    store_contrib['store_lower'] = store_contrib['store'].str.lower()\n",
    "    return store_contrib\n",
    "\n",
    "def get_contribution_pct(location, store_contrib):\n",
    "    \"\"\"Get contribution percentage for a given location.\"\"\"\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    contrib_row = store_contrib[store_contrib['store_lower'] == location_lower]\n",
    "    if not contrib_row.empty:\n",
    "        return contrib_row['contribution_pct'].values[0]\n",
    "    print(f\"Warning: No contribution percentage found for {location}\")\n",
    "\n",
    "    return 100  # Default to 100% if not found\n",
    "\n",
    "def load_supplier_data(supplier_path):\n",
    "    \"\"\"Load and clean supplier data.\"\"\"\n",
    "    print(f\"Loading supplier data: {supplier_path}\")\n",
    "    df = pd.read_csv(supplier_path, sep=';', decimal=',').fillna('')\n",
    "    df['Nama Brand'] = df['Nama Brand'].str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier information.\"\"\"\n",
    "    print(\"Merging with suppliers...\")\n",
    "    \n",
    "    # Clean supplier data\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    \n",
    "    # Deduplicate to prevent row explosion - Unique Brand+Store\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "    \n",
    "    # Ensure PO data has clean columns for merging\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "    \n",
    "    # 1. Primary Merge: Match on Brand AND Store (Toko)\n",
    "    # This prioritizes the specific supplier for that store\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "    \n",
    "    # 2. Fallback: For unmatched rows, try to find ANY supplier for that Brand\n",
    "    # Identify rows where merge failed (Nama Brand is NaN)\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    \n",
    "    if unmatched_mask.any():\n",
    "        print(f\"Found {unmatched_mask.sum()} rows without direct store match. Attempting fallback...\")\n",
    "        \n",
    "        # Get the unmatched rows and drop the empty supplier columns\n",
    "        unmatched_rows = merged_df[unmatched_mask].copy()\n",
    "        supplier_cols = [col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand']\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols)\n",
    "        \n",
    "        # Create fallback supplier list (one per brand)\n",
    "        # We take the first one found for each brand\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        \n",
    "        # Merge unmatched rows with fallback suppliers\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "        \n",
    "        # Combine the initially matched rows with the fallback-matched rows\n",
    "        matched_initial = merged_df[~unmatched_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "    \n",
    "    # Clean up supplier columns\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def calculate_inventory_metrics(df_clean, df_special_60):\n",
    "    \"\"\"\n",
    "    Calculate various inventory metrics including safety stock, reorder points, and PO quantities.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Input dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added calculated columns\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "    df = df_clean.copy()\n",
    "    \n",
    "    # Set display options\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "    # Normalise stock column name\n",
    "    stock_col = 'Stok' if 'Stok' in df.columns else 'Stock'\n",
    "\n",
    "    # Force the columns we need into numeric form\n",
    "    numeric_cols = [\n",
    "        stock_col, 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "        'Max. Lead Time', 'Sedang PO', 'HPP', 'Harga', 'sales_contribution'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    try:\n",
    "        # 1. Safety stock calculation\n",
    "        df['Safety stock'] = (df['Max. Daily Sales'] * df['Max. Lead Time']) - (df['Daily Sales'] * df['Lead Time'])\n",
    "        df['Safety stock'] = df['Safety stock'].apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 2. Reorder point calculation\n",
    "        df['Reorder point'] = np.ceil((df['Daily Sales'] * df['Lead Time']) + df['Safety stock']).fillna(0).astype(int)\n",
    "        \n",
    "        # 3. Stock cover for 30 or 60 days based on special SKUs\n",
    "        # Default to 30 days for all SKUs\n",
    "        df['target_days_cover'] = 30\n",
    "        \n",
    "        # 4. Check if we have special SKUs and update their target days to 60\n",
    "        if df_special_60 is not None and not df_special_60.empty:\n",
    "            # Find the SKU column in the main dataframe (case-insensitive)\n",
    "            sku_col = next((col for col in df.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "            # Find the SKU column in the special SKU dataframe (case-insensitive)\n",
    "            special_sku_col = next((col for col in df_special_60.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "            if sku_col and special_sku_col:\n",
    "                # Convert both to string and strip whitespace for matching\n",
    "                df[sku_col] = df[sku_col].astype(str).str.strip()\n",
    "                df_special_60[special_sku_col] = df_special_60[special_sku_col].astype(str).str.strip()\n",
    "                \n",
    "                # Update target_days_unit to 60 for special SKUs\n",
    "                special_skus = set(df_special_60[special_sku_col].unique())\n",
    "                df.loc[df[sku_col].isin(special_skus), 'target_days_cover'] = 60\n",
    "            else:\n",
    "                print(\"Warning: Could not find 'SKU' column in one of the dataframes\")\n",
    "        \n",
    "        # Calculate target days cover based on the determined days\n",
    "        df['qty_for_target_days_cover'] = (df['Daily Sales'] * df['target_days_cover']).apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        df['current_days_stock_cover'] = np.where(\n",
    "            df['Daily Sales'] > 0,\n",
    "            df[stock_col] / df['Daily Sales'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # 5. Is open PO flag\n",
    "        df['is_open_po'] = np.where(\n",
    "            (df['current_days_stock_cover'] < df['target_days_cover']) & \n",
    "            (df['Stok'] <= df['Reorder point']), 1, 0\n",
    "        )\n",
    "        \n",
    "        # 6. Initial PO quantity\n",
    "        df['initial_qty_po'] = df['qty_for_target_days_cover'] - df[stock_col] - df.get('Sedang PO', 0)\n",
    "        df['initial_qty_po'] = (\n",
    "            pd.Series(\n",
    "                np.where(df['is_open_po'] == 1, df['initial_qty_po'], 0),\n",
    "                index=df.index\n",
    "            )\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 7. Emergency PO quantity\n",
    "        df['emergency_po_qty'] = np.where(\n",
    "            df.get('Sedang PO', 0) > 0,\n",
    "            np.maximum(0, (df['Max. Lead Time'] - df['current_days_stock_cover']) * df['Daily Sales']),\n",
    "            np.ceil((df['Max. Lead Time'] - df['current_days_stock_cover']) * df['Daily Sales'])\n",
    "        )\n",
    "        \n",
    "        # Clean up emergency PO quantities\n",
    "        df['emergency_po_qty'] = (\n",
    "            df['emergency_po_qty']\n",
    "            .replace([np.inf, -np.inf], 0)\n",
    "            .fillna(0)\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 8. Updated regular PO quantity\n",
    "        df['updated_regular_po_qty'] = (df['initial_qty_po'] - df['emergency_po_qty']).clip(lower=0).astype(int)\n",
    "        \n",
    "        # 9. Final updated regular PO quantity (enforce minimum order)\n",
    "        df['final_updated_regular_po_qty'] = np.where(\n",
    "            (df['updated_regular_po_qty'] > 0) & \n",
    "            (df['updated_regular_po_qty'] < df['Min. Order']),\n",
    "            df['Min. Order'],\n",
    "            df['updated_regular_po_qty']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 10. Calculate costs if by multiplying with contribution percentage\n",
    "        df['emergency_po_cost'] = (df['emergency_po_qty'] * df['HPP']).round(2)\n",
    "        df['final_updated_regular_po_cost'] = (df['final_updated_regular_po_qty'] * df['HPP']).round(2)\n",
    "        \n",
    "        # Clean up any remaining NaN or infinite values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_inventory_metrics: {str(e)}\")\n",
    "        return df_clean\n",
    "\n",
    "def clean_po_data(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP', 'Harga'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP', 'Harga']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP', 'Harga'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert SKU to string and clean it\n",
    "        if 'SKU' in df.columns:\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP', 'Min. Order', 'Harga'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "        df['contribution_ratio'] = contribution_pct / 100\n",
    "\n",
    "\n",
    "        location_upper = location.upper()\n",
    "        exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        print(f\"Processing store: {location} - {contribution_pct}%\")\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        if padang_sales is not None:\n",
    "            # Ensure padang_sales has the required columns\n",
    "            padang_sales = padang_sales.copy()\n",
    "            padang_sales.columns = padang_sales.columns.str.strip()\n",
    "            \n",
    "            # Convert SKU to string in both dataframes\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "            padang_sales['SKU'] = padang_sales['SKU'].astype(str).str.strip()\n",
    "            \n",
    "            padang_skus = set(padang_sales['SKU'].unique())\n",
    "            df['Is in Padang'] = df['SKU'].isin(padang_skus).astype(int)\n",
    "        else:\n",
    "            print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "            df['Is in Padang'] = 0\n",
    "\n",
    "        if not needs_padang_override:\n",
    "            return df\n",
    "\n",
    "        if padang_sales is None:\n",
    "            raise ValueError(\n",
    "                \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "                \"or any store with contribution < 100%.\"\n",
    "            )\n",
    "\n",
    "        # Process Padang sales data\n",
    "        padang_df = padang_sales.copy()\n",
    "        padang_df.columns = padang_df.columns.str.strip()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        missing_cols = [col for col in required_cols if col not in padang_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in Padang sales data: {missing_cols}\")\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        if 'Daily Sales' in df.columns:\n",
    "            df['Orig Daily Sales'] = df['Daily Sales']\n",
    "        if 'Max. Daily Sales' in df.columns:\n",
    "            df['Orig Max. Daily Sales'] = df['Max. Daily Sales']\n",
    "\n",
    "        print(\"Overriding with Padang sales data...\")\n",
    "        \n",
    "        # Ensure SKU is string in both dataframes before merge\n",
    "        df['SKU'] = df['SKU'].astype(str)\n",
    "        padang_df['SKU'] = padang_df['SKU'].astype(str)\n",
    "        \n",
    "        # Merge with Padang's sales data\n",
    "        df = df.merge(\n",
    "            padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "                'Daily Sales': 'Padang Daily Sales',\n",
    "                'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "            }),\n",
    "            on='SKU',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "            df['Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Daily Sales']\n",
    "            )\n",
    "            \n",
    "        if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "            df['Max. Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Max Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Max. Daily Sales']\n",
    "            )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        columns_to_drop = [\n",
    "            'Padang Daily Sales', 'Padang Max Daily Sales',\n",
    "        ]\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        # remove duplicate SKU\n",
    "        df = df.drop_duplicates(subset=['SKU'], keep='first')\n",
    "\n",
    "        # calculate sales contribution\n",
    "        df['sales_contribution'] = df['Daily Sales'] * df['Harga']\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_store_name_from_filename(filename):\n",
    "    \"\"\"Extract store name from filename, handling different patterns.\"\"\"\n",
    "    # Remove file extension and split by spaces\n",
    "    name_parts = Path(filename).stem.split()\n",
    "    \n",
    "    # Handle cases like \"002 Miss Glam Pekanbaru.csv\" -> \"Pekanbaru\"\n",
    "    # or \"01 Miss Glam Padang.csv\" -> \"Padang\"\n",
    "    if len(name_parts) >= 3 and name_parts[1].lower() == 'miss' and name_parts[2].lower() == 'glam':\n",
    "        return ' '.join(name_parts[3:]).strip().upper()\n",
    "    elif len(name_parts) >= 2 and name_parts[0].lower() == 'miss' and name_parts[1].lower() == 'glam':\n",
    "        return ' '.join(name_parts[2:]).strip().upper()\n",
    "    # Fallback: take everything after the first space\n",
    "    elif ' ' in filename:\n",
    "        return ' '.join(name_parts[1:]).strip().upper()\n",
    "    return name_parts[0].upper()\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # List of (separator, encoding) combinations to try\n",
    "    formats_to_try = [\n",
    "        (',', 'utf-8'),      # Standard CSV with comma\n",
    "        (';', 'utf-8'),      # Semicolon with UTF-8\n",
    "        (',', 'latin1'),     # Comma with Latin1\n",
    "        (';', 'latin1'),     # Semicolon with Latin1\n",
    "        (',', 'cp1252'),     # Windows-1252 encoding\n",
    "        (';', 'cp1252')\n",
    "    ]\n",
    "    \n",
    "    for sep, enc in formats_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=sep,\n",
    "                decimal=',',\n",
    "                thousands='.',\n",
    "                encoding=enc,\n",
    "                engine='python'  # More consistent behavior with Python engine\n",
    "            )\n",
    "            # If we get here, the file was read successfully\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError, pd.errors.EmptyDataError) as e:\n",
    "            continue  # Try next format\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading {file_path} with sep='{sep}', encoding='{enc}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we get here, all attempts failed\n",
    "    print(f\"Failed to read {file_path} with any known format\")\n",
    "    return None\n",
    "\n",
    "def process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=False):\n",
    "    \"\"\"Process a single PO file and return merged data and summary.\"\"\"\n",
    "    print(f\"\\nProcessing PO file: {file_path.name} ....\")\n",
    "    \n",
    "    try:\n",
    "        # Extract location from filename using the new function\n",
    "        location = get_store_name_from_filename(file_path.name)\n",
    "        print(f\"  - Extracted location: {location}\")  # Debug print\n",
    "        \n",
    "        contribution_pct = get_contribution_pct(location, store_contrib)\n",
    "        \n",
    "        # Read the CSV with error handling\n",
    "        try:\n",
    "            # Try reading with different encodings if needed\n",
    "            if is_excel_folder:\n",
    "                df = read_excel_file(file_path)\n",
    "            else:\n",
    "                df = read_csv_file(file_path)\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "                \n",
    "            # Clean the data\n",
    "            df_clean = clean_po_data(df,location, contribution_pct, df_padang)\n",
    "\n",
    "            # update sku \n",
    "            \n",
    "            # Skip if cleaning failed\n",
    "            if df_clean.empty:\n",
    "                raise ValueError(\"Data cleaning failed\")\n",
    "        \n",
    "            # calculate metrics PO\n",
    "            df_clean = calculate_inventory_metrics(df_clean, df_special_60)\n",
    "            \n",
    "            # Merge with suppliers\n",
    "            merged_df = merge_with_suppliers(df_clean, supplier_df)\n",
    "\n",
    "            # Generate summary\n",
    "            padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "            other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                                  (merged_df['Nama Store'] != '')).sum()\n",
    "            \n",
    "            summary = {\n",
    "                'file': file_path.name,\n",
    "                'location': location,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'total_rows': len(merged_df),\n",
    "                'padang_suppliers': int(padang_count),\n",
    "                'other_suppliers': int(other_supplier_count),\n",
    "                'no_supplier': int((merged_df['Nama Store'] == '').sum()),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            return merged_df, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file data: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {file_path.name}: {str(e)}\"\n",
    "        print(f\"  - {error_msg}\")\n",
    "        return None, {\n",
    "            'file': file_path.name,\n",
    "            'location': location if 'location' in locals() else 'Unknown',\n",
    "            'contribution_pct': contribution_pct if 'contribution_pct' in locals() else 0,\n",
    "            'total_rows': 0,\n",
    "            'padang_suppliers': 0,\n",
    "            'other_suppliers': 0,\n",
    "            'no_supplier': 0,\n",
    "            'status': f\"Error: {str(e)[:100]}\"  # Truncate long error messages\n",
    "        }\n",
    "\n",
    "def load_padang_data(padang_path):\n",
    "    \"\"\"Load Padang data from either CSV or Excel file.\n",
    "    \n",
    "    Args:\n",
    "        padang_path: Path to the input file (CSV or XLSX)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and cleaned Padang data\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported or file cannot be read\n",
    "    \"\"\"\n",
    "    print(f\"Loading Padang data from {padang_path}...\")\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = str(padang_path).lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(padang_path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(padang_path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Padang data with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Padang data from {padang_path}: {str(e)}\")\n",
    "\n",
    "def format_number_for_csv(x):\n",
    "    \"\"\"Format numbers for CSV output with Indonesian locale (comma as decimal, dot as thousand)\"\"\"\n",
    "    if pd.isna(x) or x == '':\n",
    "        return x\n",
    "    try:\n",
    "        if isinstance(x, (int, float)):\n",
    "            if x == int(x):  # Whole number\n",
    "                return f\"{int(x):,d}\".replace(\",\", \".\")\n",
    "            else:  # Decimal number\n",
    "                return f\"{x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def clean_and_convert(df):\n",
    "    \"\"\"Clean and convert DataFrame columns to appropriate types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string first to handle NaN/None consistently\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Define NA values that should be treated as empty/missing\n",
    "    na_values = list(NA_VALUES)\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        # Replace NA values with empty string first (treating them as literals, not regex)\n",
    "        df[col] = df[col].replace(na_values, '', regex=False)\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if df[col].empty:\n",
    "            continue\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if col in NUMERIC_COLUMNS:\n",
    "            # Convert to numeric, coercing errors to NaN, then fill with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            # For non-numeric columns, ensure they're strings and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace empty strings with NaN and then fill with empty string\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('').infer_objects(copy=False)\n",
    "            df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            df[col] = df[col].infer_objects(copy=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file with robust error handling for problematic values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nReading excel file: {file_path.name}...\")\n",
    "        \n",
    "        # First, read the file with openpyxl directly to handle the data more carefully\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = load_workbook(\n",
    "            filename=file_path,\n",
    "            read_only=True,    # Read-only mode is faster and uses less memory\n",
    "            data_only=True,    # Get the stored value instead of the formula\n",
    "            keep_links=False   # Don't load external links\n",
    "        )\n",
    "        \n",
    "        # Get the first sheet\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Get headers from the first row\n",
    "        headers = []\n",
    "        for idx, cell in enumerate(next(ws.iter_rows(values_only=True))):\n",
    "            header = str(cell).strip() if cell not in (None, '') else f\"Column_{idx + 1}\"\n",
    "            headers.append(header)\n",
    "        \n",
    "        # Initialize data rows\n",
    "        data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for row in ws.iter_rows(min_row=2, values_only=True):  # Skip header row\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                if cell is None:\n",
    "                    row_data.append('')\n",
    "                    continue\n",
    "\n",
    "                cell_str = str(cell).strip()\n",
    "                if cell_str.upper() in NA_VALUES:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    row_data.append(cell_str)\n",
    "            \n",
    "            # Only add row if it has data\n",
    "            if any(cell != '' for cell in row_data):\n",
    "                data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Normalize column data types\n",
    "        df = clean_and_convert(df)\n",
    "        \n",
    "        print(f\" Successfully processed {file_path.name} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {file_path.name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_file(df, file_path, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame to file with consistent extension and content type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path object or string for the output file\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments to pass to to_csv or to_excel\n",
    "        \n",
    "    Returns:\n",
    "        Path: The path where the file was saved\n",
    "    \"\"\"\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the correct file extension\n",
    "    if not file_path.suffix.lower() == f'.{file_format}':\n",
    "        file_path = file_path.with_suffix(f'.{file_format}')\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_output = df.copy()\n",
    "    \n",
    "    # Common preprocessing\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].astype(str).str.strip()\n",
    "        if file_format == 'xlsx':\n",
    "            # For Excel, wrap SKU in =\"...\" to preserve leading zeros\n",
    "            df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Format numbers for CSV if needed\n",
    "    if file_format == 'csv':\n",
    "        numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "    \n",
    "    # Save based on format\n",
    "    if file_format == 'csv':\n",
    "        df_output.to_csv(\n",
    "            file_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig',\n",
    "            **kwargs\n",
    "        )\n",
    "    elif file_format == 'xlsx':\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "            df_output.to_excel(writer, index=False, **kwargs)\n",
    "            \n",
    "            # Format SKU column as text in Excel\n",
    "            if 'SKU' in df_output.columns:\n",
    "                ws = writer.sheets[list(writer.sheets.keys())[0]]\n",
    "                sku_col_idx = df_output.columns.get_loc(\"SKU\") + 1\n",
    "                for row in ws.iter_rows(\n",
    "                    min_row=2,  # Skip header\n",
    "                    max_row=ws.max_row,\n",
    "                    min_col=sku_col_idx,\n",
    "                    max_col=sku_col_idx\n",
    "                ):\n",
    "                    for cell in row:\n",
    "                        cell.number_format = numbers.FORMAT_TEXT\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "    \n",
    "    print(f\"File saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_to_complete_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save Complete format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "\n",
    "    return save_file(df, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_m2_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save M2 format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_filtered = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[['Toko', 'SKU', 'HPP', 'final_updated_regular_po_qty']]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_M2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_M2_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_emergency_po_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save emergency PO format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with emergency PO qty > 0\n",
    "    df_filtered = df[df['emergency_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[[\n",
    "        'Brand', 'SKU', 'Nama', 'Toko', 'HPP', \n",
    "        'emergency_po_qty', 'emergency_po_cost'\n",
    "    ]]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_EMERGENCY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_EMERGENCY_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    supplier_df = load_supplier_data(SUPPLIER_PATH)\n",
    "    store_contrib = load_store_contribution(STORE_CONTRIBUTION_PATH)\n",
    "    all_summaries = []\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    CURRENT_DIR = INPUT_DIR / current_date\n",
    "\n",
    "    # get padang df first\n",
    "    df_padang = load_padang_data(CURRENT_DIR / '1. Miss Glam Padang.xlsx')\n",
    "\n",
    "    # test_xlsx_convert()\n",
    "\n",
    "    # Process each PO file\n",
    "    for file_path in sorted(CURRENT_DIR.glob('*.xlsx')):\n",
    "        try:\n",
    "            merged_df, summary = process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=True)\n",
    "\n",
    "            save_to_complete_format(merged_df, file_path.name, file_format='xlsx')\n",
    "            save_to_complete_format(merged_df, file_path.name)\n",
    "            save_to_m2_format(merged_df, file_path.name)\n",
    "            save_to_emergency_po_format(merged_df, file_path.name)\n",
    "\n",
    "            # summary['output_path'] = str(output_path)\n",
    "            output_path = OUTPUT_DIR / file_path.name\n",
    "            summary['output_path'] = str(output_path)\n",
    "\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  - Location: {summary['location']}\")\n",
    "            print(f\"  - Contribution: {summary['contribution_pct']}%\")\n",
    "            print(f\"  - Rows processed: {summary['total_rows']}\")\n",
    "            print(f\"  - 'Miss Glam Padang' suppliers: {summary['padang_suppliers']} rows\")\n",
    "            print(f\"  - Other suppliers: {summary['other_suppliers']} rows\")\n",
    "            print(f\"  - No supplier data: {summary['no_supplier']} rows\")\n",
    "            print(f\"  - Saved to: {output_path}\")\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Display final summary\n",
    "    if all_summaries:\n",
    "        print(\"\\nProcessing complete! Summary:\")\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Show sample of last processed file\n",
    "        print(\"\\nSample of the last processed file:\")\n",
    "        display(merged_df)\n",
    "    else:\n",
    "        print(\"\\nNo files were processed successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
