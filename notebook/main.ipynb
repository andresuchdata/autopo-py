{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = os.listdir('data/rawpo/xlsx')\n",
    "display(raw_files)\n",
    "\n",
    "def convert_xlsx_to_csv(directory):\n",
    "    \"\"\"\n",
    "    Convert all .xlsx files in the specified directory to .csv format.\n",
    "    Skips files that already have a corresponding .csv file.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .xlsx files\n",
    "    \"\"\"\n",
    "    # Convert to Path object for easier handling\n",
    "    dir_path = Path(directory)\n",
    "    \n",
    "    # Find all .xlsx files in the directory\n",
    "    xlsx_files = list(dir_path.glob('*.xlsx'))\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No .xlsx files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} .xlsx files to process...\")\n",
    "    \n",
    "    for xlsx_file in xlsx_files:\n",
    "        # Create output filename with .csv extension\n",
    "        csv_file = xlsx_file.with_suffix('.csv')\n",
    "        \n",
    "        # Skip if CSV already exists\n",
    "        if csv_file.exists():\n",
    "            print(f\"Skipping {xlsx_file.name} - {csv_file.name} already exists\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read the Excel file\n",
    "            print(f\"\\n===========================================Converting {xlsx_file.name} to {csv_file.name}...\")\n",
    "            df = pd.read_excel(xlsx_file)\n",
    "            \n",
    "            # Write to CSV\n",
    "            df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully created {csv_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {xlsx_file.name}: {str(e)}\")\n",
    "    \n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "# Example usage:\n",
    "convert_xlsx_to_csv('data/rawpo/xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97300143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file, converting 'INF' to numpy.inf\n",
    "ori_df = pd.read_csv('data/rawpo/01 Miss Glam Padang.csv', sep=';', decimal=',')\n",
    "\n",
    "# Convert all numeric columns, handling infinity and NaN values\n",
    "for col in ori_df.select_dtypes(include=[np.number]).columns:\n",
    "    ori_df[col] = pd.to_numeric(ori_df[col], errors='coerce')\n",
    "\n",
    "df = ori_df.copy()\n",
    "df = df.rename(columns={'Stok': 'Stock'})\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# extract only the columns we need\n",
    "display(df.info())\n",
    "df = df[['Brand', 'SKU', 'Nama', 'Stock', 'Daily Sales', 'Max. Daily Sales', 'Lead Time', 'Max. Lead Time', 'Sedang PO', 'Min. Order', 'HPP']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "# contribution dictionary for each store location\n",
    "contribution_dict = {\n",
    "    'payakumbuh': 0.47,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplier mapping\n",
    "# to map an SKU and brand to specific supplier\n",
    "\n",
    "raw_supplier_df = pd.read_csv('data/supplier.csv', sep=';', decimal=',')\n",
    "raw_supplier_df = raw_supplier_df.fillna('')\n",
    "\n",
    "display(raw_supplier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f860f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert object columns to numeric where possible\n",
    "numeric_columns = ['Stock', 'Daily Sales', 'Lead Time', 'Max. Daily Sales', 'Max. Lead Time']\n",
    "\n",
    "df_clean = df.copy()\n",
    "display('Raw DataFrame: ', df)\n",
    "\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "for col in numeric_columns:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Now fill NA with 0 and convert to int\n",
    "df_clean = df_clean.fillna(0)\n",
    "\n",
    "# For non-numeric columns, keep them as they are\n",
    "non_numeric_columns = ['Brand', 'SKU']  # Add other non-numeric columns if needed\n",
    "for col in non_numeric_columns:\n",
    "    df_clean[col] = df[col]  # Keep original values\n",
    "\n",
    "# add new column 'Lead Time Sedang PO' default to 2 days\n",
    "df_clean['Lead Time Sedang PO'] = 5\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"Cleaned DataFrame:\")\n",
    "display(df_clean)\n",
    "\n",
    "# Show info of the cleaned DataFrame\n",
    "print(\"\\nDataFrame Info: Expected to have maximal non-null values...\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e2257",
   "metadata": {},
   "source": [
    "### Add Supply Chain params for AutoPO\n",
    "\n",
    "- Safety stock - (max sales x max lead time) - (avg sales x avg lead time)\n",
    "- Reorder point - avg sales x avg lead time + safety stock\n",
    "- Stock cover days (for 21 days) - avg sales x 21\n",
    "- RoP_Reference (1 -> RoP > Stock cover days, 0 -> RoP < Stock cover days)\n",
    "- Current stock days cover -> Current stock / avg sales\n",
    "- Is_open_po (1 -> Current Stock < Reorder point, 0 -> otherwise)\n",
    "- Initial_Qty_PO - Reorder point - Current stock\n",
    "\n",
    "- Is_emergency_PO - 1 -> Current stock days cover <= max lead time\n",
    "\n",
    "- Emergency_PO_Qty - (max lead time - Current stock days cover) x Avg sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683891c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# 1. Safety stock = (max sales x max lead time) - (avg sales x avg lead time)\n",
    "df_clean['Safety stock'] = (df_clean['Max. Daily Sales'] * df_clean['Max. Lead Time']) - (df_clean['Daily Sales'] * df_clean['Lead Time'])\n",
    "# round up safety stock\n",
    "df_clean['Safety stock'] = df_clean['Safety stock'].apply(lambda x: np.ceil(x)).astype(int)\n",
    "\n",
    "# 2. Reorder point = (avg sales x avg lead time) + safety stock\n",
    "df_clean['Reorder point'] = np.ceil((df_clean['Daily Sales'] * df_clean['Lead Time']) + \n",
    "                                   df_clean['Safety stock']).astype(int)\n",
    "\n",
    "# 3. Stock cover days (in Qty) for 30 days = avg sales x 30 \n",
    "df_clean['Stock cover 30 days'] = df_clean['Daily Sales'] * 30\n",
    "df_clean['Stock cover 30 days'] = df_clean['Stock cover 30 days'].apply(lambda x: np.ceil(x)).astype(int)\n",
    "\n",
    "# 5. Current stock days cover (in days) = Current stock / avg sales\n",
    "df_clean['current_stock_days_cover'] = (df_clean['Stock'].astype(float) * 1.0 ) / df_clean['Daily Sales'].astype(float)\n",
    "\n",
    "# 6. Is_open_po (1 -> Current stock < Reorder point, 0 -> otherwise)\n",
    "df_clean['is_open_po'] = np.where((df_clean['current_stock_days_cover'] <= 30) & (df_clean['Stock'] <= df_clean['Reorder point']), 1, 0)\n",
    "\n",
    "# 7. Initial_Qty_PO = Stock cover 30 days - Current stock - sedang PO\n",
    "df_clean['initial_qty_po'] = df_clean['Stock cover 30 days'] - df_clean['Stock'] - df_clean['Sedang PO']\n",
    "df_clean['initial_qty_po'] = np.where(df_clean['is_open_po'] == 1, df_clean['initial_qty_po'], 0)\n",
    "df_clean['initial_qty_po'] = df_clean['initial_qty_po'].apply(lambda x: x if x > 0 else 0).astype(int)\n",
    "\n",
    "# 9. Emergency_PO_Qty = (max lead time - Current stock days cover) x Avg sales\n",
    "# First, ensure 'Sedang PO' column exists and handle potential missing values\n",
    "if 'Sedang PO' not in df_clean.columns:\n",
    "    df_clean['Sedang PO'] = 0  # Default to 0 if column doesn't exist\n",
    "\n",
    "# Calculate emergency_po_qty based on the condition\n",
    "df_clean['emergency_po_qty'] = np.where(\n",
    "    df_clean['Sedang PO'] > 0,  # If there is 'Sedang PO' quantity\n",
    "    np.maximum(0, (df_clean['Lead Time Sedang PO'] - df_clean['current_stock_days_cover']) * \n",
    "              df_clean['Daily Sales']),\n",
    "    # Else use the original formula\n",
    "    np.ceil((df_clean['Max. Lead Time'] - df_clean['current_stock_days_cover']) * \n",
    "            df_clean['Daily Sales'])\n",
    ")\n",
    "\n",
    "# First, handle any infinite values and NaN values\n",
    "df_clean['emergency_po_qty'] = (\n",
    "    df_clean['emergency_po_qty']\n",
    "    .replace([np.inf, -np.inf], 0)  # Replace infinities with 0\n",
    "    .fillna(0)                      # Fill any remaining NaNs with 0\n",
    "    .astype(int)                    # Now safely convert to integers\n",
    ")\n",
    "\n",
    "# If you want to ensure no negative values (since it's a quantity)\n",
    "df_clean['emergency_po_qty'] = df_clean['emergency_po_qty'].clip(lower=0)\n",
    "\n",
    "# calculate updated po quantity\n",
    "df_clean['updated_regular_po_qty'] = df_clean['initial_qty_po'] - df_clean['emergency_po_qty']\n",
    "df_clean['updated_regular_po_qty'] = df_clean['updated_regular_po_qty'].apply(lambda x: x if x > 0 else 0).astype(int)\n",
    "\n",
    "# Final check updated regular PO - if less than Min. Order, use Min. Order qty\n",
    "df_clean['final_updated_regular_po_qty'] = np.where((df_clean['updated_regular_po_qty'] > 0) & (df_clean['updated_regular_po_qty'] < df_clean['Min. Order']), df_clean['Min. Order'], df_clean['updated_regular_po_qty'])\n",
    "\n",
    "\n",
    "# Calculate total cost (HPP * qty) for emergency PO and final updated regular PO\n",
    "df_clean['total_cost_emergency_po'] = df_clean['emergency_po_qty'] * df_clean['HPP']\n",
    "df_clean['total_cost_final_updated_regular_po'] = df_clean['final_updated_regular_po_qty'] * df_clean['HPP']\n",
    "\n",
    "# Handle any NaN or infinite values by replacing them with 0\n",
    "df_clean = df_clean.fillna(0)\n",
    "df_clean = df_clean.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Display the updated DataFrame with new columns\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = 'output/result.csv'\n",
    "df_clean.to_csv(csv_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "print(f\"CSV file saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178c94d",
   "metadata": {},
   "source": [
    "### Mapping Brand and SKU with supplier (add supplier column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89776ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make copies to avoid modifying originals\n",
    "df_clean_trimmed = df_clean.copy()\n",
    "raw_supplier_trimmed = raw_supplier_df.copy()\n",
    "\n",
    "# Trim whitespace from brand names\n",
    "df_clean_trimmed['Brand'] = df_clean_trimmed['Brand'].str.strip()\n",
    "raw_supplier_trimmed['Nama Brand'] = raw_supplier_trimmed['Nama Brand'].str.strip()\n",
    "\n",
    "# First, get all Padang suppliers\n",
    "padang_suppliers = raw_supplier_trimmed[\n",
    "    raw_supplier_trimmed['Nama Store'] == 'Miss Glam Padang'\n",
    "]\n",
    "\n",
    "# Then get all other suppliers (non-Padang)\n",
    "other_suppliers = raw_supplier_trimmed[\n",
    "    raw_supplier_trimmed['Nama Store'] != 'Miss Glam Padang'\n",
    "]\n",
    "\n",
    "# Step 1: Left join with Padang suppliers first (priority)\n",
    "merged_df = pd.merge(\n",
    "    df_clean_trimmed,\n",
    "    padang_suppliers,\n",
    "    left_on='Brand',\n",
    "    right_on='Nama Brand',\n",
    "    how='left',\n",
    "    suffixes=('_clean', '_supplier')\n",
    ")\n",
    "\n",
    "# Step 2: For rows without Padang supplier, try to find other suppliers\n",
    "# Get the indices of rows that didn't get a match with Padang suppliers\n",
    "no_padang_match = merged_df[merged_df['Nama Brand'].isna()].index\n",
    "\n",
    "if len(no_padang_match) > 0:\n",
    "    # Get the brands that need non-Padang suppliers\n",
    "    brands_needing_suppliers = merged_df.loc[no_padang_match, 'Brand'].unique()\n",
    "    \n",
    "    # Get the first matching supplier for each brand (you can change this logic if needed)\n",
    "    first_supplier_per_brand = other_suppliers.drop_duplicates(subset='Nama Brand')\n",
    "    \n",
    "    # Update the rows that didn't have Padang suppliers\n",
    "    for brand in brands_needing_suppliers:\n",
    "        supplier_data = first_supplier_per_brand[first_supplier_per_brand['Nama Brand'] == brand]\n",
    "        if not supplier_data.empty:\n",
    "            # Update the corresponding rows in merged_df\n",
    "            brand_mask = (merged_df['Brand'] == brand) & (merged_df['Nama Brand'].isna())\n",
    "            for col in supplier_data.columns:\n",
    "                if col in merged_df.columns and col != 'Brand':  # Don't overwrite the Brand column\n",
    "                    merged_df.loc[brand_mask, col] = supplier_data[col].values[0]\n",
    "\n",
    "# Clean up: For any remaining NaN values in supplier columns, fill with empty string or as needed\n",
    "supplier_columns = [\n",
    "    'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "    'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "    'Promo Factor', 'Delay Factor'\n",
    "]\n",
    "\n",
    "for col in supplier_columns:\n",
    "    if col in merged_df.columns:\n",
    "        if merged_df[col].dtype == 'object':\n",
    "            merged_df[col] = merged_df[col].fillna('')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Total rows in df_clean: {len(df_clean_trimmed)}\")\n",
    "print(f\"Total rows after merge: {len(merged_df)}\")\n",
    "\n",
    "# Count how many rows got Padang suppliers vs other suppliers vs no suppliers\n",
    "padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                       (merged_df['Nama Store'] != '')).sum()\n",
    "no_supplier = (merged_df['Nama Store'] == '').sum()\n",
    "\n",
    "print(f\"\\nSuppliers matched:\")\n",
    "print(f\"- 'Miss Glam Padang' suppliers: {padang_count} rows\")\n",
    "print(f\"- Other suppliers: {other_supplier_count} rows\")\n",
    "print(f\"- No supplier data: {no_supplier} rows\")\n",
    "\n",
    "# Save the result\n",
    "os.makedirs('output', exist_ok=True)\n",
    "output_path = 'output/merged_with_suppliers.csv'\n",
    "merged_df.to_csv(output_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "\n",
    "# Show a sample of the results\n",
    "print(\"\\nSample of merged data (first 5 rows):\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_clean with raw_supplier_df to see all supplier matches\n",
    "all_suppliers_merge = pd.merge(\n",
    "    df_clean_trimmed,\n",
    "    raw_supplier_trimmed,\n",
    "    left_on='Brand',\n",
    "    right_on='Nama Brand',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Group by Brand and SKU to count unique suppliers\n",
    "supplier_counts = all_suppliers_merge.groupby(['Brand', 'SKU'])['Nama Supplier'].nunique().reset_index()\n",
    "supplier_counts.columns = ['Brand', 'SKU', 'Supplier_Count']\n",
    "\n",
    "# Filter for brands/SKUs with multiple suppliers\n",
    "multi_supplier_items = supplier_counts[supplier_counts['Supplier_Count'] > 1]\n",
    "\n",
    "print(f\"Found {len(multi_supplier_items)} brand/SKU combinations with multiple suppliers\")\n",
    "print(\"\\nSample of items with multiple suppliers:\")\n",
    "display(multi_supplier_items.head())\n",
    "\n",
    "# If you want to see the actual supplier details for these items\n",
    "if not multi_supplier_items.empty:\n",
    "    print(\"\\nDetailed supplier information for multi-supplier items:\")\n",
    "    multi_supplier_details = all_suppliers_merge.merge(\n",
    "        multi_supplier_items[['Brand', 'SKU']],\n",
    "        on=['Brand', 'SKU']\n",
    "    )\n",
    "    display(multi_supplier_details[['Brand', 'SKU', 'Nama Supplier', 'Nama Store']].drop_duplicates().sort_values(['Brand', 'SKU']))\n",
    "\n",
    "    # List of SKUs to check\n",
    "skus_to_check = [\n",
    "    '8995232702124',  # ACNEMED\n",
    "    '8992821100293',  # ACNES\n",
    "    '8992821100309',  # ACNES\n",
    "    '8992821100323',  # ACNES\n",
    "    '8992821100354'   # ACNES\n",
    "]\n",
    "\n",
    "# Convert SKUs to integers (since they appear as integers in df_clean)\n",
    "skus_to_check = [int(sku) for sku in skus_to_check]\n",
    "\n",
    "# Check if these SKUs exist in df_clean\n",
    "found_skus = merged_df[merged_df['SKU'].isin(skus_to_check)]\n",
    "\n",
    "if not found_skus.empty:\n",
    "    print(\"Found matching SKUs in df_clean:\")\n",
    "    display(found_skus[['Brand', 'SKU', 'Nama']])\n",
    "else:\n",
    "    print(\"None of these SKUs were found in df_clean.\")\n",
    "    print(\"\\nChecking if there are any similar SKUs...\")\n",
    "    \n",
    "    # Check for any SKUs that contain these numbers\n",
    "    for sku in skus_to_check:\n",
    "        similar = merged_df[merged_df['SKU'].astype(str).str.contains(str(sku)[:8])]\n",
    "        if not similar.empty:\n",
    "            print(f\"\\nSKUs similar to {sku}:\")\n",
    "            display(similar[['Brand', 'SKU', 'Nama']])\n",
    "    \n",
    "    # Check the data types to ensure we're comparing correctly\n",
    "    print(\"\\nData type of SKU column:\", merged_df['SKU'].dtype)\n",
    "    print(\"Sample SKUs from df_clean:\", merged_df['SKU'].head().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a03",
   "metadata": {},
   "source": [
    "### Find brands who are missing suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find brands in df_clean that don't have a match in raw_supplier_df\n",
    "missing_brands = set(df_clean['Brand']) - set(raw_supplier_df['Nama Brand'].dropna().unique())\n",
    "\n",
    "print(f\"Number of brands in df_clean: {len(df_clean['Brand'].unique())}\")\n",
    "print(f\"Number of brands in raw_supplier_df: {len(raw_supplier_df['Nama Brand'].unique())}\")\n",
    "print(f\"\\nNumber of brands missing supplier data: {len(missing_brands)}\")\n",
    "print(\"\\nFirst 20 missing brands (alphabetical order):\")\n",
    "print(sorted(list(missing_brands))[:20])\n",
    "\n",
    "# Count how many rows are affected per missing brand\n",
    "missing_brand_counts = df_clean[df_clean['Brand'].isin(missing_brands)]['Brand'].value_counts()\n",
    "print(\"\\nTop 20 missing brands by row count:\")\n",
    "print(missing_brand_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8564e9",
   "metadata": {},
   "source": [
    "# Output grouped data per one brand and one SKU to separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'output_po'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a directory for brands without suppliers\n",
    "no_supplier_dir = os.path.join(output_dir, '0_no_suppliers')\n",
    "os.makedirs(no_supplier_dir, exist_ok=True)\n",
    "\n",
    "# Function to sanitize folder names\n",
    "def sanitize_folder_name(name):\n",
    "    # Remove or replace invalid characters\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        name = name.replace(char, '_')\n",
    "    return name.strip()\n",
    "\n",
    "# Process each group\n",
    "for (supplier_id, supplier_name, brand), group in final_df.groupby(['ID Supplier', 'Nama Supplier', 'Brand']):\n",
    "    # Skip if no supplier (shouldn't happen as we replaced NaN with defaults)\n",
    "    if pd.isna(supplier_id) or not supplier_name:\n",
    "        # Save to no_supplier_dir\n",
    "        brand_file = os.path.join(no_supplier_dir, f'{sanitize_folder_name(brand)}.csv')\n",
    "        group.to_csv(brand_file, index=False, sep=';', encoding='utf-8-sig')\n",
    "        continue\n",
    "    \n",
    "    # Create supplier directory\n",
    "    supplier_dir = os.path.join(output_dir, f'{int(supplier_id)}_{sanitize_folder_name(supplier_name)}')\n",
    "    os.makedirs(supplier_dir, exist_ok=True)\n",
    "    \n",
    "    # Save brand file\n",
    "    brand_file = os.path.join(supplier_dir, f'{sanitize_folder_name(brand)}.csv')\n",
    "    group.to_csv(brand_file, index=False, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data has been organized into supplier and brand-based folders in 'output_po'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6ba5",
   "metadata": {},
   "source": [
    "# Final batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce491334",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = [\n",
    "    'HPP', 'Harga', 'Ranking', 'Grade', 'Terjual', 'Stok', 'Lost Days',\n",
    "    'Velocity Capped', 'Daily Sales', 'Lead Time', 'Max. Daily Sales',\n",
    "    'Max. Lead Time', 'Min. Order', 'Safety Stok', 'ROP', '3W Cover',\n",
    "    'Sedang PO', 'Suggested', 'Amount', 'Promo Factor', 'Delay Factor',\n",
    "    'Stock Cover', 'Days to Backup', 'Qty to Backup'\n",
    "]\n",
    "\n",
    "NA_VALUES = {\n",
    "    'NAN', 'NA', '#N/A', 'NULL', 'NONE', '', '?', '-', 'INF', '-INF',\n",
    "    '+INF', 'INFINITY', '-INFINITY', '1.#INF', '-1.#INF', '1.#QNAN'\n",
    "}\n",
    "\n",
    "def _patch_openpyxl_number_casting():\n",
    "    \"\"\"Ensure openpyxl won't crash when encountering NAN/INF in numeric cells.\"\"\"\n",
    "    print(\"Calling _patch_openpyxl_number_casting...\")\n",
    "\n",
    "    try:\n",
    "        from openpyxl.worksheet import _reader\n",
    "\n",
    "        original_cast = _reader._cast_number\n",
    "\n",
    "        def _safe_cast_number(value):  # pragma: no cover - monkey patch\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().upper() in NA_VALUES:\n",
    "                    return 0\n",
    "            try:\n",
    "                return original_cast(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0 if value in (None, '') else value\n",
    "\n",
    "        _reader._cast_number = _safe_cast_number\n",
    "    except Exception:\n",
    "        # If patch fails we continue; runtime reader will still attempt default behaviour\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da39de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _patch_openpyxl_number_casting...\n",
      "Loading supplier data: /Users/andresuchitra/dev/missglam/autopo/data/supplier.csv\n",
      "Loading Padang data from data/rawpo/xlsx/1. Miss Glam Padang.xlsx...\n",
      "Successfully loaded Padang data with 6761 rows\n",
      "\n",
      "Processing PO file: 1. Miss Glam Padang.csv ....\n",
      "  - Extracted location: PADANG\n",
      "Processing store: PADANG - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 138 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/1. Miss Glam Padang.csv\n",
      "  - Location: PADANG\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 6761\n",
      "  - 'Miss Glam Padang' suppliers: 6623 rows\n",
      "  - Other suppliers: 17 rows\n",
      "  - No supplier data: 121 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/1. Miss Glam Padang.csv\n",
      "\n",
      "Processing PO file: 10. Miss Glam Palembang.csv ....\n",
      "  - Extracted location: PALEMBANG\n",
      "Processing store: PALEMBANG - 26.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 116 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/10. Miss Glam Palembang.csv\n",
      "  - Location: PALEMBANG\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 4873\n",
      "  - 'Miss Glam Padang' suppliers: 10 rows\n",
      "  - Other suppliers: 4796 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/10. Miss Glam Palembang.csv\n",
      "\n",
      "Processing PO file: 11. Miss Glam Damar.csv ....\n",
      "  - Extracted location: DAMAR\n",
      "Processing store: DAMAR - 91.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 134 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/11. Miss Glam Damar.csv\n",
      "  - Location: DAMAR\n",
      "  - Contribution: 91%\n",
      "  - Rows processed: 6656\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 6531 rows\n",
      "  - No supplier data: 125 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/11. Miss Glam Damar.csv\n",
      "\n",
      "Processing PO file: 12. Miss Glam Bangka.csv ....\n",
      "  - Extracted location: BANGKA\n",
      "Processing store: BANGKA - 28.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 129 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/12. Miss Glam Bangka.csv\n",
      "  - Location: BANGKA\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 4542\n",
      "  - 'Miss Glam Padang' suppliers: 21 rows\n",
      "  - Other suppliers: 4443 rows\n",
      "  - No supplier data: 78 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/12. Miss Glam Bangka.csv\n",
      "\n",
      "Processing PO file: 13. Miss Glam Payakumbuh.csv ....\n",
      "  - Extracted location: PAYAKUMBUH\n",
      "Processing store: PAYAKUMBUH - 47.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 83 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/13. Miss Glam Payakumbuh.csv\n",
      "  - Location: PAYAKUMBUH\n",
      "  - Contribution: 47%\n",
      "  - Rows processed: 5304\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 5224 rows\n",
      "  - No supplier data: 80 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/13. Miss Glam Payakumbuh.csv\n",
      "\n",
      "Processing PO file: 14. Miss Glam Solok.csv ....\n",
      "  - Extracted location: SOLOK\n",
      "Processing store: SOLOK - 37.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 64 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/14. Miss Glam Solok.csv\n",
      "  - Location: SOLOK\n",
      "  - Contribution: 37%\n",
      "  - Rows processed: 4657\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 4597 rows\n",
      "  - No supplier data: 60 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/14. Miss Glam Solok.csv\n",
      "\n",
      "Processing PO file: 15. Miss Glam Tembilahan.csv ....\n",
      "  - Extracted location: TEMBILAHAN\n",
      "Processing store: TEMBILAHAN - 27.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 80 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/15. Miss Glam Tembilahan.csv\n",
      "  - Location: TEMBILAHAN\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 4351\n",
      "  - 'Miss Glam Padang' suppliers: 9 rows\n",
      "  - Other suppliers: 4277 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/15. Miss Glam Tembilahan.csv\n",
      "\n",
      "Processing PO file: 16. Miss Glam Lubuk Linggau.csv ....\n",
      "  - Extracted location: LUBUK LINGGAU\n",
      "Processing store: LUBUK LINGGAU - 26.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 175 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/16. Miss Glam Lubuk Linggau.csv\n",
      "  - Location: LUBUK LINGGAU\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 4410\n",
      "  - 'Miss Glam Padang' suppliers: 7 rows\n",
      "  - Other suppliers: 4345 rows\n",
      "  - No supplier data: 58 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/16. Miss Glam Lubuk Linggau.csv\n",
      "\n",
      "Processing PO file: 17. Miss Glam Dumai.csv ....\n",
      "  - Extracted location: DUMAI\n",
      "Processing store: DUMAI - 36.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 67 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/17. Miss Glam Dumai.csv\n",
      "  - Location: DUMAI\n",
      "  - Contribution: 36%\n",
      "  - Rows processed: 4778\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 4713 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/17. Miss Glam Dumai.csv\n",
      "\n",
      "Processing PO file: 18. Miss Glam Kedaton.csv ....\n",
      "  - Extracted location: KEDATON\n",
      "Processing store: KEDATON - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 118 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/18. Miss Glam Kedaton.csv\n",
      "  - Location: KEDATON\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4212\n",
      "  - 'Miss Glam Padang' suppliers: 13 rows\n",
      "  - Other suppliers: 4132 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/18. Miss Glam Kedaton.csv\n",
      "\n",
      "Processing PO file: 19. Miss Glam Rantau Prapat.csv ....\n",
      "  - Extracted location: RANTAU PRAPAT\n",
      "Processing store: RANTAU PRAPAT - 27.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 98 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/19. Miss Glam Rantau Prapat.csv\n",
      "  - Location: RANTAU PRAPAT\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 4278\n",
      "  - 'Miss Glam Padang' suppliers: 19 rows\n",
      "  - Other suppliers: 4191 rows\n",
      "  - No supplier data: 68 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/19. Miss Glam Rantau Prapat.csv\n",
      "\n",
      "Processing PO file: 2. Miss Glam Pekanbaru.csv ....\n",
      "  - Extracted location: PEKANBARU\n",
      "Processing store: PEKANBARU - 60.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 119 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/2. Miss Glam Pekanbaru.csv\n",
      "  - Location: PEKANBARU\n",
      "  - Contribution: 60%\n",
      "  - Rows processed: 5957\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5847 rows\n",
      "  - No supplier data: 106 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/2. Miss Glam Pekanbaru.csv\n",
      "\n",
      "Processing PO file: 20. Miss Glam Tanjung Pinang.csv ....\n",
      "  - Extracted location: TANJUNG PINANG\n",
      "Processing store: TANJUNG PINANG - 19.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 95 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/20. Miss Glam Tanjung Pinang.csv\n",
      "  - Location: TANJUNG PINANG\n",
      "  - Contribution: 19%\n",
      "  - Rows processed: 4037\n",
      "  - 'Miss Glam Padang' suppliers: 8 rows\n",
      "  - Other suppliers: 3961 rows\n",
      "  - No supplier data: 68 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/20. Miss Glam Tanjung Pinang.csv\n",
      "\n",
      "Processing PO file: 21. Miss Glam Sutomo.csv ....\n",
      "  - Extracted location: SUTOMO\n",
      "Processing store: SUTOMO - 49.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 90 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/21. Miss Glam Sutomo.csv\n",
      "  - Location: SUTOMO\n",
      "  - Contribution: 49%\n",
      "  - Rows processed: 5706\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5625 rows\n",
      "  - No supplier data: 77 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/21. Miss Glam Sutomo.csv\n",
      "\n",
      "Processing PO file: 22. Miss Glam Pasaman Barat.csv ....\n",
      "  - Extracted location: PASAMAN BARAT\n",
      "Processing store: PASAMAN BARAT - 17.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 61 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/22. Miss Glam Pasaman Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/22. Miss Glam Pasaman Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/22. Miss Glam Pasaman Barat.csv\n",
      "  - Location: PASAMAN BARAT\n",
      "  - Contribution: 17%\n",
      "  - Rows processed: 3842\n",
      "  - 'Miss Glam Padang' suppliers: 6 rows\n",
      "  - Other suppliers: 3794 rows\n",
      "  - No supplier data: 42 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/22. Miss Glam Pasaman Barat.csv\n",
      "\n",
      "Processing PO file: 23. Miss Glam Halat.csv ....\n",
      "  - Extracted location: HALAT\n",
      "Processing store: HALAT - 31.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 121 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/23. Miss Glam Halat.csv\n",
      "  - Location: HALAT\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 4805\n",
      "  - 'Miss Glam Padang' suppliers: 15 rows\n",
      "  - Other suppliers: 4696 rows\n",
      "  - No supplier data: 94 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/23. Miss Glam Halat.csv\n",
      "\n",
      "Processing PO file: 24. Miss Glam Duri.csv ....\n",
      "  - Extracted location: DURI\n",
      "Processing store: DURI - 28.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 57 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/24. Miss Glam Duri.csv\n",
      "  - Location: DURI\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 3968\n",
      "  - 'Miss Glam Padang' suppliers: 2 rows\n",
      "  - Other suppliers: 3914 rows\n",
      "  - No supplier data: 52 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/24. Miss Glam Duri.csv\n",
      "\n",
      "Processing PO file: 25. Miss Glam Sudirman.csv ....\n",
      "  - Extracted location: SUDIRMAN\n",
      "Processing store: SUDIRMAN - 44.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 112 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/25. Miss Glam Sudirman.csv\n",
      "  - Location: SUDIRMAN\n",
      "  - Contribution: 44%\n",
      "  - Rows processed: 5632\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5530 rows\n",
      "  - No supplier data: 98 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/25. Miss Glam Sudirman.csv\n",
      "\n",
      "Processing PO file: 26. Miss Glam Dr. Mansyur.csv ....\n",
      "  - Extracted location: DR. MANSYUR\n",
      "Processing store: DR. MANSYUR - 25.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 126 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/26. Miss Glam Dr. Mansyur.csv\n",
      "  - Location: DR. MANSYUR\n",
      "  - Contribution: 25%\n",
      "  - Rows processed: 4963\n",
      "  - 'Miss Glam Padang' suppliers: 16 rows\n",
      "  - Other suppliers: 4860 rows\n",
      "  - No supplier data: 87 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/26. Miss Glam Dr. Mansyur.csv\n",
      "\n",
      "Processing PO file: 27. Miss Glam P. Sidimpuan.csv ....\n",
      "  - Extracted location: P. SIDIMPUAN\n",
      "Processing store: P. SIDIMPUAN - 31.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 46 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/27. Miss Glam P. Sidimpuan.csv\n",
      "  - Location: P. SIDIMPUAN\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 3771\n",
      "  - 'Miss Glam Padang' suppliers: 5 rows\n",
      "  - Other suppliers: 3729 rows\n",
      "  - No supplier data: 37 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/27. Miss Glam P. Sidimpuan.csv\n",
      "\n",
      "Processing PO file: 28. Miss Glam Aceh.csv ....\n",
      "  - Extracted location: ACEH\n",
      "Processing store: ACEH - 15.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 72 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/28. Miss Glam Aceh.csv\n",
      "  - Location: ACEH\n",
      "  - Contribution: 15%\n",
      "  - Rows processed: 3552\n",
      "  - 'Miss Glam Padang' suppliers: 14 rows\n",
      "  - Other suppliers: 3493 rows\n",
      "  - No supplier data: 45 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/28. Miss Glam Aceh.csv\n",
      "\n",
      "Processing PO file: 29. Miss Glam Marpoyan.csv ....\n",
      "  - Extracted location: MARPOYAN\n",
      "Processing store: MARPOYAN - 30.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 110 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/29. Miss Glam Marpoyan.csv\n",
      "  - Location: MARPOYAN\n",
      "  - Contribution: 30%\n",
      "  - Rows processed: 4621\n",
      "  - 'Miss Glam Padang' suppliers: 19 rows\n",
      "  - Other suppliers: 4537 rows\n",
      "  - No supplier data: 65 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/29. Miss Glam Marpoyan.csv\n",
      "\n",
      "Processing PO file: 3. Miss Glam Jambi.csv ....\n",
      "  - Extracted location: JAMBI\n",
      "Processing store: JAMBI - 33.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 131 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/3. Miss Glam Jambi.csv\n",
      "  - Location: JAMBI\n",
      "  - Contribution: 33%\n",
      "  - Rows processed: 5236\n",
      "  - 'Miss Glam Padang' suppliers: 4 rows\n",
      "  - Other suppliers: 5145 rows\n",
      "  - No supplier data: 87 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/3. Miss Glam Jambi.csv\n",
      "\n",
      "Processing PO file: 30. Miss Glam Sei Penuh.csv ....\n",
      "  - Extracted location: SEI PENUH\n",
      "Processing store: SEI PENUH - 21.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 72 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/30. Miss Glam Sei Penuh.csv\n",
      "  - Location: SEI PENUH\n",
      "  - Contribution: 21%\n",
      "  - Rows processed: 3581\n",
      "  - 'Miss Glam Padang' suppliers: 27 rows\n",
      "  - Other suppliers: 3526 rows\n",
      "  - No supplier data: 28 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/30. Miss Glam Sei Penuh.csv\n",
      "\n",
      "Processing PO file: 31. Miss Glam Mayang.csv ....\n",
      "  - Extracted location: MAYANG\n",
      "Processing store: MAYANG - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 235 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/31. Miss Glam Mayang.csv\n",
      "  - Location: MAYANG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4267\n",
      "  - 'Miss Glam Padang' suppliers: 30 rows\n",
      "  - Other suppliers: 4190 rows\n",
      "  - No supplier data: 47 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/31. Miss Glam Mayang.csv\n",
      "\n",
      "Processing PO file: 32. Miss Glam Soeta.csv ....\n",
      "  - Extracted location: SOETA\n",
      "Warning: No contribution percentage found for SOETA\n",
      "Processing store: SOETA - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 2008 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/32. Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/32. Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/32. Miss Glam Soeta.csv\n",
      "  - Location: SOETA\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 4490\n",
      "  - 'Miss Glam Padang' suppliers: 123 rows\n",
      "  - Other suppliers: 4327 rows\n",
      "  - No supplier data: 40 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/32. Miss Glam Soeta.csv\n",
      "\n",
      "Processing PO file: 33. Miss Glam Balikpapan.csv ....\n",
      "  - Extracted location: BALIKPAPAN\n",
      "Warning: No contribution percentage found for BALIKPAPAN\n",
      "Processing store: BALIKPAPAN - 100.0%\n",
      "Merging with suppliers...\n",
      "Found 1586 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/33. Miss Glam Balikpapan.csv\n",
      "  - Location: BALIKPAPAN\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 4491\n",
      "  - 'Miss Glam Padang' suppliers: 84 rows\n",
      "  - Other suppliers: 4346 rows\n",
      "  - No supplier data: 61 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/33. Miss Glam Balikpapan.csv\n",
      "\n",
      "Processing PO file: 4. Miss Glam Bukittinggi.csv ....\n",
      "  - Extracted location: BUKITTINGGI\n",
      "Processing store: BUKITTINGGI - 45.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 71 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/4. Miss Glam Bukittinggi.csv\n",
      "  - Location: BUKITTINGGI\n",
      "  - Contribution: 45%\n",
      "  - Rows processed: 5315\n",
      "  - 'Miss Glam Padang' suppliers: 1 rows\n",
      "  - Other suppliers: 5250 rows\n",
      "  - No supplier data: 64 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/4. Miss Glam Bukittinggi.csv\n",
      "\n",
      "Processing PO file: 5. Miss Glam Panam.csv ....\n",
      "  - Extracted location: PANAM\n",
      "Processing store: PANAM - 46.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 100 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/5. Miss Glam Panam.csv\n",
      "  - Location: PANAM\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 5272\n",
      "  - 'Miss Glam Padang' suppliers: 0 rows\n",
      "  - Other suppliers: 5176 rows\n",
      "  - No supplier data: 96 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/5. Miss Glam Panam.csv\n",
      "\n",
      "Processing PO file: 6. Miss Glam Muaro Bungo.csv ....\n",
      "  - Extracted location: MUARO BUNGO\n",
      "Processing store: MUARO BUNGO - 42.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 95 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/6. Miss Glam Muaro Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/6. Miss Glam Muaro Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/6. Miss Glam Muaro Bungo.csv\n",
      "  - Location: MUARO BUNGO\n",
      "  - Contribution: 42%\n",
      "  - Rows processed: 4909\n",
      "  - 'Miss Glam Padang' suppliers: 7 rows\n",
      "  - Other suppliers: 4835 rows\n",
      "  - No supplier data: 67 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/6. Miss Glam Muaro Bungo.csv\n",
      "\n",
      "Processing PO file: 7. Miss Glam Lampung.csv ....\n",
      "  - Extracted location: LAMPUNG\n",
      "Processing store: LAMPUNG - 18.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 106 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/7. Miss Glam Lampung.csv\n",
      "  - Location: LAMPUNG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 4437\n",
      "  - 'Miss Glam Padang' suppliers: 17 rows\n",
      "  - Other suppliers: 4358 rows\n",
      "  - No supplier data: 62 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/7. Miss Glam Lampung.csv\n",
      "\n",
      "Processing PO file: 8. Miss Glam Bengkulu.csv ....\n",
      "  - Extracted location: BENGKULU\n",
      "Processing store: BENGKULU - 14.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 107 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/8. Miss Glam Bengkulu.csv\n",
      "  - Location: BENGKULU\n",
      "  - Contribution: 14%\n",
      "  - Rows processed: 3961\n",
      "  - 'Miss Glam Padang' suppliers: 13 rows\n",
      "  - Other suppliers: 3876 rows\n",
      "  - No supplier data: 72 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/8. Miss Glam Bengkulu.csv\n",
      "\n",
      "Processing PO file: 9. Miss Glam Medan.csv ....\n",
      "  - Extracted location: MEDAN\n",
      "Processing store: MEDAN - 46.0%\n",
      "Overriding with Padang sales data...\n",
      "Merging with suppliers...\n",
      "Found 151 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/complete/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/m2/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/output/emergency/9. Miss Glam Medan.csv\n",
      "  - Location: MEDAN\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 5763\n",
      "  - 'Miss Glam Padang' suppliers: 14 rows\n",
      "  - Other suppliers: 5640 rows\n",
      "  - No supplier data: 109 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/output/complete/9. Miss Glam Medan.csv\n",
      "\n",
      "Processing complete! Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>location</th>\n",
       "      <th>contribution_pct</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>padang_suppliers</th>\n",
       "      <th>other_suppliers</th>\n",
       "      <th>no_supplier</th>\n",
       "      <th>status</th>\n",
       "      <th>output_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Miss Glam Padang.csv</td>\n",
       "      <td>PADANG</td>\n",
       "      <td>100</td>\n",
       "      <td>6761</td>\n",
       "      <td>6623</td>\n",
       "      <td>17</td>\n",
       "      <td>121</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10. Miss Glam Palembang.csv</td>\n",
       "      <td>PALEMBANG</td>\n",
       "      <td>26</td>\n",
       "      <td>4873</td>\n",
       "      <td>10</td>\n",
       "      <td>4796</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11. Miss Glam Damar.csv</td>\n",
       "      <td>DAMAR</td>\n",
       "      <td>91</td>\n",
       "      <td>6656</td>\n",
       "      <td>0</td>\n",
       "      <td>6531</td>\n",
       "      <td>125</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12. Miss Glam Bangka.csv</td>\n",
       "      <td>BANGKA</td>\n",
       "      <td>28</td>\n",
       "      <td>4542</td>\n",
       "      <td>21</td>\n",
       "      <td>4443</td>\n",
       "      <td>78</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13. Miss Glam Payakumbuh.csv</td>\n",
       "      <td>PAYAKUMBUH</td>\n",
       "      <td>47</td>\n",
       "      <td>5304</td>\n",
       "      <td>0</td>\n",
       "      <td>5224</td>\n",
       "      <td>80</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14. Miss Glam Solok.csv</td>\n",
       "      <td>SOLOK</td>\n",
       "      <td>37</td>\n",
       "      <td>4657</td>\n",
       "      <td>0</td>\n",
       "      <td>4597</td>\n",
       "      <td>60</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15. Miss Glam Tembilahan.csv</td>\n",
       "      <td>TEMBILAHAN</td>\n",
       "      <td>27</td>\n",
       "      <td>4351</td>\n",
       "      <td>9</td>\n",
       "      <td>4277</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16. Miss Glam Lubuk Linggau.csv</td>\n",
       "      <td>LUBUK LINGGAU</td>\n",
       "      <td>26</td>\n",
       "      <td>4410</td>\n",
       "      <td>7</td>\n",
       "      <td>4345</td>\n",
       "      <td>58</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17. Miss Glam Dumai.csv</td>\n",
       "      <td>DUMAI</td>\n",
       "      <td>36</td>\n",
       "      <td>4778</td>\n",
       "      <td>0</td>\n",
       "      <td>4713</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18. Miss Glam Kedaton.csv</td>\n",
       "      <td>KEDATON</td>\n",
       "      <td>18</td>\n",
       "      <td>4212</td>\n",
       "      <td>13</td>\n",
       "      <td>4132</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19. Miss Glam Rantau Prapat.csv</td>\n",
       "      <td>RANTAU PRAPAT</td>\n",
       "      <td>27</td>\n",
       "      <td>4278</td>\n",
       "      <td>19</td>\n",
       "      <td>4191</td>\n",
       "      <td>68</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2. Miss Glam Pekanbaru.csv</td>\n",
       "      <td>PEKANBARU</td>\n",
       "      <td>60</td>\n",
       "      <td>5957</td>\n",
       "      <td>4</td>\n",
       "      <td>5847</td>\n",
       "      <td>106</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20. Miss Glam Tanjung Pinang.csv</td>\n",
       "      <td>TANJUNG PINANG</td>\n",
       "      <td>19</td>\n",
       "      <td>4037</td>\n",
       "      <td>8</td>\n",
       "      <td>3961</td>\n",
       "      <td>68</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21. Miss Glam Sutomo.csv</td>\n",
       "      <td>SUTOMO</td>\n",
       "      <td>49</td>\n",
       "      <td>5706</td>\n",
       "      <td>4</td>\n",
       "      <td>5625</td>\n",
       "      <td>77</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22. Miss Glam Pasaman Barat.csv</td>\n",
       "      <td>PASAMAN BARAT</td>\n",
       "      <td>17</td>\n",
       "      <td>3842</td>\n",
       "      <td>6</td>\n",
       "      <td>3794</td>\n",
       "      <td>42</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23. Miss Glam Halat.csv</td>\n",
       "      <td>HALAT</td>\n",
       "      <td>31</td>\n",
       "      <td>4805</td>\n",
       "      <td>15</td>\n",
       "      <td>4696</td>\n",
       "      <td>94</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24. Miss Glam Duri.csv</td>\n",
       "      <td>DURI</td>\n",
       "      <td>28</td>\n",
       "      <td>3968</td>\n",
       "      <td>2</td>\n",
       "      <td>3914</td>\n",
       "      <td>52</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25. Miss Glam Sudirman.csv</td>\n",
       "      <td>SUDIRMAN</td>\n",
       "      <td>44</td>\n",
       "      <td>5632</td>\n",
       "      <td>4</td>\n",
       "      <td>5530</td>\n",
       "      <td>98</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26. Miss Glam Dr. Mansyur.csv</td>\n",
       "      <td>DR. MANSYUR</td>\n",
       "      <td>25</td>\n",
       "      <td>4963</td>\n",
       "      <td>16</td>\n",
       "      <td>4860</td>\n",
       "      <td>87</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27. Miss Glam P. Sidimpuan.csv</td>\n",
       "      <td>P. SIDIMPUAN</td>\n",
       "      <td>31</td>\n",
       "      <td>3771</td>\n",
       "      <td>5</td>\n",
       "      <td>3729</td>\n",
       "      <td>37</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28. Miss Glam Aceh.csv</td>\n",
       "      <td>ACEH</td>\n",
       "      <td>15</td>\n",
       "      <td>3552</td>\n",
       "      <td>14</td>\n",
       "      <td>3493</td>\n",
       "      <td>45</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29. Miss Glam Marpoyan.csv</td>\n",
       "      <td>MARPOYAN</td>\n",
       "      <td>30</td>\n",
       "      <td>4621</td>\n",
       "      <td>19</td>\n",
       "      <td>4537</td>\n",
       "      <td>65</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3. Miss Glam Jambi.csv</td>\n",
       "      <td>JAMBI</td>\n",
       "      <td>33</td>\n",
       "      <td>5236</td>\n",
       "      <td>4</td>\n",
       "      <td>5145</td>\n",
       "      <td>87</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30. Miss Glam Sei Penuh.csv</td>\n",
       "      <td>SEI PENUH</td>\n",
       "      <td>21</td>\n",
       "      <td>3581</td>\n",
       "      <td>27</td>\n",
       "      <td>3526</td>\n",
       "      <td>28</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31. Miss Glam Mayang.csv</td>\n",
       "      <td>MAYANG</td>\n",
       "      <td>18</td>\n",
       "      <td>4267</td>\n",
       "      <td>30</td>\n",
       "      <td>4190</td>\n",
       "      <td>47</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32. Miss Glam Soeta.csv</td>\n",
       "      <td>SOETA</td>\n",
       "      <td>100</td>\n",
       "      <td>4490</td>\n",
       "      <td>123</td>\n",
       "      <td>4327</td>\n",
       "      <td>40</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33. Miss Glam Balikpapan.csv</td>\n",
       "      <td>BALIKPAPAN</td>\n",
       "      <td>100</td>\n",
       "      <td>4491</td>\n",
       "      <td>84</td>\n",
       "      <td>4346</td>\n",
       "      <td>61</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4. Miss Glam Bukittinggi.csv</td>\n",
       "      <td>BUKITTINGGI</td>\n",
       "      <td>45</td>\n",
       "      <td>5315</td>\n",
       "      <td>1</td>\n",
       "      <td>5250</td>\n",
       "      <td>64</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5. Miss Glam Panam.csv</td>\n",
       "      <td>PANAM</td>\n",
       "      <td>46</td>\n",
       "      <td>5272</td>\n",
       "      <td>0</td>\n",
       "      <td>5176</td>\n",
       "      <td>96</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6. Miss Glam Muaro Bungo.csv</td>\n",
       "      <td>MUARO BUNGO</td>\n",
       "      <td>42</td>\n",
       "      <td>4909</td>\n",
       "      <td>7</td>\n",
       "      <td>4835</td>\n",
       "      <td>67</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7. Miss Glam Lampung.csv</td>\n",
       "      <td>LAMPUNG</td>\n",
       "      <td>18</td>\n",
       "      <td>4437</td>\n",
       "      <td>17</td>\n",
       "      <td>4358</td>\n",
       "      <td>62</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8. Miss Glam Bengkulu.csv</td>\n",
       "      <td>BENGKULU</td>\n",
       "      <td>14</td>\n",
       "      <td>3961</td>\n",
       "      <td>13</td>\n",
       "      <td>3876</td>\n",
       "      <td>72</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9. Miss Glam Medan.csv</td>\n",
       "      <td>MEDAN</td>\n",
       "      <td>46</td>\n",
       "      <td>5763</td>\n",
       "      <td>14</td>\n",
       "      <td>5640</td>\n",
       "      <td>109</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/outpu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file        location contribution_pct  \\\n",
       "0            1. Miss Glam Padang.csv          PADANG              100   \n",
       "1        10. Miss Glam Palembang.csv       PALEMBANG               26   \n",
       "2            11. Miss Glam Damar.csv           DAMAR               91   \n",
       "3           12. Miss Glam Bangka.csv          BANGKA               28   \n",
       "4       13. Miss Glam Payakumbuh.csv      PAYAKUMBUH               47   \n",
       "5            14. Miss Glam Solok.csv           SOLOK               37   \n",
       "6       15. Miss Glam Tembilahan.csv      TEMBILAHAN               27   \n",
       "7    16. Miss Glam Lubuk Linggau.csv   LUBUK LINGGAU               26   \n",
       "8            17. Miss Glam Dumai.csv           DUMAI               36   \n",
       "9          18. Miss Glam Kedaton.csv         KEDATON               18   \n",
       "10   19. Miss Glam Rantau Prapat.csv   RANTAU PRAPAT               27   \n",
       "11        2. Miss Glam Pekanbaru.csv       PEKANBARU               60   \n",
       "12  20. Miss Glam Tanjung Pinang.csv  TANJUNG PINANG               19   \n",
       "13          21. Miss Glam Sutomo.csv          SUTOMO               49   \n",
       "14   22. Miss Glam Pasaman Barat.csv   PASAMAN BARAT               17   \n",
       "15           23. Miss Glam Halat.csv           HALAT               31   \n",
       "16            24. Miss Glam Duri.csv            DURI               28   \n",
       "17        25. Miss Glam Sudirman.csv        SUDIRMAN               44   \n",
       "18     26. Miss Glam Dr. Mansyur.csv     DR. MANSYUR               25   \n",
       "19    27. Miss Glam P. Sidimpuan.csv    P. SIDIMPUAN               31   \n",
       "20            28. Miss Glam Aceh.csv            ACEH               15   \n",
       "21        29. Miss Glam Marpoyan.csv        MARPOYAN               30   \n",
       "22            3. Miss Glam Jambi.csv           JAMBI               33   \n",
       "23       30. Miss Glam Sei Penuh.csv       SEI PENUH               21   \n",
       "24          31. Miss Glam Mayang.csv          MAYANG               18   \n",
       "25           32. Miss Glam Soeta.csv           SOETA              100   \n",
       "26      33. Miss Glam Balikpapan.csv      BALIKPAPAN              100   \n",
       "27      4. Miss Glam Bukittinggi.csv     BUKITTINGGI               45   \n",
       "28            5. Miss Glam Panam.csv           PANAM               46   \n",
       "29      6. Miss Glam Muaro Bungo.csv     MUARO BUNGO               42   \n",
       "30          7. Miss Glam Lampung.csv         LAMPUNG               18   \n",
       "31         8. Miss Glam Bengkulu.csv        BENGKULU               14   \n",
       "32            9. Miss Glam Medan.csv           MEDAN               46   \n",
       "\n",
       "    total_rows  padang_suppliers  other_suppliers  no_supplier   status  \\\n",
       "0         6761              6623               17          121  Success   \n",
       "1         4873                10             4796           67  Success   \n",
       "2         6656                 0             6531          125  Success   \n",
       "3         4542                21             4443           78  Success   \n",
       "4         5304                 0             5224           80  Success   \n",
       "5         4657                 0             4597           60  Success   \n",
       "6         4351                 9             4277           65  Success   \n",
       "7         4410                 7             4345           58  Success   \n",
       "8         4778                 0             4713           65  Success   \n",
       "9         4212                13             4132           67  Success   \n",
       "10        4278                19             4191           68  Success   \n",
       "11        5957                 4             5847          106  Success   \n",
       "12        4037                 8             3961           68  Success   \n",
       "13        5706                 4             5625           77  Success   \n",
       "14        3842                 6             3794           42  Success   \n",
       "15        4805                15             4696           94  Success   \n",
       "16        3968                 2             3914           52  Success   \n",
       "17        5632                 4             5530           98  Success   \n",
       "18        4963                16             4860           87  Success   \n",
       "19        3771                 5             3729           37  Success   \n",
       "20        3552                14             3493           45  Success   \n",
       "21        4621                19             4537           65  Success   \n",
       "22        5236                 4             5145           87  Success   \n",
       "23        3581                27             3526           28  Success   \n",
       "24        4267                30             4190           47  Success   \n",
       "25        4490               123             4327           40  Success   \n",
       "26        4491                84             4346           61  Success   \n",
       "27        5315                 1             5250           64  Success   \n",
       "28        5272                 0             5176           96  Success   \n",
       "29        4909                 7             4835           67  Success   \n",
       "30        4437                17             4358           62  Success   \n",
       "31        3961                13             3876           72  Success   \n",
       "32        5763                14             5640          109  Success   \n",
       "\n",
       "                                          output_path  \n",
       "0   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "1   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "2   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "3   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "4   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "5   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "6   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "7   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "8   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "9   /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "10  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "11  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "12  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "13  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "14  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "15  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "16  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "17  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "18  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "19  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "20  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "21  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "22  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "23  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "24  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "25  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "26  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "27  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "28  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "29  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "30  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "31  /Users/andresuchitra/dev/missglam/autopo/outpu...  \n",
       "32  /Users/andresuchitra/dev/missglam/autopo/outpu...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the last processed file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama</th>\n",
       "      <th>Toko</th>\n",
       "      <th>Stok</th>\n",
       "      <th>Daily Sales</th>\n",
       "      <th>Max. Daily Sales</th>\n",
       "      <th>Lead Time</th>\n",
       "      <th>Max. Lead Time</th>\n",
       "      <th>Min. Order</th>\n",
       "      <th>...</th>\n",
       "      <th>Reorder point</th>\n",
       "      <th>Stock cover 30 days</th>\n",
       "      <th>current_stock_days_cover</th>\n",
       "      <th>is_open_po</th>\n",
       "      <th>initial_qty_po</th>\n",
       "      <th>emergency_po_qty</th>\n",
       "      <th>updated_regular_po_qty</th>\n",
       "      <th>final_updated_regular_po_qty</th>\n",
       "      <th>emergency_po_cost</th>\n",
       "      <th>final_updated_regular_po_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>10400614911</td>\n",
       "      <td>ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>200.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>11200219810</td>\n",
       "      <td>ACNAWAY Mugwort Blackhead Treatment Step 1 17ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>11200219943</td>\n",
       "      <td>ACNAWAY Mugwort Blackhead Treatment Step 2 17ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>10400517459</td>\n",
       "      <td>ACNAWAY Mugwort Daily Sunscreen Only For Acne ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>44.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>101001107647</td>\n",
       "      <td>ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>319590.00</td>\n",
       "      <td>31959.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>YOUVIT</td>\n",
       "      <td>60100406456</td>\n",
       "      <td>YOUVIT Ezzleep 7 Gummies 28gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28428.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5759</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402932</td>\n",
       "      <td>YU CHUN Mei Cordyceps Brightening Cleanser 100ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>86.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402703</td>\n",
       "      <td>YU CHUN Mei Cordyceps Lightening Day Cream 30gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>72.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402710</td>\n",
       "      <td>YU CHUN Mei Cordyceps Lightening Night Cream 30g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>44289.00</td>\n",
       "      <td>132867.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>YU CHUN</td>\n",
       "      <td>8997014402918</td>\n",
       "      <td>YU CHUN Mei Serum Whitening Essence 30ml</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>217.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5763 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand            SKU  \\\n",
       "0     ACNAWAY    10400614911   \n",
       "1     ACNAWAY    11200219810   \n",
       "2     ACNAWAY    11200219943   \n",
       "3     ACNAWAY    10400517459   \n",
       "4     ACNAWAY   101001107647   \n",
       "...       ...            ...   \n",
       "5758   YOUVIT    60100406456   \n",
       "5759  YU CHUN  8997014402932   \n",
       "5760  YU CHUN  8997014402703   \n",
       "5761  YU CHUN  8997014402710   \n",
       "5762  YU CHUN  8997014402918   \n",
       "\n",
       "                                                   Nama             Toko  \\\n",
       "0     ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...  Miss Glam Medan   \n",
       "1       ACNAWAY Mugwort Blackhead Treatment Step 1 17ml  Miss Glam Medan   \n",
       "2       ACNAWAY Mugwort Blackhead Treatment Step 2 17ml  Miss Glam Medan   \n",
       "3     ACNAWAY Mugwort Daily Sunscreen Only For Acne ...  Miss Glam Medan   \n",
       "4     ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...  Miss Glam Medan   \n",
       "...                                                 ...              ...   \n",
       "5758                      YOUVIT Ezzleep 7 Gummies 28gr  Miss Glam Medan   \n",
       "5759   YU CHUN Mei Cordyceps Brightening Cleanser 100ml  Miss Glam Medan   \n",
       "5760    YU CHUN Mei Cordyceps Lightening Day Cream 30gr  Miss Glam Medan   \n",
       "5761   YU CHUN Mei Cordyceps Lightening Night Cream 30g  Miss Glam Medan   \n",
       "5762           YU CHUN Mei Serum Whitening Essence 30ml  Miss Glam Medan   \n",
       "\n",
       "      Stok  Daily Sales  Max. Daily Sales  Lead Time  Max. Lead Time  \\\n",
       "0     6.00         0.03              1.00       5.00           28.00   \n",
       "1     0.00         0.00              0.00       0.00            0.00   \n",
       "2     0.00         0.00              0.00       0.00            0.00   \n",
       "3    12.00         0.27              3.00       5.00           28.00   \n",
       "4     0.00         0.35              1.84       5.00           28.00   \n",
       "...    ...          ...               ...        ...             ...   \n",
       "5758  0.00         0.03              1.00       1.00            2.00   \n",
       "5759  2.00         0.02              0.46       1.00            2.00   \n",
       "5760  4.00         0.06              0.46       1.00            2.00   \n",
       "5761  0.00         0.04              0.46       1.00            2.00   \n",
       "5762  5.00         0.02              0.46       1.00            2.00   \n",
       "\n",
       "      Min. Order  ...  Reorder point  Stock cover 30 days  \\\n",
       "0           1.00  ...             29                    1   \n",
       "1           1.00  ...              0                    0   \n",
       "2           1.00  ...              0                    0   \n",
       "3           1.00  ...             85                    9   \n",
       "4           1.00  ...             52                   11   \n",
       "...          ...  ...            ...                  ...   \n",
       "5758        1.00  ...              3                    1   \n",
       "5759        3.00  ...              2                    1   \n",
       "5760        3.00  ...              2                    2   \n",
       "5761        3.00  ...              2                    2   \n",
       "5762        3.00  ...              2                    1   \n",
       "\n",
       "      current_stock_days_cover  is_open_po  initial_qty_po  emergency_po_qty  \\\n",
       "0                       200.00           0               0                 0   \n",
       "1                         0.00           1               0                 0   \n",
       "2                         0.00           1               0                 0   \n",
       "3                        44.44           0               0                 0   \n",
       "4                         0.00           1              11                10   \n",
       "...                        ...         ...             ...               ...   \n",
       "5758                      0.00           1               1                 1   \n",
       "5759                     86.96           0               0                 0   \n",
       "5760                     72.46           0               0                 0   \n",
       "5761                      0.00           1               2                 1   \n",
       "5762                    217.39           0               0                 0   \n",
       "\n",
       "      updated_regular_po_qty final_updated_regular_po_qty emergency_po_cost  \\\n",
       "0                          0                            0              0.00   \n",
       "1                          0                            0              0.00   \n",
       "2                          0                            0              0.00   \n",
       "3                          0                            0              0.00   \n",
       "4                          1                            1         319590.00   \n",
       "...                      ...                          ...               ...   \n",
       "5758                       0                            0          28428.00   \n",
       "5759                       0                            0              0.00   \n",
       "5760                       0                            0              0.00   \n",
       "5761                       1                            3          44289.00   \n",
       "5762                       0                            0              0.00   \n",
       "\n",
       "      final_updated_regular_po_cost  \n",
       "0                              0.00  \n",
       "1                              0.00  \n",
       "2                              0.00  \n",
       "3                              0.00  \n",
       "4                          31959.00  \n",
       "...                             ...  \n",
       "5758                           0.00  \n",
       "5759                           0.00  \n",
       "5760                           0.00  \n",
       "5761                      132867.00  \n",
       "5762                           0.00  \n",
       "\n",
       "[5763 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import display\n",
    "import locale\n",
    "from locale import atof\n",
    "import numpy as np\n",
    "from openpyxl.styles import numbers\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "\n",
    "_patch_openpyxl_number_casting()\n",
    "\n",
    "# Apply the formatting to numeric columns in your final output\n",
    "def format_dataframe_display(df):\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # Apply formatting to numeric columns\n",
    "    for col in df_display.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: format_id_number(x, 2) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/Users/andresuchitra/dev/missglam/autopo')\n",
    "SUPPLIER_PATH = BASE_DIR / 'data/supplier.csv'\n",
    "RAWPO_DIR = BASE_DIR / 'data/rawpo/csv'\n",
    "RAWPO_XLSX_DIR = BASE_DIR / 'data/rawpo/xlsx'\n",
    "STORE_CONTRIBUTION_PATH = BASE_DIR / 'data/store_contribution.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'output/complete'\n",
    "OUTPUT_EXCEL_DIR = BASE_DIR / 'output/excel'\n",
    "OUTPUT_M2_DIR = BASE_DIR / 'output/m2'\n",
    "OUTPUT_EMERGENCY_DIR = BASE_DIR / 'output/emergency'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EXCEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_M2_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EMERGENCY_DIR, exist_ok=True)\n",
    "\n",
    "def load_store_contribution(store_contribution_path):\n",
    "    \"\"\"Load and prepare store contribution data.\"\"\"\n",
    "    store_contrib = pd.read_csv(store_contribution_path, header=None, \n",
    "                              names=['store', 'contribution_pct'])\n",
    "    # Convert store names to lowercase for case-insensitive matching\n",
    "    store_contrib['store_lower'] = store_contrib['store'].str.lower()\n",
    "    return store_contrib\n",
    "\n",
    "def get_contribution_pct(location, store_contrib):\n",
    "    \"\"\"Get contribution percentage for a given location.\"\"\"\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    contrib_row = store_contrib[store_contrib['store_lower'] == location_lower]\n",
    "    if not contrib_row.empty:\n",
    "        return contrib_row['contribution_pct'].values[0]\n",
    "    print(f\"Warning: No contribution percentage found for {location}\")\n",
    "\n",
    "    return 100  # Default to 100% if not found\n",
    "\n",
    "def load_supplier_data(supplier_path):\n",
    "    \"\"\"Load and clean supplier data.\"\"\"\n",
    "    print(f\"Loading supplier data: {supplier_path}\")\n",
    "    df = pd.read_csv(supplier_path, sep=';', decimal=',').fillna('')\n",
    "    df['Nama Brand'] = df['Nama Brand'].str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier information.\"\"\"\n",
    "    print(\"Merging with suppliers...\")\n",
    "    \n",
    "    # Clean supplier data\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    \n",
    "    # Deduplicate to prevent row explosion - Unique Brand+Store\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "    \n",
    "    # Ensure PO data has clean columns for merging\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "    \n",
    "    # 1. Primary Merge: Match on Brand AND Store (Toko)\n",
    "    # This prioritizes the specific supplier for that store\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "    \n",
    "    # 2. Fallback: For unmatched rows, try to find ANY supplier for that Brand\n",
    "    # Identify rows where merge failed (Nama Brand is NaN)\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    \n",
    "    if unmatched_mask.any():\n",
    "        print(f\"Found {unmatched_mask.sum()} rows without direct store match. Attempting fallback...\")\n",
    "        \n",
    "        # Get the unmatched rows and drop the empty supplier columns\n",
    "        unmatched_rows = merged_df[unmatched_mask].copy()\n",
    "        supplier_cols = [col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand']\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols)\n",
    "        \n",
    "        # Create fallback supplier list (one per brand)\n",
    "        # We take the first one found for each brand\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        \n",
    "        # Merge unmatched rows with fallback suppliers\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "        \n",
    "        # Combine the initially matched rows with the fallback-matched rows\n",
    "        matched_initial = merged_df[~unmatched_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "    \n",
    "    # Clean up supplier columns\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def calculate_inventory_metrics(df_clean):\n",
    "    \"\"\"\n",
    "    Calculate various inventory metrics including safety stock, reorder points, and PO quantities.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Input dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added calculated columns\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "    df = df_clean.copy()\n",
    "    \n",
    "    # Set display options\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "    # Normalise stock column name\n",
    "    stock_col = 'Stok' if 'Stok' in df.columns else 'Stock'\n",
    "\n",
    "    # Force the columns we need into numeric form\n",
    "    numeric_cols = [\n",
    "        stock_col, 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "        'Max. Lead Time', 'Sedang PO', 'HPP', 'Lead Time Sedang PO'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    try:\n",
    "        # 1. Safety stock calculation\n",
    "        df['Safety stock'] = (df['Max. Daily Sales'] * df['Max. Lead Time']) - (df['Daily Sales'] * df['Lead Time'])\n",
    "        df['Safety stock'] = df['Safety stock'].apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 2. Reorder point calculation\n",
    "        df['Reorder point'] = np.ceil((df['Daily Sales'] * df['Lead Time']) + df['Safety stock']).fillna(0).astype(int)\n",
    "        \n",
    "        # 3. Stock cover for 30 days\n",
    "        df['Stock cover 30 days'] = (df['Daily Sales'] * 30).apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 4. Current stock days cover\n",
    "        df['current_stock_days_cover'] = np.where(\n",
    "            df['Daily Sales'] > 0,\n",
    "            df[stock_col] / df['Daily Sales'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # 5. Is open PO flag\n",
    "        df['is_open_po'] = np.where(\n",
    "            (df['current_stock_days_cover'] < 30) & \n",
    "            (df['Stok'] <= df['Reorder point']), 1, 0\n",
    "        )\n",
    "        \n",
    "        # 6. Initial PO quantity\n",
    "        df['initial_qty_po'] = df['Stock cover 30 days'] - df[stock_col] - df.get('Sedang PO', 0)\n",
    "        df['initial_qty_po'] = (\n",
    "            pd.Series(\n",
    "                np.where(df['is_open_po'] == 1, df['initial_qty_po'], 0),\n",
    "                index=df.index\n",
    "            )\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 7. Emergency PO quantity\n",
    "        df['emergency_po_qty'] = np.where(\n",
    "            df.get('Sedang PO', 0) > 0,\n",
    "            np.maximum(0, (df['Lead Time Sedang PO'] - df['current_stock_days_cover']) * df['Daily Sales']),\n",
    "            np.ceil((df['Max. Lead Time'] - df['current_stock_days_cover']) * df['Daily Sales'])\n",
    "        )\n",
    "        \n",
    "        # Clean up emergency PO quantities\n",
    "        df['emergency_po_qty'] = (\n",
    "            df['emergency_po_qty']\n",
    "            .replace([np.inf, -np.inf], 0)\n",
    "            .fillna(0)\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 8. Updated regular PO quantity\n",
    "        df['updated_regular_po_qty'] = (df['initial_qty_po'] - df['emergency_po_qty']).clip(lower=0).astype(int)\n",
    "        \n",
    "        # 9. Final updated regular PO quantity (enforce minimum order)\n",
    "        df['final_updated_regular_po_qty'] = np.where(\n",
    "            (df['updated_regular_po_qty'] > 0) & \n",
    "            (df['updated_regular_po_qty'] < df['Min. Order']),\n",
    "            df['Min. Order'],\n",
    "            df['updated_regular_po_qty']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 10. Calculate costs if by multiplying with contribution percentage\n",
    "        df['emergency_po_cost'] = (df['emergency_po_qty'] * df['HPP']).round(2)\n",
    "        df['final_updated_regular_po_cost'] = (df['final_updated_regular_po_qty'] * df['HPP']).round(2)\n",
    "        \n",
    "        # Clean up any remaining NaN or infinite values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_inventory_metrics: {str(e)}\")\n",
    "        return df_clean\n",
    "\n",
    "def clean_po_data(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert SKU to string and clean it\n",
    "        if 'SKU' in df.columns:\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP', 'Min. Order'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "        df['contribution_ratio'] = contribution_pct / 100\n",
    "\n",
    "        # Add default values for other required columns\n",
    "        if 'Lead Time Sedang PO' not in df.columns:\n",
    "            df['Lead Time Sedang PO'] = 5  # Default value\n",
    "\n",
    "        location_upper = location.upper()\n",
    "        exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        print(f\"Processing store: {location} - {contribution_pct}%\")\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        if padang_sales is not None:\n",
    "            # Ensure padang_sales has the required columns\n",
    "            padang_sales = padang_sales.copy()\n",
    "            padang_sales.columns = padang_sales.columns.str.strip()\n",
    "            \n",
    "            # Convert SKU to string in both dataframes\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "            padang_sales['SKU'] = padang_sales['SKU'].astype(str).str.strip()\n",
    "            \n",
    "            padang_skus = set(padang_sales['SKU'].unique())\n",
    "            df['Is in Padang'] = df['SKU'].isin(padang_skus).astype(int)\n",
    "        else:\n",
    "            print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "            df['Is in Padang'] = 0\n",
    "\n",
    "        if not needs_padang_override:\n",
    "            return df\n",
    "\n",
    "        if padang_sales is None:\n",
    "            raise ValueError(\n",
    "                \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "                \"or any store with contribution < 100%.\"\n",
    "            )\n",
    "\n",
    "        # Process Padang sales data\n",
    "        padang_df = padang_sales.copy()\n",
    "        padang_df.columns = padang_df.columns.str.strip()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        missing_cols = [col for col in required_cols if col not in padang_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in Padang sales data: {missing_cols}\")\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        if 'Daily Sales' in df.columns:\n",
    "            df['Orig Daily Sales'] = df['Daily Sales']\n",
    "        if 'Max. Daily Sales' in df.columns:\n",
    "            df['Orig Max. Daily Sales'] = df['Max. Daily Sales']\n",
    "\n",
    "        print(\"Overriding with Padang sales data...\")\n",
    "        \n",
    "        # Ensure SKU is string in both dataframes before merge\n",
    "        df['SKU'] = df['SKU'].astype(str)\n",
    "        padang_df['SKU'] = padang_df['SKU'].astype(str)\n",
    "        \n",
    "        # Merge with Padang's sales data\n",
    "        df = df.merge(\n",
    "            padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "                'Daily Sales': 'Padang Daily Sales',\n",
    "                'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "            }),\n",
    "            on='SKU',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "            df['Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Daily Sales']\n",
    "            )\n",
    "            \n",
    "        if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "            df['Max. Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Max Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Max. Daily Sales']\n",
    "            )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        columns_to_drop = [\n",
    "            'Padang Daily Sales', 'Padang Max Daily Sales', \n",
    "            'Orig Daily Sales', 'Orig Max. Daily Sales'\n",
    "        ]\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_store_name_from_filename(filename):\n",
    "    \"\"\"Extract store name from filename, handling different patterns.\"\"\"\n",
    "    # Remove file extension and split by spaces\n",
    "    name_parts = Path(filename).stem.split()\n",
    "    \n",
    "    # Handle cases like \"002 Miss Glam Pekanbaru.csv\" -> \"Pekanbaru\"\n",
    "    # or \"01 Miss Glam Padang.csv\" -> \"Padang\"\n",
    "    if len(name_parts) >= 3 and name_parts[1].lower() == 'miss' and name_parts[2].lower() == 'glam':\n",
    "        return ' '.join(name_parts[3:]).strip().upper()\n",
    "    elif len(name_parts) >= 2 and name_parts[0].lower() == 'miss' and name_parts[1].lower() == 'glam':\n",
    "        return ' '.join(name_parts[2:]).strip().upper()\n",
    "    # Fallback: take everything after the first space\n",
    "    elif ' ' in filename:\n",
    "        return ' '.join(name_parts[1:]).strip().upper()\n",
    "    return name_parts[0].upper()\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # List of (separator, encoding) combinations to try\n",
    "    formats_to_try = [\n",
    "        (',', 'utf-8'),      # Standard CSV with comma\n",
    "        (';', 'utf-8'),      # Semicolon with UTF-8\n",
    "        (',', 'latin1'),     # Comma with Latin1\n",
    "        (';', 'latin1'),     # Semicolon with Latin1\n",
    "        (',', 'cp1252'),     # Windows-1252 encoding\n",
    "        (';', 'cp1252')\n",
    "    ]\n",
    "    \n",
    "    for sep, enc in formats_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=sep,\n",
    "                decimal=',',\n",
    "                thousands='.',\n",
    "                encoding=enc,\n",
    "                engine='python'  # More consistent behavior with Python engine\n",
    "            )\n",
    "            # If we get here, the file was read successfully\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError, pd.errors.EmptyDataError) as e:\n",
    "            continue  # Try next format\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading {file_path} with sep='{sep}', encoding='{enc}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we get here, all attempts failed\n",
    "    print(f\"Failed to read {file_path} with any known format\")\n",
    "    return None\n",
    "\n",
    "def process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=False):\n",
    "    \"\"\"Process a single PO file and return merged data and summary.\"\"\"\n",
    "    print(f\"\\nProcessing PO file: {file_path.name} ....\")\n",
    "    \n",
    "    try:\n",
    "        # Extract location from filename using the new function\n",
    "        location = get_store_name_from_filename(file_path.name)\n",
    "        print(f\"  - Extracted location: {location}\")  # Debug print\n",
    "        \n",
    "        contribution_pct = get_contribution_pct(location, store_contrib)\n",
    "        \n",
    "        # Read the CSV with error handling\n",
    "        try:\n",
    "            # Try reading with different encodings if needed\n",
    "            if is_excel_folder:\n",
    "                df = read_excel_file(file_path)\n",
    "            else:\n",
    "                df = read_csv_file(file_path)\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "                \n",
    "            # Clean the data\n",
    "            df_clean = clean_po_data(df,location, contribution_pct, df_padang)\n",
    "            \n",
    "            # Skip if cleaning failed\n",
    "            if df_clean.empty:\n",
    "                raise ValueError(\"Data cleaning failed\")\n",
    "            \n",
    "            # Merge with suppliers\n",
    "            merged_df = merge_with_suppliers(df_clean, supplier_df)\n",
    "\n",
    "            # calculate metrics PO\n",
    "            merged_df = calculate_inventory_metrics(merged_df)\n",
    "            \n",
    "            # Generate summary\n",
    "            padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "            other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                                  (merged_df['Nama Store'] != '')).sum()\n",
    "            \n",
    "            summary = {\n",
    "                'file': file_path.name,\n",
    "                'location': location,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'total_rows': len(merged_df),\n",
    "                'padang_suppliers': int(padang_count),\n",
    "                'other_suppliers': int(other_supplier_count),\n",
    "                'no_supplier': int((merged_df['Nama Store'] == '').sum()),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            return merged_df, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file data: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {file_path.name}: {str(e)}\"\n",
    "        print(f\"  - {error_msg}\")\n",
    "        return None, {\n",
    "            'file': file_path.name,\n",
    "            'location': location if 'location' in locals() else 'Unknown',\n",
    "            'contribution_pct': contribution_pct if 'contribution_pct' in locals() else 0,\n",
    "            'total_rows': 0,\n",
    "            'padang_suppliers': 0,\n",
    "            'other_suppliers': 0,\n",
    "            'no_supplier': 0,\n",
    "            'status': f\"Error: {str(e)[:100]}\"  # Truncate long error messages\n",
    "        }\n",
    "\n",
    "def load_padang_data(padang_path):\n",
    "    \"\"\"Load Padang data from either CSV or Excel file.\n",
    "    \n",
    "    Args:\n",
    "        padang_path: Path to the input file (CSV or XLSX)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and cleaned Padang data\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported or file cannot be read\n",
    "    \"\"\"\n",
    "    print(f\"Loading Padang data from {padang_path}...\")\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = str(padang_path).lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(padang_path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(padang_path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Padang data with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Padang data from {padang_path}: {str(e)}\")\n",
    "\n",
    "def format_number_for_csv(x):\n",
    "    \"\"\"Format numbers for CSV output with Indonesian locale (comma as decimal, dot as thousand)\"\"\"\n",
    "    if pd.isna(x) or x == '':\n",
    "        return x\n",
    "    try:\n",
    "        if isinstance(x, (int, float)):\n",
    "            if x == int(x):  # Whole number\n",
    "                return f\"{int(x):,d}\".replace(\",\", \".\")\n",
    "            else:  # Decimal number\n",
    "                return f\"{x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def clean_and_convert(df):\n",
    "    \"\"\"Clean and convert DataFrame columns to appropriate types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string first to handle NaN/None consistently\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Define NA values that should be treated as empty/missing\n",
    "    na_values = list(NA_VALUES)\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        # Replace NA values with empty string first (treating them as literals, not regex)\n",
    "        df[col] = df[col].replace(na_values, '', regex=False)\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if df[col].empty:\n",
    "            continue\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if col in NUMERIC_COLUMNS:\n",
    "            # Convert to numeric, coercing errors to NaN, then fill with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            # For non-numeric columns, ensure they're strings and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace empty strings with NaN and then fill with empty string\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            df[col] = df[col].replace('', np.nan).fillna('').infer_objects(copy=False)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file with robust error handling for problematic values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nReading excel file: {file_path.name}...\")\n",
    "        \n",
    "        # First, read the file with openpyxl directly to handle the data more carefully\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = load_workbook(\n",
    "            filename=file_path,\n",
    "            read_only=True,    # Read-only mode is faster and uses less memory\n",
    "            data_only=True,    # Get the stored value instead of the formula\n",
    "            keep_links=False   # Don't load external links\n",
    "        )\n",
    "        \n",
    "        # Get the first sheet\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Get headers from the first row\n",
    "        headers = []\n",
    "        for idx, cell in enumerate(next(ws.iter_rows(values_only=True))):\n",
    "            header = str(cell).strip() if cell not in (None, '') else f\"Column_{idx + 1}\"\n",
    "            headers.append(header)\n",
    "        \n",
    "        # Initialize data rows\n",
    "        data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for row in ws.iter_rows(min_row=2, values_only=True):  # Skip header row\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                if cell is None:\n",
    "                    row_data.append('')\n",
    "                    continue\n",
    "\n",
    "                cell_str = str(cell).strip()\n",
    "                if cell_str.upper() in NA_VALUES:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    row_data.append(cell_str)\n",
    "            \n",
    "            # Only add row if it has data\n",
    "            if any(cell != '' for cell in row_data):\n",
    "                data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Normalize column data types\n",
    "        df = clean_and_convert(df)\n",
    "        \n",
    "        print(f\" Successfully processed {file_path.name} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {file_path.name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_file(df, file_path, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame to file with consistent extension and content type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path object or string for the output file\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments to pass to to_csv or to_excel\n",
    "        \n",
    "    Returns:\n",
    "        Path: The path where the file was saved\n",
    "    \"\"\"\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the correct file extension\n",
    "    if not file_path.suffix.lower() == f'.{file_format}':\n",
    "        file_path = file_path.with_suffix(f'.{file_format}')\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_output = df.copy()\n",
    "    \n",
    "    # Common preprocessing\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].astype(str).str.strip()\n",
    "        if file_format == 'xlsx':\n",
    "            # For Excel, wrap SKU in =\"...\" to preserve leading zeros\n",
    "            df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Format numbers for CSV if needed\n",
    "    if file_format == 'csv':\n",
    "        numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "    \n",
    "    # Save based on format\n",
    "    if file_format == 'csv':\n",
    "        df_output.to_csv(\n",
    "            file_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig',\n",
    "            **kwargs\n",
    "        )\n",
    "    elif file_format == 'xlsx':\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "            df_output.to_excel(writer, index=False, **kwargs)\n",
    "            \n",
    "            # Format SKU column as text in Excel\n",
    "            if 'SKU' in df_output.columns:\n",
    "                ws = writer.sheets[list(writer.sheets.keys())[0]]\n",
    "                sku_col_idx = df_output.columns.get_loc(\"SKU\") + 1\n",
    "                for row in ws.iter_rows(\n",
    "                    min_row=2,  # Skip header\n",
    "                    max_row=ws.max_row,\n",
    "                    min_col=sku_col_idx,\n",
    "                    max_col=sku_col_idx\n",
    "                ):\n",
    "                    for cell in row:\n",
    "                        cell.number_format = numbers.FORMAT_TEXT\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "    \n",
    "    print(f\"File saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_to_complete_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save Complete format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "\n",
    "    return save_file(df, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_m2_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save M2 format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_filtered = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[['Toko', 'SKU', 'HPP', 'final_updated_regular_po_qty']]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_M2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_M2_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_emergency_po_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save emergency PO format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with emergency PO qty > 0\n",
    "    df_filtered = df[df['emergency_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[[\n",
    "        'Brand', 'SKU', 'Nama', 'Toko', 'HPP', \n",
    "        'emergency_po_qty', 'emergency_po_cost'\n",
    "    ]]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_EMERGENCY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_EMERGENCY_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    supplier_df = load_supplier_data(SUPPLIER_PATH)\n",
    "    store_contrib = load_store_contribution(STORE_CONTRIBUTION_PATH)\n",
    "    all_summaries = []\n",
    "\n",
    "    # get padang df first\n",
    "    df_padang = load_padang_data('data/rawpo/xlsx/1. Miss Glam Padang.xlsx')\n",
    "\n",
    "    # test_xlsx_convert()\n",
    "\n",
    "    # Process each PO file\n",
    "    for file_path in sorted(RAWPO_DIR.glob('*.csv')):\n",
    "        try:\n",
    "            merged_df, summary = process_po_file(file_path, supplier_df, store_contrib, df_padang)\n",
    "\n",
    "            save_to_complete_format(merged_df, file_path.name)\n",
    "            save_to_m2_format(merged_df, file_path.name)\n",
    "            save_to_emergency_po_format(merged_df, file_path.name)\n",
    "\n",
    "            # summary['output_path'] = str(output_path)\n",
    "            output_path = OUTPUT_DIR / file_path.name\n",
    "            summary['output_path'] = str(output_path)\n",
    "\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  - Location: {summary['location']}\")\n",
    "            print(f\"  - Contribution: {summary['contribution_pct']}%\")\n",
    "            print(f\"  - Rows processed: {summary['total_rows']}\")\n",
    "            print(f\"  - 'Miss Glam Padang' suppliers: {summary['padang_suppliers']} rows\")\n",
    "            print(f\"  - Other suppliers: {summary['other_suppliers']} rows\")\n",
    "            print(f\"  - No supplier data: {summary['no_supplier']} rows\")\n",
    "            print(f\"  - Saved to: {output_path}\")\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Display final summary\n",
    "    if all_summaries:\n",
    "        print(\"\\nProcessing complete! Summary:\")\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Show sample of last processed file\n",
    "        print(\"\\nSample of the last processed file:\")\n",
    "        display(merged_df)\n",
    "    else:\n",
    "        print(\"\\nNo files were processed successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
