{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26400c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468e9d6",
   "metadata": {},
   "source": [
    "# Stock health Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6ba5",
   "metadata": {},
   "source": [
    "# Final batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce491334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/Users/andresuchitra/dev/missglam/autopo/notebook')\n",
    "SUPPLIER_PATH = BASE_DIR / 'data/supplier.csv'\n",
    "RAWPO_DIR = BASE_DIR / 'data/rawpo/csv'\n",
    "INPUT_DIR = BASE_DIR / 'data/input'\n",
    "RAWPO_XLSX_DIR = BASE_DIR / 'data/rawpo/xlsx'\n",
    "STORE_CONTRIBUTION_PATH = BASE_DIR / 'data/store_contribution.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'output/complete'\n",
    "OUTPUT_EXCEL_DIR = BASE_DIR / 'output/excel'\n",
    "OUTPUT_M2_DIR = BASE_DIR / 'output/m2'\n",
    "OUTPUT_EMERGENCY_DIR = BASE_DIR / 'output/emergency'\n",
    "TOP_100_SKU_DIR = BASE_DIR / 'data/top_100_sku'\n",
    "\n",
    "NUMERIC_COLUMNS = [\n",
    "    'HPP', 'Harga', 'Ranking', 'Grade', 'Terjual', 'Stok', 'Lost Days',\n",
    "    'Velocity Capped', 'Daily Sales', 'Lead Time', 'Max. Daily Sales',\n",
    "    'Max. Lead Time', 'Min. Order', 'Safety Stok', 'ROP', '3W Cover',\n",
    "    'Sedang PO', 'Suggested', 'Amount', 'Promo Factor', 'Delay Factor',\n",
    "    'Stock Cover', 'Days to Backup', 'Qty to Backup'\n",
    "]\n",
    "\n",
    "NA_VALUES = {\n",
    "    'NAN', 'NA', '#N/A', 'NULL', 'NONE', '', '?', '-', 'INF', '-INF',\n",
    "    '+INF', 'INFINITY', '-INFINITY', '1.#INF', '-1.#INF', '1.#QNAN'\n",
    "}\n",
    "\n",
    "def _patch_openpyxl_number_casting():\n",
    "    \"\"\"Ensure openpyxl won't crash when encountering NAN/INF in numeric cells.\"\"\"\n",
    "    print(\"Calling _patch_openpyxl_number_casting...\")\n",
    "\n",
    "    try:\n",
    "        from openpyxl.worksheet import _reader\n",
    "\n",
    "        original_cast = _reader._cast_number\n",
    "\n",
    "        def _safe_cast_number(value):  # pragma: no cover - monkey patch\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().upper() in NA_VALUES:\n",
    "                    return 0\n",
    "            try:\n",
    "                return original_cast(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0 if value in (None, '') else value\n",
    "\n",
    "        _reader._cast_number = _safe_cast_number\n",
    "    except Exception:\n",
    "        # If patch fails we continue; runtime reader will still attempt default behaviour\n",
    "        pass\n",
    "\n",
    "def load_special_sku_60(path):\n",
    "    print(f\"Loading Special SKU with 60 days target cover data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check file extension\n",
    "        file_ext = str(path).lower().split('.')[-1]\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Special SKU with 60 days target cover data with {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Special SKU with 60 days target cover data from {path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792be23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_top_100_sku_for_store(location, top_100_dir=TOP_100_SKU_DIR, expected_header_keys=None, max_rows=100):\n",
    "    \"\"\"Load Top 100 SKU data for a given store.\n",
    "\n",
    "    This function will:\n",
    "    - Find the matching Top 100 file for the store in ``top_100_dir``\n",
    "    - Detect which row actually contains the header (can be on row 1, 2, 3, ...)\n",
    "    - Read and return the cleaned DataFrame limited to top ``max_rows`` SKUs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        location_upper = str(location).strip().upper()\n",
    "        top_100_dir = Path(top_100_dir)\n",
    "\n",
    "        if not top_100_dir.exists():\n",
    "            print(f\"Top 100 SKU directory does not exist: {top_100_dir}\")\n",
    "            return None\n",
    "\n",
    "        # Use the same filename → store name logic to match files\n",
    "        matching_files = []\n",
    "        for path in sorted(top_100_dir.glob('*')):\n",
    "            if not path.is_file():\n",
    "                continue\n",
    "            store_name = get_store_name_from_filename(path.name)\n",
    "            if store_name == location_upper:\n",
    "                matching_files.append(path)\n",
    "\n",
    "        if not matching_files:\n",
    "            print(f\"No Top 100 SKU file found for store: {location_upper}\")\n",
    "            return None\n",
    "\n",
    "        # If multiple files match, take the first one deterministically\n",
    "        file_path = matching_files[0]\n",
    "        print(f\"Loading Top 100 SKU for {location_upper} from: {file_path}\")\n",
    "\n",
    "        suffix = file_path.suffix.lower()\n",
    "\n",
    "        # Step 1: read raw file without assuming header row\n",
    "        if suffix in ['.xlsx', '.xls']:\n",
    "            raw_df = pd.read_excel(file_path, header=None, engine='openpyxl')\n",
    "        else:\n",
    "            read_ok = False\n",
    "            raw_df = None\n",
    "            for sep in [';', ',']:\n",
    "                for enc in ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']:\n",
    "                    try:\n",
    "                        raw_df = pd.read_csv(file_path, header=None, sep=sep, encoding=enc)\n",
    "                        read_ok = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if read_ok:\n",
    "                    break\n",
    "\n",
    "            if not read_ok or raw_df is None:\n",
    "                print(f\"Failed to read Top 100 SKU file: {file_path}\")\n",
    "                return None\n",
    "\n",
    "        if raw_df is None or raw_df.empty:\n",
    "            print(f\"Top 100 SKU file is empty: {file_path}\")\n",
    "            return None\n",
    "\n",
    "                # Step 2: detect which row is the header\n",
    "        # Strategy:\n",
    "        #   1. Prefer any row containing a cell equal to \"sku\" (case-insensitive)\n",
    "        #   2. Otherwise, take the first non-empty row\n",
    "        header_row = None\n",
    "        max_header_search = min(15, len(raw_df))  # look a bit deeper if needed\n",
    "\n",
    "        for idx in range(max_header_search):\n",
    "            row_values = raw_df.iloc[idx].astype(str).str.strip()\n",
    "            lowered = [v.lower() for v in row_values]\n",
    "\n",
    "            if \"sku\" in lowered:\n",
    "                header_row = idx\n",
    "                break\n",
    "\n",
    "        if header_row is None:\n",
    "            # fallback: first non-empty row\n",
    "            for idx in range(max_header_search):\n",
    "                if raw_df.iloc[idx].notna().any():\n",
    "                    header_row = idx\n",
    "                    break\n",
    "\n",
    "        # As a final fallback, if still None but we have rows, use row 0\n",
    "        if header_row is None:\n",
    "            header_row = 0\n",
    "\n",
    "        # Step 3: build df from raw_df using that header row\n",
    "        header = raw_df.iloc[header_row].astype(str).str.strip().tolist()\n",
    "        data = raw_df.iloc[header_row + 1 :].reset_index(drop=True)\n",
    "        data.columns = header\n",
    "        df = data\n",
    "\n",
    "        # Basic cleaning\n",
    "        df.columns = df.columns.astype(str).str.strip()\n",
    "\n",
    "        # Normalize SKU column if present under any casing\n",
    "        sku_col = None\n",
    "        for c in df.columns:\n",
    "            if str(c).strip().lower() == \"sku\":\n",
    "                sku_col = c\n",
    "                break\n",
    "\n",
    "        if sku_col is not None:\n",
    "            df[sku_col] = df[sku_col].astype(str).str.strip()\n",
    "            # also expose as \"SKU\" for downstream code\n",
    "            if sku_col != \"SKU\":\n",
    "                df[\"SKU\"] = df[sku_col]\n",
    "\n",
    "        # Drop completely empty rows\n",
    "        df = df.dropna(how='all')\n",
    "\n",
    "        # Limit to top N rows\n",
    "        if max_rows is not None and max_rows > 0:\n",
    "            df = df.head(max_rows)\n",
    "\n",
    "        print(f\"Loaded Top 100 SKU for {location_upper}: {len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Top 100 SKU for {location}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05c26b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying empty-Brand-safe merge_with_suppliers override...\n"
     ]
    }
   ],
   "source": [
    "# Override merge_with_suppliers to prevent filling supplier data for rows with empty Brand\n",
    "print(\"Applying empty-Brand-safe merge_with_suppliers override...\")\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier info, skipping fallback for blank Brand rows.\"\"\"\n",
    "    print(\"Merging with suppliers (override)...\")\n",
    "\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "\n",
    "    df_clean = df_clean.copy()\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "\n",
    "    merged_df['_brand_clean'] = merged_df['Brand'].astype(str).str.strip()\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    fallback_mask = unmatched_mask & (merged_df['_brand_clean'] != '')\n",
    "\n",
    "    if fallback_mask.any():\n",
    "        print(\n",
    "            f\"Found {fallback_mask.sum()} rows without store match; attempting brand-only fallback for non-empty brands...\"\n",
    "        )\n",
    "        unmatched_rows = merged_df[fallback_mask].copy()\n",
    "        supplier_cols = [\n",
    "            col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand'\n",
    "        ]\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols, errors='ignore')\n",
    "        unmatched_rows['Brand'] = unmatched_rows['_brand_clean']\n",
    "\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "\n",
    "        matched_initial = merged_df[~fallback_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "\n",
    "    merged_df = merged_df.drop(columns=['_brand_clean'], errors='ignore')\n",
    "\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store',\n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da39de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _patch_openpyxl_number_casting...\n",
      "Loading supplier data: /Users/andresuchitra/dev/missglam/autopo/notebook/data/supplier.csv\n",
      "Loading Padang data from /Users/andresuchitra/dev/missglam/autopo/notebook/data/input/20251213/1. Miss Glam Padang.xlsx...\n",
      "Successfully loaded Padang data with 37318 rows\n",
      "\n",
      "Processing PO file: 1. Miss Glam Padang.xlsx ....\n",
      "  - Extracted location: PADANG\n",
      "\n",
      "Reading excel file: 1. Miss Glam Padang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 1. Miss Glam Padang.xlsx with 37318 rows\n",
      "Loading Top 100 SKU for PADANG from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/1. Miss Glam Padang.xlsx\n",
      "Loaded Top 100 SKU for PADANG: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26171 rows without direct store match. Attempting fallback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x10705de40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py\", line 1980, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py\", line 1997, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/1. Miss Glam Padang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/1. Miss Glam Padang.csv\n",
      "  - Location: PADANG\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 37318\n",
      "  - 'Miss Glam Padang' suppliers: 11147 rows\n",
      "  - Other suppliers: 537 rows\n",
      "  - No supplier data: 25634 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/1. Miss Glam Padang.xlsx\n",
      "\n",
      "Processing PO file: 10. Miss Glam Palembang.xlsx ....\n",
      "  - Extracted location: PALEMBANG\n",
      "\n",
      "Reading excel file: 10. Miss Glam Palembang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 10. Miss Glam Palembang.xlsx with 37060 rows\n",
      "Loading Top 100 SKU for PALEMBANG from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/10. Miss Glam Palembang.xlsx\n",
      "Loaded Top 100 SKU for PALEMBANG: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26467 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/10. Miss Glam Palembang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/10. Miss Glam Palembang.csv\n",
      "  - Location: PALEMBANG\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 37060\n",
      "  - 'Miss Glam Padang' suppliers: 209 rows\n",
      "  - Other suppliers: 11336 rows\n",
      "  - No supplier data: 25515 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/10. Miss Glam Palembang.xlsx\n",
      "\n",
      "Processing PO file: 11. Miss Glam Damar.xlsx ....\n",
      "  - Extracted location: DAMAR\n",
      "\n",
      "Reading excel file: 11. Miss Glam Damar.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 11. Miss Glam Damar.xlsx with 37467 rows\n",
      "Loading Top 100 SKU for DAMAR from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/11. Miss Glam Damar.xlsx\n",
      "Loaded Top 100 SKU for DAMAR: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26327 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/11. Miss Glam Damar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/11. Miss Glam Damar.csv\n",
      "  - Location: DAMAR\n",
      "  - Contribution: 91%\n",
      "  - Rows processed: 37467\n",
      "  - 'Miss Glam Padang' suppliers: 6 rows\n",
      "  - Other suppliers: 11668 rows\n",
      "  - No supplier data: 25793 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/11. Miss Glam Damar.xlsx\n",
      "\n",
      "Processing PO file: 12. Miss Glam Bangka.xlsx ....\n",
      "  - Extracted location: BANGKA\n",
      "\n",
      "Reading excel file: 12. Miss Glam Bangka.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 12. Miss Glam Bangka.xlsx with 37007 rows\n",
      "Loading Top 100 SKU for BANGKA from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/12. Miss Glam Bangka.xlsx\n",
      "Loaded Top 100 SKU for BANGKA: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26532 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/12. Miss Glam Bangka.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/12. Miss Glam Bangka.csv\n",
      "  - Location: BANGKA\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 37007\n",
      "  - 'Miss Glam Padang' suppliers: 274 rows\n",
      "  - Other suppliers: 11221 rows\n",
      "  - No supplier data: 25512 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/12. Miss Glam Bangka.xlsx\n",
      "\n",
      "Processing PO file: 13. Miss Glam Payakumbuh.xlsx ....\n",
      "  - Extracted location: PAYAKUMBUH\n",
      "\n",
      "Reading excel file: 13. Miss Glam Payakumbuh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 13. Miss Glam Payakumbuh.xlsx with 37100 rows\n",
      "Loading Top 100 SKU for PAYAKUMBUH from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/13. Miss Glam Payakumbuh.xlsx\n",
      "Loaded Top 100 SKU for PAYAKUMBUH: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26322 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/13. Miss Glam Payakumbuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/13. Miss Glam Payakumbuh.csv\n",
      "  - Location: PAYAKUMBUH\n",
      "  - Contribution: 47%\n",
      "  - Rows processed: 37100\n",
      "  - 'Miss Glam Padang' suppliers: 112 rows\n",
      "  - Other suppliers: 11415 rows\n",
      "  - No supplier data: 25573 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/13. Miss Glam Payakumbuh.xlsx\n",
      "\n",
      "Processing PO file: 14. Miss Glam Solok.xlsx ....\n",
      "  - Extracted location: SOLOK\n",
      "\n",
      "Reading excel file: 14. Miss Glam Solok.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 14. Miss Glam Solok.xlsx with 37085 rows\n",
      "Loading Top 100 SKU for SOLOK from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/14. Miss Glam Solok.xlsx\n",
      "Loaded Top 100 SKU for SOLOK: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26310 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/14. Miss Glam Solok.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/14. Miss Glam Solok.csv\n",
      "  - Location: SOLOK\n",
      "  - Contribution: 37%\n",
      "  - Rows processed: 37085\n",
      "  - 'Miss Glam Padang' suppliers: 110 rows\n",
      "  - Other suppliers: 11408 rows\n",
      "  - No supplier data: 25567 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/14. Miss Glam Solok.xlsx\n",
      "\n",
      "Processing PO file: 15. Miss Glam Tembilahan.xlsx ....\n",
      "  - Extracted location: TEMBILAHAN\n",
      "\n",
      "Reading excel file: 15. Miss Glam Tembilahan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 15. Miss Glam Tembilahan.xlsx with 36914 rows\n",
      "Loading Top 100 SKU for TEMBILAHAN from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/15. Miss Glam Tembilahan.xlsx\n",
      "Loaded Top 100 SKU for TEMBILAHAN: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26378 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/15. Miss Glam Tembilahan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/15. Miss Glam Tembilahan.csv\n",
      "  - Location: TEMBILAHAN\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 36914\n",
      "  - 'Miss Glam Padang' suppliers: 202 rows\n",
      "  - Other suppliers: 11200 rows\n",
      "  - No supplier data: 25512 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/15. Miss Glam Tembilahan.xlsx\n",
      "\n",
      "Processing PO file: 16. Miss Glam Lubuk Linggau.xlsx ....\n",
      "  - Extracted location: LUBUK LINGGAU\n",
      "\n",
      "Reading excel file: 16. Miss Glam Lubuk Linggau.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 16. Miss Glam Lubuk Linggau.xlsx with 36983 rows\n",
      "Loading Top 100 SKU for LUBUK LINGGAU from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/16. Miss Glam Lubuk Linggau.xlsx\n",
      "Loaded Top 100 SKU for LUBUK LINGGAU: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26773 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/16. Miss Glam Lubuk Linggau.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/16. Miss Glam Lubuk Linggau.csv\n",
      "  - Location: LUBUK LINGGAU\n",
      "  - Contribution: 26%\n",
      "  - Rows processed: 36983\n",
      "  - 'Miss Glam Padang' suppliers: 275 rows\n",
      "  - Other suppliers: 11167 rows\n",
      "  - No supplier data: 25541 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/16. Miss Glam Lubuk Linggau.xlsx\n",
      "\n",
      "Processing PO file: 17. Miss Glam Dumai.xlsx ....\n",
      "  - Extracted location: DUMAI\n",
      "\n",
      "Reading excel file: 17. Miss Glam Dumai.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 17. Miss Glam Dumai.xlsx with 37020 rows\n",
      "Loading Top 100 SKU for DUMAI from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/17. Miss Glam Dumai.xlsx\n",
      "Loaded Top 100 SKU for DUMAI: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26345 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/17. Miss Glam Dumai.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/17. Miss Glam Dumai.csv\n",
      "  - Location: DUMAI\n",
      "  - Contribution: 36%\n",
      "  - Rows processed: 37020\n",
      "  - 'Miss Glam Padang' suppliers: 170 rows\n",
      "  - Other suppliers: 11320 rows\n",
      "  - No supplier data: 25530 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/17. Miss Glam Dumai.xlsx\n",
      "\n",
      "Processing PO file: 18. Miss Glam Kedaton.xlsx ....\n",
      "  - Extracted location: KEDATON\n",
      "\n",
      "Reading excel file: 18. Miss Glam Kedaton.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 18. Miss Glam Kedaton.xlsx with 37049 rows\n",
      "Loading Top 100 SKU for KEDATON from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/18. Miss Glam Kedaton.xlsx\n",
      "Loaded Top 100 SKU for KEDATON: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26494 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/18. Miss Glam Kedaton.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/18. Miss Glam Kedaton.csv\n",
      "  - Location: KEDATON\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 37049\n",
      "  - 'Miss Glam Padang' suppliers: 200 rows\n",
      "  - Other suppliers: 11284 rows\n",
      "  - No supplier data: 25565 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/18. Miss Glam Kedaton.xlsx\n",
      "\n",
      "Processing PO file: 19. Miss Glam Rantau Prapat.xlsx ....\n",
      "  - Extracted location: RANTAU PRAPAT\n",
      "\n",
      "Reading excel file: 19. Miss Glam Rantau Prapat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 19. Miss Glam Rantau Prapat.xlsx with 36949 rows\n",
      "Loading Top 100 SKU for RANTAU PRAPAT from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/19. Miss Glam Rantau Prapat.xlsx\n",
      "Loaded Top 100 SKU for RANTAU PRAPAT: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26448 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/19. Miss Glam Rantau Prapat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/19. Miss Glam Rantau Prapat.csv\n",
      "  - Location: RANTAU PRAPAT\n",
      "  - Contribution: 27%\n",
      "  - Rows processed: 36949\n",
      "  - 'Miss Glam Padang' suppliers: 263 rows\n",
      "  - Other suppliers: 11177 rows\n",
      "  - No supplier data: 25509 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/19. Miss Glam Rantau Prapat.xlsx\n",
      "\n",
      "Processing PO file: 2. Miss Glam Pekanbaru.xlsx ....\n",
      "  - Extracted location: PEKANBARU\n",
      "\n",
      "Reading excel file: 2. Miss Glam Pekanbaru.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 2. Miss Glam Pekanbaru.xlsx with 37266 rows\n",
      "Loading Top 100 SKU for PEKANBARU from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/2. Miss Glam Pekanbaru.xlsx\n",
      "Loaded Top 100 SKU for PEKANBARU: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26379 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/2. Miss Glam Pekanbaru.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/2. Miss Glam Pekanbaru.csv\n",
      "  - Location: PEKANBARU\n",
      "  - Contribution: 60%\n",
      "  - Rows processed: 37266\n",
      "  - 'Miss Glam Padang' suppliers: 130 rows\n",
      "  - Other suppliers: 11492 rows\n",
      "  - No supplier data: 25644 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/2. Miss Glam Pekanbaru.xlsx\n",
      "\n",
      "Processing PO file: 20. Miss Glam Tanjung Pinang.xlsx ....\n",
      "  - Extracted location: TANJUNG PINANG\n",
      "\n",
      "Reading excel file: 20. Miss Glam Tanjung Pinang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 20. Miss Glam Tanjung Pinang.xlsx with 36893 rows\n",
      "Loading Top 100 SKU for TANJUNG PINANG from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/20. Miss Glam Tanjung Pinang.xlsx\n",
      "Loaded Top 100 SKU for TANJUNG PINANG: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26506 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/20. Miss Glam Tanjung Pinang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/20. Miss Glam Tanjung Pinang.csv\n",
      "  - Location: TANJUNG PINANG\n",
      "  - Contribution: 19%\n",
      "  - Rows processed: 36893\n",
      "  - 'Miss Glam Padang' suppliers: 256 rows\n",
      "  - Other suppliers: 11139 rows\n",
      "  - No supplier data: 25498 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/20. Miss Glam Tanjung Pinang.xlsx\n",
      "\n",
      "Processing PO file: 21. Miss Glam Sutomo.xlsx ....\n",
      "  - Extracted location: SUTOMO\n",
      "\n",
      "Reading excel file: 21. Miss Glam Sutomo.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 21. Miss Glam Sutomo.xlsx with 37263 rows\n",
      "Loading Top 100 SKU for SUTOMO from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/21. Miss Glam Sutomo.xlsx\n",
      "Loaded Top 100 SKU for SUTOMO: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26333 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/21. Miss Glam Sutomo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/21. Miss Glam Sutomo.csv\n",
      "  - Location: SUTOMO\n",
      "  - Contribution: 49%\n",
      "  - Rows processed: 37263\n",
      "  - 'Miss Glam Padang' suppliers: 74 rows\n",
      "  - Other suppliers: 11541 rows\n",
      "  - No supplier data: 25648 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/21. Miss Glam Sutomo.xlsx\n",
      "\n",
      "Processing PO file: 22. Miss Glam Pasamanan Barat.xlsx ....\n",
      "  - Extracted location: PASAMANAN BARAT\n",
      "Warning: No contribution percentage found for PASAMANAN BARAT\n",
      "\n",
      "Reading excel file: 22. Miss Glam Pasamanan Barat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 22. Miss Glam Pasamanan Barat.xlsx with 36976 rows\n",
      "No Top 100 SKU file found for store: PASAMANAN BARAT\n",
      "  - No valid Top 100 data for PASAMANAN BARAT, setting is_top_100_sku = 0\n",
      "  - No valid Top 100 data for PASAMANAN BARAT, setting is_top_100_sku = 0\n",
      "Merging with suppliers...\n",
      "Found 26323 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasamanan Barat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasamanan Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/22. Miss Glam Pasamanan Barat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/22. Miss Glam Pasamanan Barat.csv\n",
      "  - Location: PASAMANAN BARAT\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 36976\n",
      "  - 'Miss Glam Padang' suppliers: 168 rows\n",
      "  - Other suppliers: 11308 rows\n",
      "  - No supplier data: 25500 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/22. Miss Glam Pasamanan Barat.xlsx\n",
      "\n",
      "Processing PO file: 23. Miss Glam Halat.xlsx ....\n",
      "  - Extracted location: HALAT\n",
      "\n",
      "Reading excel file: 23. Miss Glam Halat.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 23. Miss Glam Halat.xlsx with 37065 rows\n",
      "Loading Top 100 SKU for HALAT from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/23. Miss Glam Halat.xlsx\n",
      "Loaded Top 100 SKU for HALAT: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26032 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/23. Miss Glam Halat.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/23. Miss Glam Halat.csv\n",
      "  - Location: HALAT\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 37065\n",
      "  - 'Miss Glam Padang' suppliers: 189 rows\n",
      "  - Other suppliers: 11323 rows\n",
      "  - No supplier data: 25553 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/23. Miss Glam Halat.xlsx\n",
      "\n",
      "Processing PO file: 24. Miss Glam Duri.xlsx ....\n",
      "  - Extracted location: DURI\n",
      "\n",
      "Reading excel file: 24. Miss Glam Duri.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 24. Miss Glam Duri.xlsx with 37018 rows\n",
      "Loading Top 100 SKU for DURI from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/24. Miss Glam Duri.xlsx\n",
      "Loaded Top 100 SKU for DURI: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26357 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/24. Miss Glam Duri.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/24. Miss Glam Duri.csv\n",
      "  - Location: DURI\n",
      "  - Contribution: 28%\n",
      "  - Rows processed: 37018\n",
      "  - 'Miss Glam Padang' suppliers: 187 rows\n",
      "  - Other suppliers: 11297 rows\n",
      "  - No supplier data: 25534 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/24. Miss Glam Duri.xlsx\n",
      "\n",
      "Processing PO file: 25. Miss Glam Sudirman.xlsx ....\n",
      "  - Extracted location: SUDIRMAN\n",
      "\n",
      "Reading excel file: 25. Miss Glam Sudirman.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 25. Miss Glam Sudirman.xlsx with 37360 rows\n",
      "Loading Top 100 SKU for SUDIRMAN from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/25. Miss Glam Sudirman.xlsx\n",
      "Loaded Top 100 SKU for SUDIRMAN: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26481 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/25. Miss Glam Sudirman.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/25. Miss Glam Sudirman.csv\n",
      "  - Location: SUDIRMAN\n",
      "  - Contribution: 44%\n",
      "  - Rows processed: 37360\n",
      "  - 'Miss Glam Padang' suppliers: 130 rows\n",
      "  - Other suppliers: 11448 rows\n",
      "  - No supplier data: 25782 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/25. Miss Glam Sudirman.xlsx\n",
      "\n",
      "Processing PO file: 26. Miss Glam Dr. Mansyur.xlsx ....\n",
      "  - Extracted location: DR. MANSYUR\n",
      "\n",
      "Reading excel file: 26. Miss Glam Dr. Mansyur.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 26. Miss Glam Dr. Mansyur.xlsx with 37070 rows\n",
      "Loading Top 100 SKU for DR. MANSYUR from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/26. Miss Glam Dr. Mansyur.xlsx\n",
      "Loaded Top 100 SKU for DR. MANSYUR: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26068 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/26. Miss Glam Dr. Mansyur.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/26. Miss Glam Dr. Mansyur.csv\n",
      "  - Location: DR. MANSYUR\n",
      "  - Contribution: 25%\n",
      "  - Rows processed: 37070\n",
      "  - 'Miss Glam Padang' suppliers: 209 rows\n",
      "  - Other suppliers: 11327 rows\n",
      "  - No supplier data: 25534 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/26. Miss Glam Dr. Mansyur.xlsx\n",
      "\n",
      "Processing PO file: 27. Miss Glam P. Sidimpuan.xlsx ....\n",
      "  - Extracted location: P. SIDIMPUAN\n",
      "\n",
      "Reading excel file: 27. Miss Glam P. Sidimpuan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 27. Miss Glam P. Sidimpuan.xlsx with 37000 rows\n",
      "Loading Top 100 SKU for P. SIDIMPUAN from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/27. Miss Glam P. Sidimpuan.xlsx\n",
      "Loaded Top 100 SKU for P. SIDIMPUAN: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26549 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/27. Miss Glam P. Sidimpuan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/27. Miss Glam P. Sidimpuan.csv\n",
      "  - Location: P. SIDIMPUAN\n",
      "  - Contribution: 31%\n",
      "  - Rows processed: 37000\n",
      "  - 'Miss Glam Padang' suppliers: 285 rows\n",
      "  - Other suppliers: 11172 rows\n",
      "  - No supplier data: 25543 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/27. Miss Glam P. Sidimpuan.xlsx\n",
      "\n",
      "Processing PO file: 28. Miss Glam Aceh.xlsx ....\n",
      "  - Extracted location: ACEH\n",
      "\n",
      "Reading excel file: 28. Miss Glam Aceh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 28. Miss Glam Aceh.xlsx with 36942 rows\n",
      "Loading Top 100 SKU for ACEH from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/28. Miss Glam Aceh.xlsx\n",
      "Loaded Top 100 SKU for ACEH: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26612 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/28. Miss Glam Aceh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/28. Miss Glam Aceh.csv\n",
      "  - Location: ACEH\n",
      "  - Contribution: 15%\n",
      "  - Rows processed: 36942\n",
      "  - 'Miss Glam Padang' suppliers: 295 rows\n",
      "  - Other suppliers: 11160 rows\n",
      "  - No supplier data: 25487 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/28. Miss Glam Aceh.xlsx\n",
      "\n",
      "Processing PO file: 29. Miss Glam Marpoyan.xlsx ....\n",
      "  - Extracted location: MARPOYAN\n",
      "\n",
      "Reading excel file: 29. Miss Glam Marpoyan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 29. Miss Glam Marpoyan.xlsx with 37058 rows\n",
      "Loading Top 100 SKU for MARPOYAN from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/29. Miss Glam Marpoyan.xlsx\n",
      "Loaded Top 100 SKU for MARPOYAN: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26480 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/29. Miss Glam Marpoyan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/29. Miss Glam Marpoyan.csv\n",
      "  - Location: MARPOYAN\n",
      "  - Contribution: 30%\n",
      "  - Rows processed: 37058\n",
      "  - 'Miss Glam Padang' suppliers: 231 rows\n",
      "  - Other suppliers: 11286 rows\n",
      "  - No supplier data: 25541 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/29. Miss Glam Marpoyan.xlsx\n",
      "\n",
      "Processing PO file: 3. Miss Glam Jambi.xlsx ....\n",
      "  - Extracted location: JAMBI\n",
      "\n",
      "Reading excel file: 3. Miss Glam Jambi.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 3. Miss Glam Jambi.xlsx with 37143 rows\n",
      "Loading Top 100 SKU for JAMBI from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/3. Miss Glam Jambi.xlsx\n",
      "Loaded Top 100 SKU for JAMBI: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26473 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/3. Miss Glam Jambi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/3. Miss Glam Jambi.csv\n",
      "  - Location: JAMBI\n",
      "  - Contribution: 33%\n",
      "  - Rows processed: 37143\n",
      "  - 'Miss Glam Padang' suppliers: 189 rows\n",
      "  - Other suppliers: 11374 rows\n",
      "  - No supplier data: 25580 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/3. Miss Glam Jambi.xlsx\n",
      "\n",
      "Processing PO file: 30. Miss Glam Sei Penuh.xlsx ....\n",
      "  - Extracted location: SEI PENUH\n",
      "\n",
      "Reading excel file: 30. Miss Glam Sei Penuh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 30. Miss Glam Sei Penuh.xlsx with 36989 rows\n",
      "Loading Top 100 SKU for SEI PENUH from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/30. Miss Glam Sei Penuh.xlsx\n",
      "Loaded Top 100 SKU for SEI PENUH: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26628 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/30. Miss Glam Sei Penuh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/30. Miss Glam Sei Penuh.csv\n",
      "  - Location: SEI PENUH\n",
      "  - Contribution: 21%\n",
      "  - Rows processed: 36989\n",
      "  - 'Miss Glam Padang' suppliers: 382 rows\n",
      "  - Other suppliers: 11102 rows\n",
      "  - No supplier data: 25505 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/30. Miss Glam Sei Penuh.xlsx\n",
      "\n",
      "Processing PO file: 31. Miss Glam Mayang.xlsx ....\n",
      "  - Extracted location: MAYANG\n",
      "\n",
      "Reading excel file: 31. Miss Glam Mayang.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 31. Miss Glam Mayang.xlsx with 36996 rows\n",
      "Loading Top 100 SKU for MAYANG from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/31. Miss Glam Mayang.xlsx\n",
      "Loaded Top 100 SKU for MAYANG: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26929 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/31. Miss Glam Mayang.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/31. Miss Glam Mayang.csv\n",
      "  - Location: MAYANG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 36996\n",
      "  - 'Miss Glam Padang' suppliers: 374 rows\n",
      "  - Other suppliers: 11120 rows\n",
      "  - No supplier data: 25502 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/31. Miss Glam Mayang.xlsx\n",
      "\n",
      "Processing PO file: 32. Miss Glam Soeta.xlsx ....\n",
      "  - Extracted location: SOETA\n",
      "Warning: No contribution percentage found for SOETA\n",
      "\n",
      "Reading excel file: 32. Miss Glam Soeta.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 32. Miss Glam Soeta.xlsx with 37596 rows\n",
      "Loading Top 100 SKU for SOETA from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/32. Miss Glam Soeta.xlsx\n",
      "Loaded Top 100 SKU for SOETA: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 30895 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32. Miss Glam Soeta.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32. Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/32. Miss Glam Soeta.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/32. Miss Glam Soeta.csv\n",
      "  - Location: SOETA\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 37596\n",
      "  - 'Miss Glam Padang' suppliers: 858 rows\n",
      "  - Other suppliers: 10925 rows\n",
      "  - No supplier data: 25813 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/32. Miss Glam Soeta.xlsx\n",
      "\n",
      "Processing PO file: 33. Miss Glam Balikpapan.xlsx ....\n",
      "  - Extracted location: BALIKPAPAN\n",
      "Warning: No contribution percentage found for BALIKPAPAN\n",
      "\n",
      "Reading excel file: 33. Miss Glam Balikpapan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 33. Miss Glam Balikpapan.xlsx with 37242 rows\n",
      "Loading Top 100 SKU for BALIKPAPAN from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/33. Miss Glam Balikpapan.xlsx\n",
      "Loaded Top 100 SKU for BALIKPAPAN: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 30456 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/33. Miss Glam Balikpapan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/33. Miss Glam Balikpapan.csv\n",
      "  - Location: BALIKPAPAN\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 37242\n",
      "  - 'Miss Glam Padang' suppliers: 879 rows\n",
      "  - Other suppliers: 10663 rows\n",
      "  - No supplier data: 25700 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/33. Miss Glam Balikpapan.xlsx\n",
      "\n",
      "Processing PO file: 34. Miss Glam Pematang Siantar.xlsx ....\n",
      "  - Extracted location: PEMATANG SIANTAR\n",
      "Warning: No contribution percentage found for PEMATANG SIANTAR\n",
      "\n",
      "Reading excel file: 34. Miss Glam Pematang Siantar.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 34. Miss Glam Pematang Siantar.xlsx with 38063 rows\n",
      "No Top 100 SKU file found for store: PEMATANG SIANTAR\n",
      "  - No valid Top 100 data for PEMATANG SIANTAR, setting is_top_100_sku = 0\n",
      "  - No valid Top 100 data for PEMATANG SIANTAR, setting is_top_100_sku = 0\n",
      "Merging with suppliers...\n",
      "Found 38063 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/34. Miss Glam Pematang Siantar.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/34. Miss Glam Pematang Siantar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/34. Miss Glam Pematang Siantar.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/34. Miss Glam Pematang Siantar.csv\n",
      "  - Location: PEMATANG SIANTAR\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 38063\n",
      "  - 'Miss Glam Padang' suppliers: 2586 rows\n",
      "  - Other suppliers: 9314 rows\n",
      "  - No supplier data: 26163 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/34. Miss Glam Pematang Siantar.xlsx\n",
      "\n",
      "Processing PO file: 35. Miss Glam Batoh.xlsx ....\n",
      "  - Extracted location: BATOH\n",
      "Warning: No contribution percentage found for BATOH\n",
      "\n",
      "Reading excel file: 35. Miss Glam Batoh.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n",
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 35. Miss Glam Batoh.xlsx with 38058 rows\n",
      "No Top 100 SKU file found for store: BATOH\n",
      "  - No valid Top 100 data for BATOH, setting is_top_100_sku = 0\n",
      "  - No valid Top 100 data for BATOH, setting is_top_100_sku = 0\n",
      "Merging with suppliers...\n",
      "Found 38058 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/35. Miss Glam Batoh.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/35. Miss Glam Batoh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/35. Miss Glam Batoh.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/35. Miss Glam Batoh.csv\n",
      "  - Location: BATOH\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 38058\n",
      "  - 'Miss Glam Padang' suppliers: 2586 rows\n",
      "  - Other suppliers: 9312 rows\n",
      "  - No supplier data: 26160 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/35. Miss Glam Batoh.xlsx\n",
      "\n",
      "Processing PO file: 4. Miss Glam Bukittinggi.xlsx ....\n",
      "  - Extracted location: BUKITTINGGI\n",
      "\n",
      "Reading excel file: 4. Miss Glam Bukittinggi.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 4. Miss Glam Bukittinggi.xlsx with 37099 rows\n",
      "Loading Top 100 SKU for BUKITTINGGI from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/4. Miss Glam Bukittinggi.xlsx\n",
      "Loaded Top 100 SKU for BUKITTINGGI: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26295 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/4. Miss Glam Bukittinggi.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/4. Miss Glam Bukittinggi.csv\n",
      "  - Location: BUKITTINGGI\n",
      "  - Contribution: 45%\n",
      "  - Rows processed: 37099\n",
      "  - 'Miss Glam Padang' suppliers: 108 rows\n",
      "  - Other suppliers: 11445 rows\n",
      "  - No supplier data: 25546 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/4. Miss Glam Bukittinggi.xlsx\n",
      "\n",
      "Processing PO file: 5. Miss Glam Panam.xlsx ....\n",
      "  - Extracted location: PANAM\n",
      "\n",
      "Reading excel file: 5. Miss Glam Panam.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 5. Miss Glam Panam.xlsx with 37058 rows\n",
      "Loading Top 100 SKU for PANAM from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/5. Miss Glam Panam.xlsx\n",
      "Loaded Top 100 SKU for PANAM: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26357 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/5. Miss Glam Panam.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/5. Miss Glam Panam.csv\n",
      "  - Location: PANAM\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 37058\n",
      "  - 'Miss Glam Padang' suppliers: 158 rows\n",
      "  - Other suppliers: 11331 rows\n",
      "  - No supplier data: 25569 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/5. Miss Glam Panam.xlsx\n",
      "\n",
      "Processing PO file: 6. Miss Glam Muara Bungo.xlsx ....\n",
      "  - Extracted location: MUARA BUNGO\n",
      "Warning: No contribution percentage found for MUARA BUNGO\n",
      "\n",
      "Reading excel file: 6. Miss Glam Muara Bungo.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 6. Miss Glam Muara Bungo.xlsx with 37049 rows\n",
      "No Top 100 SKU file found for store: MUARA BUNGO\n",
      "  - No valid Top 100 data for MUARA BUNGO, setting is_top_100_sku = 0\n",
      "  - No valid Top 100 data for MUARA BUNGO, setting is_top_100_sku = 0\n",
      "Merging with suppliers...\n",
      "Found 26502 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muara Bungo.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muara Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/6. Miss Glam Muara Bungo.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/6. Miss Glam Muara Bungo.csv\n",
      "  - Location: MUARA BUNGO\n",
      "  - Contribution: 100%\n",
      "  - Rows processed: 37049\n",
      "  - 'Miss Glam Padang' suppliers: 248 rows\n",
      "  - Other suppliers: 11265 rows\n",
      "  - No supplier data: 25536 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/6. Miss Glam Muara Bungo.xlsx\n",
      "\n",
      "Processing PO file: 7. Miss Glam Lampung.xlsx ....\n",
      "  - Extracted location: LAMPUNG\n",
      "\n",
      "Reading excel file: 7. Miss Glam Lampung.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 7. Miss Glam Lampung.xlsx with 36987 rows\n",
      "Loading Top 100 SKU for LAMPUNG from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/7. Miss Glam Lampung.xlsx\n",
      "Loaded Top 100 SKU for LAMPUNG: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26407 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/7. Miss Glam Lampung.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/7. Miss Glam Lampung.csv\n",
      "  - Location: LAMPUNG\n",
      "  - Contribution: 18%\n",
      "  - Rows processed: 36987\n",
      "  - 'Miss Glam Padang' suppliers: 220 rows\n",
      "  - Other suppliers: 11278 rows\n",
      "  - No supplier data: 25489 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/7. Miss Glam Lampung.xlsx\n",
      "\n",
      "Processing PO file: 8. Miss Glam Bengkulu.xlsx ....\n",
      "  - Extracted location: BENGKULU\n",
      "\n",
      "Reading excel file: 8. Miss Glam Bengkulu.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 8. Miss Glam Bengkulu.xlsx with 36935 rows\n",
      "Loading Top 100 SKU for BENGKULU from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/8. Miss Glam Bengkulu.xlsx\n",
      "Loaded Top 100 SKU for BENGKULU: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26472 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/8. Miss Glam Bengkulu.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/8. Miss Glam Bengkulu.csv\n",
      "  - Location: BENGKULU\n",
      "  - Contribution: 14%\n",
      "  - Rows processed: 36935\n",
      "  - 'Miss Glam Padang' suppliers: 264 rows\n",
      "  - Other suppliers: 11188 rows\n",
      "  - No supplier data: 25483 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/8. Miss Glam Bengkulu.xlsx\n",
      "\n",
      "Processing PO file: 9. Miss Glam Medan.xlsx ....\n",
      "  - Extracted location: MEDAN\n",
      "\n",
      "Reading excel file: 9. Miss Glam Medan.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/7219xcjd2dj829bf02_x9zy80000gn/T/ipykernel_3470/680799811.py:695: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace('', np.nan).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 9. Miss Glam Medan.xlsx with 37161 rows\n",
      "Loading Top 100 SKU for MEDAN from: /Users/andresuchitra/dev/missglam/autopo/notebook/data/top_100_sku/9. Miss Glam Medan.xlsx\n",
      "Loaded Top 100 SKU for MEDAN: 100 rows\n",
      "Merging with suppliers...\n",
      "Found 26431 rows without direct store match. Attempting fallback...\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.xlsx\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/m2/9. Miss Glam Medan.csv\n",
      "File saved to /Users/andresuchitra/dev/missglam/autopo/notebook/output/emergency/9. Miss Glam Medan.csv\n",
      "  - Location: MEDAN\n",
      "  - Contribution: 46%\n",
      "  - Rows processed: 37161\n",
      "  - 'Miss Glam Padang' suppliers: 189 rows\n",
      "  - Other suppliers: 11392 rows\n",
      "  - No supplier data: 25580 rows\n",
      "  - Saved to: /Users/andresuchitra/dev/missglam/autopo/notebook/output/complete/9. Miss Glam Medan.xlsx\n",
      "\n",
      "Processing complete! Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>location</th>\n",
       "      <th>contribution_pct</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>padang_suppliers</th>\n",
       "      <th>other_suppliers</th>\n",
       "      <th>no_supplier</th>\n",
       "      <th>status</th>\n",
       "      <th>output_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Miss Glam Padang.xlsx</td>\n",
       "      <td>PADANG</td>\n",
       "      <td>100</td>\n",
       "      <td>37318</td>\n",
       "      <td>11147</td>\n",
       "      <td>537</td>\n",
       "      <td>25634</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10. Miss Glam Palembang.xlsx</td>\n",
       "      <td>PALEMBANG</td>\n",
       "      <td>26</td>\n",
       "      <td>37060</td>\n",
       "      <td>209</td>\n",
       "      <td>11336</td>\n",
       "      <td>25515</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11. Miss Glam Damar.xlsx</td>\n",
       "      <td>DAMAR</td>\n",
       "      <td>91</td>\n",
       "      <td>37467</td>\n",
       "      <td>6</td>\n",
       "      <td>11668</td>\n",
       "      <td>25793</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12. Miss Glam Bangka.xlsx</td>\n",
       "      <td>BANGKA</td>\n",
       "      <td>28</td>\n",
       "      <td>37007</td>\n",
       "      <td>274</td>\n",
       "      <td>11221</td>\n",
       "      <td>25512</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13. Miss Glam Payakumbuh.xlsx</td>\n",
       "      <td>PAYAKUMBUH</td>\n",
       "      <td>47</td>\n",
       "      <td>37100</td>\n",
       "      <td>112</td>\n",
       "      <td>11415</td>\n",
       "      <td>25573</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14. Miss Glam Solok.xlsx</td>\n",
       "      <td>SOLOK</td>\n",
       "      <td>37</td>\n",
       "      <td>37085</td>\n",
       "      <td>110</td>\n",
       "      <td>11408</td>\n",
       "      <td>25567</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15. Miss Glam Tembilahan.xlsx</td>\n",
       "      <td>TEMBILAHAN</td>\n",
       "      <td>27</td>\n",
       "      <td>36914</td>\n",
       "      <td>202</td>\n",
       "      <td>11200</td>\n",
       "      <td>25512</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16. Miss Glam Lubuk Linggau.xlsx</td>\n",
       "      <td>LUBUK LINGGAU</td>\n",
       "      <td>26</td>\n",
       "      <td>36983</td>\n",
       "      <td>275</td>\n",
       "      <td>11167</td>\n",
       "      <td>25541</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17. Miss Glam Dumai.xlsx</td>\n",
       "      <td>DUMAI</td>\n",
       "      <td>36</td>\n",
       "      <td>37020</td>\n",
       "      <td>170</td>\n",
       "      <td>11320</td>\n",
       "      <td>25530</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18. Miss Glam Kedaton.xlsx</td>\n",
       "      <td>KEDATON</td>\n",
       "      <td>18</td>\n",
       "      <td>37049</td>\n",
       "      <td>200</td>\n",
       "      <td>11284</td>\n",
       "      <td>25565</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19. Miss Glam Rantau Prapat.xlsx</td>\n",
       "      <td>RANTAU PRAPAT</td>\n",
       "      <td>27</td>\n",
       "      <td>36949</td>\n",
       "      <td>263</td>\n",
       "      <td>11177</td>\n",
       "      <td>25509</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2. Miss Glam Pekanbaru.xlsx</td>\n",
       "      <td>PEKANBARU</td>\n",
       "      <td>60</td>\n",
       "      <td>37266</td>\n",
       "      <td>130</td>\n",
       "      <td>11492</td>\n",
       "      <td>25644</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20. Miss Glam Tanjung Pinang.xlsx</td>\n",
       "      <td>TANJUNG PINANG</td>\n",
       "      <td>19</td>\n",
       "      <td>36893</td>\n",
       "      <td>256</td>\n",
       "      <td>11139</td>\n",
       "      <td>25498</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21. Miss Glam Sutomo.xlsx</td>\n",
       "      <td>SUTOMO</td>\n",
       "      <td>49</td>\n",
       "      <td>37263</td>\n",
       "      <td>74</td>\n",
       "      <td>11541</td>\n",
       "      <td>25648</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22. Miss Glam Pasamanan Barat.xlsx</td>\n",
       "      <td>PASAMANAN BARAT</td>\n",
       "      <td>100</td>\n",
       "      <td>36976</td>\n",
       "      <td>168</td>\n",
       "      <td>11308</td>\n",
       "      <td>25500</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23. Miss Glam Halat.xlsx</td>\n",
       "      <td>HALAT</td>\n",
       "      <td>31</td>\n",
       "      <td>37065</td>\n",
       "      <td>189</td>\n",
       "      <td>11323</td>\n",
       "      <td>25553</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24. Miss Glam Duri.xlsx</td>\n",
       "      <td>DURI</td>\n",
       "      <td>28</td>\n",
       "      <td>37018</td>\n",
       "      <td>187</td>\n",
       "      <td>11297</td>\n",
       "      <td>25534</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25. Miss Glam Sudirman.xlsx</td>\n",
       "      <td>SUDIRMAN</td>\n",
       "      <td>44</td>\n",
       "      <td>37360</td>\n",
       "      <td>130</td>\n",
       "      <td>11448</td>\n",
       "      <td>25782</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26. Miss Glam Dr. Mansyur.xlsx</td>\n",
       "      <td>DR. MANSYUR</td>\n",
       "      <td>25</td>\n",
       "      <td>37070</td>\n",
       "      <td>209</td>\n",
       "      <td>11327</td>\n",
       "      <td>25534</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27. Miss Glam P. Sidimpuan.xlsx</td>\n",
       "      <td>P. SIDIMPUAN</td>\n",
       "      <td>31</td>\n",
       "      <td>37000</td>\n",
       "      <td>285</td>\n",
       "      <td>11172</td>\n",
       "      <td>25543</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28. Miss Glam Aceh.xlsx</td>\n",
       "      <td>ACEH</td>\n",
       "      <td>15</td>\n",
       "      <td>36942</td>\n",
       "      <td>295</td>\n",
       "      <td>11160</td>\n",
       "      <td>25487</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29. Miss Glam Marpoyan.xlsx</td>\n",
       "      <td>MARPOYAN</td>\n",
       "      <td>30</td>\n",
       "      <td>37058</td>\n",
       "      <td>231</td>\n",
       "      <td>11286</td>\n",
       "      <td>25541</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3. Miss Glam Jambi.xlsx</td>\n",
       "      <td>JAMBI</td>\n",
       "      <td>33</td>\n",
       "      <td>37143</td>\n",
       "      <td>189</td>\n",
       "      <td>11374</td>\n",
       "      <td>25580</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30. Miss Glam Sei Penuh.xlsx</td>\n",
       "      <td>SEI PENUH</td>\n",
       "      <td>21</td>\n",
       "      <td>36989</td>\n",
       "      <td>382</td>\n",
       "      <td>11102</td>\n",
       "      <td>25505</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31. Miss Glam Mayang.xlsx</td>\n",
       "      <td>MAYANG</td>\n",
       "      <td>18</td>\n",
       "      <td>36996</td>\n",
       "      <td>374</td>\n",
       "      <td>11120</td>\n",
       "      <td>25502</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32. Miss Glam Soeta.xlsx</td>\n",
       "      <td>SOETA</td>\n",
       "      <td>100</td>\n",
       "      <td>37596</td>\n",
       "      <td>858</td>\n",
       "      <td>10925</td>\n",
       "      <td>25813</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33. Miss Glam Balikpapan.xlsx</td>\n",
       "      <td>BALIKPAPAN</td>\n",
       "      <td>100</td>\n",
       "      <td>37242</td>\n",
       "      <td>879</td>\n",
       "      <td>10663</td>\n",
       "      <td>25700</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34. Miss Glam Pematang Siantar.xlsx</td>\n",
       "      <td>PEMATANG SIANTAR</td>\n",
       "      <td>100</td>\n",
       "      <td>38063</td>\n",
       "      <td>2586</td>\n",
       "      <td>9314</td>\n",
       "      <td>26163</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35. Miss Glam Batoh.xlsx</td>\n",
       "      <td>BATOH</td>\n",
       "      <td>100</td>\n",
       "      <td>38058</td>\n",
       "      <td>2586</td>\n",
       "      <td>9312</td>\n",
       "      <td>26160</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4. Miss Glam Bukittinggi.xlsx</td>\n",
       "      <td>BUKITTINGGI</td>\n",
       "      <td>45</td>\n",
       "      <td>37099</td>\n",
       "      <td>108</td>\n",
       "      <td>11445</td>\n",
       "      <td>25546</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5. Miss Glam Panam.xlsx</td>\n",
       "      <td>PANAM</td>\n",
       "      <td>46</td>\n",
       "      <td>37058</td>\n",
       "      <td>158</td>\n",
       "      <td>11331</td>\n",
       "      <td>25569</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6. Miss Glam Muara Bungo.xlsx</td>\n",
       "      <td>MUARA BUNGO</td>\n",
       "      <td>100</td>\n",
       "      <td>37049</td>\n",
       "      <td>248</td>\n",
       "      <td>11265</td>\n",
       "      <td>25536</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7. Miss Glam Lampung.xlsx</td>\n",
       "      <td>LAMPUNG</td>\n",
       "      <td>18</td>\n",
       "      <td>36987</td>\n",
       "      <td>220</td>\n",
       "      <td>11278</td>\n",
       "      <td>25489</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8. Miss Glam Bengkulu.xlsx</td>\n",
       "      <td>BENGKULU</td>\n",
       "      <td>14</td>\n",
       "      <td>36935</td>\n",
       "      <td>264</td>\n",
       "      <td>11188</td>\n",
       "      <td>25483</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9. Miss Glam Medan.xlsx</td>\n",
       "      <td>MEDAN</td>\n",
       "      <td>46</td>\n",
       "      <td>37161</td>\n",
       "      <td>189</td>\n",
       "      <td>11392</td>\n",
       "      <td>25580</td>\n",
       "      <td>Success</td>\n",
       "      <td>/Users/andresuchitra/dev/missglam/autopo/noteb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file          location contribution_pct  \\\n",
       "0              1. Miss Glam Padang.xlsx            PADANG              100   \n",
       "1          10. Miss Glam Palembang.xlsx         PALEMBANG               26   \n",
       "2              11. Miss Glam Damar.xlsx             DAMAR               91   \n",
       "3             12. Miss Glam Bangka.xlsx            BANGKA               28   \n",
       "4         13. Miss Glam Payakumbuh.xlsx        PAYAKUMBUH               47   \n",
       "5              14. Miss Glam Solok.xlsx             SOLOK               37   \n",
       "6         15. Miss Glam Tembilahan.xlsx        TEMBILAHAN               27   \n",
       "7      16. Miss Glam Lubuk Linggau.xlsx     LUBUK LINGGAU               26   \n",
       "8              17. Miss Glam Dumai.xlsx             DUMAI               36   \n",
       "9            18. Miss Glam Kedaton.xlsx           KEDATON               18   \n",
       "10     19. Miss Glam Rantau Prapat.xlsx     RANTAU PRAPAT               27   \n",
       "11          2. Miss Glam Pekanbaru.xlsx         PEKANBARU               60   \n",
       "12    20. Miss Glam Tanjung Pinang.xlsx    TANJUNG PINANG               19   \n",
       "13            21. Miss Glam Sutomo.xlsx            SUTOMO               49   \n",
       "14   22. Miss Glam Pasamanan Barat.xlsx   PASAMANAN BARAT              100   \n",
       "15             23. Miss Glam Halat.xlsx             HALAT               31   \n",
       "16              24. Miss Glam Duri.xlsx              DURI               28   \n",
       "17          25. Miss Glam Sudirman.xlsx          SUDIRMAN               44   \n",
       "18       26. Miss Glam Dr. Mansyur.xlsx       DR. MANSYUR               25   \n",
       "19      27. Miss Glam P. Sidimpuan.xlsx      P. SIDIMPUAN               31   \n",
       "20              28. Miss Glam Aceh.xlsx              ACEH               15   \n",
       "21          29. Miss Glam Marpoyan.xlsx          MARPOYAN               30   \n",
       "22              3. Miss Glam Jambi.xlsx             JAMBI               33   \n",
       "23         30. Miss Glam Sei Penuh.xlsx         SEI PENUH               21   \n",
       "24            31. Miss Glam Mayang.xlsx            MAYANG               18   \n",
       "25             32. Miss Glam Soeta.xlsx             SOETA              100   \n",
       "26        33. Miss Glam Balikpapan.xlsx        BALIKPAPAN              100   \n",
       "27  34. Miss Glam Pematang Siantar.xlsx  PEMATANG SIANTAR              100   \n",
       "28             35. Miss Glam Batoh.xlsx             BATOH              100   \n",
       "29        4. Miss Glam Bukittinggi.xlsx       BUKITTINGGI               45   \n",
       "30              5. Miss Glam Panam.xlsx             PANAM               46   \n",
       "31        6. Miss Glam Muara Bungo.xlsx       MUARA BUNGO              100   \n",
       "32            7. Miss Glam Lampung.xlsx           LAMPUNG               18   \n",
       "33           8. Miss Glam Bengkulu.xlsx          BENGKULU               14   \n",
       "34              9. Miss Glam Medan.xlsx             MEDAN               46   \n",
       "\n",
       "    total_rows  padang_suppliers  other_suppliers  no_supplier   status  \\\n",
       "0        37318             11147              537        25634  Success   \n",
       "1        37060               209            11336        25515  Success   \n",
       "2        37467                 6            11668        25793  Success   \n",
       "3        37007               274            11221        25512  Success   \n",
       "4        37100               112            11415        25573  Success   \n",
       "5        37085               110            11408        25567  Success   \n",
       "6        36914               202            11200        25512  Success   \n",
       "7        36983               275            11167        25541  Success   \n",
       "8        37020               170            11320        25530  Success   \n",
       "9        37049               200            11284        25565  Success   \n",
       "10       36949               263            11177        25509  Success   \n",
       "11       37266               130            11492        25644  Success   \n",
       "12       36893               256            11139        25498  Success   \n",
       "13       37263                74            11541        25648  Success   \n",
       "14       36976               168            11308        25500  Success   \n",
       "15       37065               189            11323        25553  Success   \n",
       "16       37018               187            11297        25534  Success   \n",
       "17       37360               130            11448        25782  Success   \n",
       "18       37070               209            11327        25534  Success   \n",
       "19       37000               285            11172        25543  Success   \n",
       "20       36942               295            11160        25487  Success   \n",
       "21       37058               231            11286        25541  Success   \n",
       "22       37143               189            11374        25580  Success   \n",
       "23       36989               382            11102        25505  Success   \n",
       "24       36996               374            11120        25502  Success   \n",
       "25       37596               858            10925        25813  Success   \n",
       "26       37242               879            10663        25700  Success   \n",
       "27       38063              2586             9314        26163  Success   \n",
       "28       38058              2586             9312        26160  Success   \n",
       "29       37099               108            11445        25546  Success   \n",
       "30       37058               158            11331        25569  Success   \n",
       "31       37049               248            11265        25536  Success   \n",
       "32       36987               220            11278        25489  Success   \n",
       "33       36935               264            11188        25483  Success   \n",
       "34       37161               189            11392        25580  Success   \n",
       "\n",
       "                                          output_path  \n",
       "0   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "1   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "2   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "3   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "4   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "5   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "6   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "7   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "8   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "9   /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "10  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "11  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "12  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "13  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "14  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "15  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "16  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "17  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "18  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "19  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "20  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "21  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "22  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "23  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "24  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "25  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "26  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "27  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "28  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "29  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "30  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "31  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "32  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "33  /Users/andresuchitra/dev/missglam/autopo/noteb...  \n",
       "34  /Users/andresuchitra/dev/missglam/autopo/noteb...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the last processed file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Kategori Brand</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Nama</th>\n",
       "      <th>Toko</th>\n",
       "      <th>Stok</th>\n",
       "      <th>Daily Sales</th>\n",
       "      <th>Max. Daily Sales</th>\n",
       "      <th>Lead Time</th>\n",
       "      <th>Max. Lead Time</th>\n",
       "      <th>...</th>\n",
       "      <th>Nama Supplier</th>\n",
       "      <th>ID Brand</th>\n",
       "      <th>Nama Brand</th>\n",
       "      <th>ID Store</th>\n",
       "      <th>Nama Store</th>\n",
       "      <th>Hari Order</th>\n",
       "      <th>Min. Purchase</th>\n",
       "      <th>Trading Term</th>\n",
       "      <th>Promo Factor</th>\n",
       "      <th>Delay Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>BRAND VIRAL</td>\n",
       "      <td>10400614911</td>\n",
       "      <td>ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>BRAND VIRAL</td>\n",
       "      <td>10100824612</td>\n",
       "      <td>ACNAWAY Mugwort Acne Clear Bar Soap 100gr</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>BRAND VIRAL</td>\n",
       "      <td>10400517459</td>\n",
       "      <td>ACNAWAY Mugwort Daily Sunscreen Only For Acne ...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>BRAND VIRAL</td>\n",
       "      <td>101001107647</td>\n",
       "      <td>ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>BRAND VIRAL</td>\n",
       "      <td>10500637717</td>\n",
       "      <td>ACNAWAY Mugwort Gel Mask Anti Pores Masker Gel...</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>...</td>\n",
       "      <td>PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>ACNAWAY</td>\n",
       "      <td>19.00</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>2.00</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37156</th>\n",
       "      <td>ZYWELL</td>\n",
       "      <td>DELISTING</td>\n",
       "      <td>10500322936</td>\n",
       "      <td>ZYWELL Pell Off Mask Cellendula 10g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37157</th>\n",
       "      <td>ZYWELL</td>\n",
       "      <td>DELISTING</td>\n",
       "      <td>18200200793</td>\n",
       "      <td>ZYWELL Pell Off Mask CHamomille 10g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37158</th>\n",
       "      <td>ZYWELL</td>\n",
       "      <td>DELISTING</td>\n",
       "      <td>10500300101</td>\n",
       "      <td>ZYWELL Pell Off Mask Jasmine 10g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37159</th>\n",
       "      <td>ZYWELL</td>\n",
       "      <td>DELISTING</td>\n",
       "      <td>10500322858</td>\n",
       "      <td>ZYWELL Pell Off Mask Rose 10g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37160</th>\n",
       "      <td>ZYWELL</td>\n",
       "      <td>DELISTING</td>\n",
       "      <td>10500322711</td>\n",
       "      <td>ZYWELL Pell Off Mask Saffron10g</td>\n",
       "      <td>Miss Glam Medan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37161 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand Kategori Brand           SKU  \\\n",
       "0      ACNAWAY    BRAND VIRAL   10400614911   \n",
       "1      ACNAWAY    BRAND VIRAL   10100824612   \n",
       "2      ACNAWAY    BRAND VIRAL   10400517459   \n",
       "3      ACNAWAY    BRAND VIRAL  101001107647   \n",
       "4      ACNAWAY    BRAND VIRAL   10500637717   \n",
       "...        ...            ...           ...   \n",
       "37156   ZYWELL      DELISTING   10500322936   \n",
       "37157   ZYWELL      DELISTING   18200200793   \n",
       "37158   ZYWELL      DELISTING   10500300101   \n",
       "37159   ZYWELL      DELISTING   10500322858   \n",
       "37160   ZYWELL      DELISTING   10500322711   \n",
       "\n",
       "                                                    Nama             Toko  \\\n",
       "0      ACNAWAY 3 in 1 Acne Sun Serum Sunscreen Serum ...  Miss Glam Medan   \n",
       "1              ACNAWAY Mugwort Acne Clear Bar Soap 100gr  Miss Glam Medan   \n",
       "2      ACNAWAY Mugwort Daily Sunscreen Only For Acne ...  Miss Glam Medan   \n",
       "3      ACNAWAY Mugwort Gel Facial Wash Mugwort + Cent...  Miss Glam Medan   \n",
       "4      ACNAWAY Mugwort Gel Mask Anti Pores Masker Gel...  Miss Glam Medan   \n",
       "...                                                  ...              ...   \n",
       "37156                ZYWELL Pell Off Mask Cellendula 10g  Miss Glam Medan   \n",
       "37157                ZYWELL Pell Off Mask CHamomille 10g  Miss Glam Medan   \n",
       "37158                   ZYWELL Pell Off Mask Jasmine 10g  Miss Glam Medan   \n",
       "37159                      ZYWELL Pell Off Mask Rose 10g  Miss Glam Medan   \n",
       "37160                    ZYWELL Pell Off Mask Saffron10g  Miss Glam Medan   \n",
       "\n",
       "       Stok  Daily Sales  Max. Daily Sales  Lead Time  Max. Lead Time  ...  \\\n",
       "0      6.00         0.03              1.00       5.00           28.00  ...   \n",
       "1     22.00         0.03              0.00       5.00           28.00  ...   \n",
       "2      8.00         0.34              3.00       5.00           28.00  ...   \n",
       "3      4.00         0.34              2.00       5.00           28.00  ...   \n",
       "4      0.00         0.22              0.00       5.00           28.00  ...   \n",
       "...     ...          ...               ...        ...             ...  ...   \n",
       "37156  0.00         0.00              0.00       0.00            0.00  ...   \n",
       "37157  0.00         0.00              0.00       0.00            0.00  ...   \n",
       "37158  0.00         0.00              0.00       0.00            0.00  ...   \n",
       "37159  0.00         0.00              0.00       0.00            0.00  ...   \n",
       "37160  0.00         0.00              0.00       0.00            0.00  ...   \n",
       "\n",
       "                                        Nama Supplier  ID Brand  Nama Brand  \\\n",
       "0      PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   1480.00     ACNAWAY   \n",
       "1      PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   1480.00     ACNAWAY   \n",
       "2      PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   1480.00     ACNAWAY   \n",
       "3      PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   1480.00     ACNAWAY   \n",
       "4      PT. BERSAMA DISTRIVERSA INDONESIA (DC CIPUTAT)   1480.00     ACNAWAY   \n",
       "...                                               ...       ...         ...   \n",
       "37156                                                      0.00         NaN   \n",
       "37157                                                      0.00         NaN   \n",
       "37158                                                      0.00         NaN   \n",
       "37159                                                      0.00         NaN   \n",
       "37160                                                      0.00         NaN   \n",
       "\n",
       "       ID Store       Nama Store  Hari Order  Min. Purchase  Trading Term  \\\n",
       "0         19.00  Miss Glam Medan        2.00      500000.00          0.00   \n",
       "1         19.00  Miss Glam Medan        2.00      500000.00          0.00   \n",
       "2         19.00  Miss Glam Medan        2.00      500000.00          0.00   \n",
       "3         19.00  Miss Glam Medan        2.00      500000.00          0.00   \n",
       "4         19.00  Miss Glam Medan        2.00      500000.00          0.00   \n",
       "...         ...              ...         ...            ...           ...   \n",
       "37156      0.00                         0.00           0.00          0.00   \n",
       "37157      0.00                         0.00           0.00          0.00   \n",
       "37158      0.00                         0.00           0.00          0.00   \n",
       "37159      0.00                         0.00           0.00          0.00   \n",
       "37160      0.00                         0.00           0.00          0.00   \n",
       "\n",
       "       Promo Factor  Delay Factor  \n",
       "0                                  \n",
       "1                                  \n",
       "2                                  \n",
       "3                                  \n",
       "4                                  \n",
       "...             ...           ...  \n",
       "37156                              \n",
       "37157                              \n",
       "37158                              \n",
       "37159                              \n",
       "37160                              \n",
       "\n",
       "[37161 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import display\n",
    "from locale import atof\n",
    "import numpy as np\n",
    "from openpyxl.styles import numbers\n",
    "from datetime import datetime\n",
    "\n",
    "_patch_openpyxl_number_casting()\n",
    "\n",
    "# Apply the formatting to numeric columns in your final output\n",
    "def format_dataframe_display(df):\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # Apply formatting to numeric columns\n",
    "    for col in df_display.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: format_id_number(x, 2) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EXCEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_M2_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EMERGENCY_DIR, exist_ok=True)\n",
    "\n",
    "# df_special_60 = load_special_sku_60(BASE_DIR / 'data/special_sku_60.csv')\n",
    "\n",
    "# display(df_special_60)\n",
    "\n",
    "def load_store_contribution(store_contribution_path):\n",
    "    \"\"\"Load and prepare store contribution data.\"\"\"\n",
    "    store_contrib = pd.read_csv(store_contribution_path, header=None, \n",
    "                              names=['store', 'contribution_pct'])\n",
    "    # Convert store names to lowercase for case-insensitive matching\n",
    "    store_contrib['store_lower'] = store_contrib['store'].str.lower()\n",
    "    return store_contrib\n",
    "\n",
    "def get_contribution_pct(location, store_contrib):\n",
    "    \"\"\"Get contribution percentage for a given location.\"\"\"\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    contrib_row = store_contrib[store_contrib['store_lower'] == location_lower]\n",
    "    if not contrib_row.empty:\n",
    "        return contrib_row['contribution_pct'].values[0]\n",
    "    print(f\"Warning: No contribution percentage found for {location}\")\n",
    "\n",
    "    return 100  # Default to 100% if not found\n",
    "\n",
    "def load_supplier_data(supplier_path):\n",
    "    \"\"\"Load and clean supplier data.\"\"\"\n",
    "    print(f\"Loading supplier data: {supplier_path}\")\n",
    "    df = pd.read_csv(supplier_path, sep=';', decimal=',').fillna('')\n",
    "    df['Nama Brand'] = df['Nama Brand'].str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_with_suppliers_v1(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier information.\"\"\"\n",
    "    print(\"Merging with suppliers...\")\n",
    "    \n",
    "    # Clean supplier data\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    \n",
    "    # Deduplicate to prevent row explosion - Unique Brand+Store\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "    \n",
    "    # Ensure PO data has clean columns for merging\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "    \n",
    "    # 1. Primary Merge: Match on Brand AND Store (Toko)\n",
    "    # This prioritizes the specific supplier for that store\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "    \n",
    "    # 2. Fallback: For unmatched rows, try to find ANY supplier for that Brand\n",
    "    # Identify rows where merge failed (Nama Brand is NaN)\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    \n",
    "    if unmatched_mask.any():\n",
    "        print(f\"Found {unmatched_mask.sum()} rows without direct store match. Attempting fallback...\")\n",
    "        \n",
    "        # Get the unmatched rows and drop the empty supplier columns\n",
    "        unmatched_rows = merged_df[unmatched_mask].copy()\n",
    "        supplier_cols = [col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand']\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols)\n",
    "        \n",
    "        # Create fallback supplier list (one per brand)\n",
    "        # We take the first one found for each brand\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        \n",
    "        # Merge unmatched rows with fallback suppliers\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "        \n",
    "        # Combine the initially matched rows with the fallback-matched rows\n",
    "        matched_initial = merged_df[~unmatched_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "    \n",
    "    # Clean up supplier columns\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def calculate_inventory_metrics(df_clean):\n",
    "    \"\"\"\n",
    "    Calculate various inventory metrics including safety stock, reorder points, and PO quantities.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Input dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added calculated columns\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "    df = df_clean.copy()\n",
    "    \n",
    "    # Set display options\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "    # Normalise stock column name\n",
    "    stock_col = 'Stok' if 'Stok' in df.columns else 'Stock'\n",
    "    # Ensure is_top_100_sku exists (0 for non-top-100 by default)\n",
    "    if 'is_top_100_sku' not in df.columns:\n",
    "        df['is_top_100_sku'] = 0\n",
    "    df['is_top_100_sku'] = pd.to_numeric(df['is_top_100_sku'], errors='coerce').fillna(0).astype(int)\n",
    "    # Force the columns we need into numeric form\n",
    "    numeric_cols = [\n",
    "        stock_col, 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "        'Max. Lead Time', 'Sedang PO', 'HPP', 'Harga', 'sales_contribution'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    try:\n",
    "        # 1. Safety stock calculation\n",
    "        df['Safety stock'] = (df['Max. Daily Sales'] * df['Max. Lead Time']) - (df['Daily Sales'] * df['Lead Time'])\n",
    "        df['Safety stock'] = df['Safety stock'].apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 2. Reorder point calculation\n",
    "        df['Reorder point'] = np.ceil((df['Daily Sales'] * df['Lead Time']) + df['Safety stock']).fillna(0).astype(int)\n",
    "        \n",
    "        # 3. Stock cover for 30 or 60 days based on special SKUs\n",
    "        # Default to 30 days for all SKUs\n",
    "        df['target_days_cover'] = 30\n",
    "        \n",
    "        # 4. [DISABLED TO TOP 100 logic]Check if we have special SKUs and update their target days to 60\n",
    "        # if df_special_60 is not None and not df_special_60.empty:\n",
    "        #     # Find the SKU column in thxwe main dataframe (case-insensitive)\n",
    "        #     sku_col = next((col for col in df.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "        #     # Find the SKU column in the special SKU dataframe (case-insensitive)\n",
    "        #     special_sku_col = next((col for col in df_special_60.columns if col.lower() == 'sku'), None)\n",
    "            \n",
    "        #     if sku_col and special_sku_col:\n",
    "        #         # Convert both to string and strip whitespace for matching\n",
    "        #         df[sku_col] = df[sku_col].astype(str).str.strip()\n",
    "        #         df_special_60[special_sku_col] = df_special_60[special_sku_col].astype(str).str.strip()\n",
    "                \n",
    "        #         # Update target_days_unit to 60 for special SKUs\n",
    "        #         special_skus = set(df_special_60[special_sku_col].unique())\n",
    "        #         df.loc[df[sku_col].isin(special_skus), 'target_days_cover'] = 60\n",
    "        #     else:\n",
    "        #         print(\"Warning: Could not find 'SKU' column in one of the dataframes\")\n",
    "        \n",
    "        # Calculate target days cover based on the determined days\n",
    "        df['qty_for_target_days_cover'] = (\n",
    "            df['Daily Sales'] * df['target_days_cover']\n",
    "        ).apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        df['current_days_stock_cover'] = np.where(\n",
    "            df['Daily Sales'] > 0,\n",
    "            df[stock_col] / df['Daily Sales'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # 5. Is open PO flag\n",
    "        df['is_open_po'] = np.where(\n",
    "            (df['current_days_stock_cover'] < df['target_days_cover']) & \n",
    "            (df['Stok'] <= df['Reorder point']), 1, 0\n",
    "        )\n",
    "        \n",
    "        # 6. Initial PO quantity\n",
    "        df['initial_qty_po'] = df['qty_for_target_days_cover'] - df[stock_col] - df.get('Sedang PO', 0)\n",
    "        df['initial_qty_po'] = (\n",
    "            pd.Series(\n",
    "                np.where(df['is_open_po'] == 1, df['initial_qty_po'], 0),\n",
    "                index=df.index\n",
    "            )\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 7. Emergency PO quantity\n",
    "        #    For non top-100 SKUs, this must always be 0\n",
    "        base_emergency = np.where(\n",
    "            df.get('Sedang PO', 0) > 0,\n",
    "            np.maximum(0, (df['Max. Lead Time'] - df['current_days_stock_cover']) * df['Daily Sales']),\n",
    "            np.ceil((df['Max. Lead Time'] - df['current_days_stock_cover']) * df['Daily Sales'])\n",
    "        )\n",
    "        df['emergency_po_qty'] = base_emergency * df['is_top_100_sku']\n",
    "        # Clean up emergency PO quantities\n",
    "        df['emergency_po_qty'] = (\n",
    "            df['emergency_po_qty']\n",
    "            .replace([np.inf, -np.inf], 0)\n",
    "            .fillna(0)\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 8. Updated regular PO quantity:\n",
    "        # it will be same as initial suggested PO both for SKU Top 100 and others\n",
    "        df['updated_regular_po_qty'] = df['initial_qty_po'].clip(lower=0).astype(int)\n",
    "        \n",
    "        # 9. Final updated regular PO quantity (enforce minimum order)\n",
    "        df['final_updated_regular_po_qty'] = np.where(\n",
    "            (df['updated_regular_po_qty'] > 0) & \n",
    "            (df['updated_regular_po_qty'] < df['Min. Order']),\n",
    "            df['Min. Order'],\n",
    "            df['updated_regular_po_qty']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 10. Calculate costs if by multiplying with contribution percentage\n",
    "        df['emergency_po_cost'] = (df['emergency_po_qty'] * df['HPP']).round(2)\n",
    "        df['final_updated_regular_po_cost'] = (df['final_updated_regular_po_qty'] * df['HPP']).round(2)\n",
    "        \n",
    "        # Clean up any remaining NaN or infinite values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_inventory_metrics: {str(e)}\")\n",
    "        return df_clean\n",
    "\n",
    "def clean_po_data(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "        # remove duplicate SKU\n",
    "        df = df.drop_duplicates(subset=['SKU'], keep='first')\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'Kategori Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP', 'Harga'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP', 'Harga']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP', 'Harga'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert SKU to string and clean it\n",
    "        if 'SKU' in df.columns:\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP', 'Min. Order', 'Harga'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "\n",
    "        # location_upper = location.upper()\n",
    "        # exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        # needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        # if padang_sales is not None:\n",
    "        #     # Ensure padang_sales has the required columns\n",
    "        #     padang_sales = padang_sales.copy()\n",
    "        #     padang_sales.columns = padang_sales.columns.str.strip()\n",
    "            \n",
    "        #     # Convert SKU to string in both dataframes\n",
    "        #     df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "        #     padang_sales['SKU'] = padang_sales['SKU'].astype(str).str.strip()\n",
    "            \n",
    "        #     padang_skus = set(padang_sales['SKU'].unique())\n",
    "        #     df['Is in Padang'] = df['SKU'].isin(padang_skus).astype(int)\n",
    "        # else:\n",
    "        #     print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "        #     df['Is in Padang'] = 0\n",
    "\n",
    "        # if not needs_padang_override:\n",
    "        #     return df\n",
    "\n",
    "        # if padang_sales is None:\n",
    "        #     raise ValueError(\n",
    "        #         \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "        #         \"or any store with contribution < 100%.\"\n",
    "        #     )\n",
    "\n",
    "        # Process Padang sales data\n",
    "        # padang_df = padang_sales.copy()\n",
    "        # padang_df.columns = padang_df.columns.str.strip()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        # required_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        # missing_cols = [col for col in required_cols if col not in padang_df.columns]\n",
    "        # if missing_cols:\n",
    "        #     raise ValueError(f\"Missing required columns in Padang sales data: {missing_cols}\")\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        # if 'Daily Sales' in df.columns:\n",
    "        #     df['Orig Daily Sales'] = df['Daily Sales']\n",
    "        # if 'Max. Daily Sales' in df.columns:\n",
    "        #     df['Orig Max. Daily Sales'] = df['Max. Daily Sales']\n",
    "\n",
    "        # print(\"Overriding with Padang sales data...\")\n",
    "        \n",
    "        # Ensure SKU is string in both dataframes before merge\n",
    "        # df['SKU'] = df['SKU'].astype(str)\n",
    "        # padang_df['SKU'] = padang_df['SKU'].astype(str)\n",
    "        \n",
    "        # DISABLE Padang sales override\n",
    "        # Merge with Padang's sales data\n",
    "        # df = df.merge(\n",
    "        #     padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "        #         'Daily Sales': 'Padang Daily Sales',\n",
    "        #         'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "        #     }),\n",
    "        #     on='SKU',\n",
    "        #     how='left'\n",
    "        # )\n",
    "        \n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        # if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "        #     df['Daily Sales'] = np.where(\n",
    "        #         df['Is in Padang'] == 1,\n",
    "        #         df['Padang Daily Sales'] * df['contribution_ratio'],\n",
    "        #         df['Orig Daily Sales']\n",
    "        #     )\n",
    "            \n",
    "        # if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "        #     df['Max. Daily Sales'] = np.where(\n",
    "        #         df['Is in Padang'] == 1,\n",
    "        #         df['Padang Max Daily Sales'] * df['contribution_ratio'],\n",
    "        #         df['Orig Max. Daily Sales']\n",
    "        #     )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        # columns_to_drop = [\n",
    "        #     'Padang Daily Sales', 'Padang Max Daily Sales',\n",
    "        # ]\n",
    "        # df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        # calculate sales contribution\n",
    "        df['sales_contribution'] = df['Daily Sales'] * df['Harga']\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_store_name_from_filename(filename):\n",
    "    \"\"\"Extract store name from filename, handling different patterns.\"\"\"\n",
    "    # Remove file extension and split by spaces\n",
    "    name_parts = Path(filename).stem.split()\n",
    "    \n",
    "    # Handle cases like \"002 Miss Glam Pekanbaru.csv\" -> \"Pekanbaru\"\n",
    "    # or \"01 Miss Glam Padang.csv\" -> \"Padang\"\n",
    "    if len(name_parts) >= 3 and name_parts[1].lower() == 'miss' and name_parts[2].lower() == 'glam':\n",
    "        return ' '.join(name_parts[3:]).strip().upper()\n",
    "    elif len(name_parts) >= 2 and name_parts[0].lower() == 'miss' and name_parts[1].lower() == 'glam':\n",
    "        return ' '.join(name_parts[2:]).strip().upper()\n",
    "    # Fallback: take everything after the first space\n",
    "    elif ' ' in filename:\n",
    "        return ' '.join(name_parts[1:]).strip().upper()\n",
    "    return name_parts[0].upper()\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # List of (separator, encoding) combinations to try\n",
    "    formats_to_try = [\n",
    "        (',', 'utf-8'),      # Standard CSV with comma\n",
    "        (';', 'utf-8'),      # Semicolon with UTF-8\n",
    "        (',', 'latin1'),     # Comma with Latin1\n",
    "        (';', 'latin1'),     # Semicolon with Latin1\n",
    "        (',', 'cp1252'),     # Windows-1252 encoding\n",
    "        (';', 'cp1252')\n",
    "    ]\n",
    "    \n",
    "    for sep, enc in formats_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=sep,\n",
    "                decimal=',',\n",
    "                thousands='.',\n",
    "                encoding=enc,\n",
    "                engine='python'  # More consistent behavior with Python engine\n",
    "            )\n",
    "            # If we get here, the file was read successfully\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError, pd.errors.EmptyDataError) as e:\n",
    "            continue  # Try next format\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading {file_path} with sep='{sep}', encoding='{enc}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we get here, all attempts failed\n",
    "    print(f\"Failed to read {file_path} with any known format\")\n",
    "    return None\n",
    "\n",
    "def _find_sku_col(cols):\n",
    "    for c in cols:\n",
    "        if str(c).strip().lower() == \"sku\":\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=False):\n",
    "    \"\"\"Process a single PO file and return merged data and summary.\"\"\"\n",
    "    print(f\"\\nProcessing PO file: {file_path.name} ....\")\n",
    "    \n",
    "    try:\n",
    "        # Extract location from filename using the new function\n",
    "        location = get_store_name_from_filename(file_path.name)\n",
    "        print(f\"  - Extracted location: {location}\")  # Debug print\n",
    "        \n",
    "        contribution_pct = get_contribution_pct(location, store_contrib)\n",
    "        \n",
    "        # Read the CSV with error handling\n",
    "        try:\n",
    "            # Try reading with different encodings if needed\n",
    "            if is_excel_folder:\n",
    "                df = read_excel_file(file_path)\n",
    "            else:\n",
    "                df = read_csv_file(file_path)\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df is None or df.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "                \n",
    "            # Clean the data\n",
    "            df_clean = clean_po_data(df, location, contribution_pct, df_padang)\n",
    "            # Skip if cleaning failed\n",
    "            if df_clean is None or df_clean.empty:\n",
    "                raise ValueError(\"Data cleaning failed\")\n",
    "\n",
    "            # --- Top 100 SKU handling ---\n",
    "            df_top_100 = load_top_100_sku_for_store(\n",
    "                location,\n",
    "                expected_header_keys=[\"SKU\", \"Nama\", \"Brand\", \"Toko\"]\n",
    "            )\n",
    "\n",
    "            top_sku_col = _find_sku_col(df_top_100.columns) if df_top_100 is not None else None\n",
    "            clean_sku_col = _find_sku_col(df_clean.columns)\n",
    "            if (\n",
    "                df_top_100 is not None\n",
    "                and not df_top_100.empty\n",
    "                and top_sku_col is not None\n",
    "                and clean_sku_col is not None\n",
    "            ):\n",
    "                top_skus = set(df_top_100[top_sku_col].astype(str).str.strip())\n",
    "                df_clean[\"is_top_100_sku\"] = (\n",
    "                    df_clean[clean_sku_col].astype(str).str.strip().isin(top_skus).astype(int)\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  - No valid Top 100 data for {location}, setting is_top_100_sku = 0\")\n",
    "                df_clean[\"is_top_100_sku\"] = 0\n",
    "\n",
    "            if (\n",
    "                df_top_100 is not None\n",
    "                and not df_top_100.empty\n",
    "                and \"SKU\" in df_top_100.columns\n",
    "                and \"SKU\" in df_clean.columns\n",
    "            ):\n",
    "                top_skus = set(df_top_100[\"SKU\"].astype(str).str.strip())\n",
    "                df_clean[\"is_top_100_sku\"] = (\n",
    "                    df_clean[\"SKU\"].astype(str).str.strip().isin(top_skus).astype(int)\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  - No valid Top 100 data for {location}, setting is_top_100_sku = 0\")\n",
    "                df_clean[\"is_top_100_sku\"] = 0\n",
    "            # --- end Top 100 SKU handling ---\n",
    "        \n",
    "            # calculate metrics PO\n",
    "            df_clean = calculate_inventory_metrics(df_clean)\n",
    "            \n",
    "            # Merge with suppliers\n",
    "            merged_df = merge_with_suppliers(df_clean, supplier_df)\n",
    "            # Generate summary\n",
    "            padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "            other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                                  (merged_df['Nama Store'] != '')).sum()\n",
    "            \n",
    "            summary = {\n",
    "                'file': file_path.name,\n",
    "                'location': location,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'total_rows': len(merged_df),\n",
    "                'padang_suppliers': int(padang_count),\n",
    "                'other_suppliers': int(other_supplier_count),\n",
    "                'no_supplier': int((merged_df['Nama Store'] == '').sum()),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            return merged_df, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file data: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {file_path.name}: {str(e)}\"\n",
    "        print(f\"  - {error_msg}\")\n",
    "        return None, {\n",
    "            'file': file_path.name,\n",
    "            'location': location if 'location' in locals() else 'Unknown',\n",
    "            'contribution_pct': contribution_pct if 'contribution_pct' in locals() else 0,\n",
    "            'total_rows': 0,\n",
    "            'padang_suppliers': 0,\n",
    "            'other_suppliers': 0,\n",
    "            'no_supplier': 0,\n",
    "            'status': f\"Error: {str(e)[:100]}\"  # Truncate long error messages\n",
    "        }\n",
    "\n",
    "def load_padang_data(padang_path):\n",
    "    \"\"\"Load Padang data from either CSV or Excel file.\n",
    "    \n",
    "    Args:\n",
    "        padang_path: Path to the input file (CSV or XLSX)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and cleaned Padang data\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported or file cannot be read\n",
    "    \"\"\"\n",
    "    print(f\"Loading Padang data from {padang_path}...\")\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = str(padang_path).lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(padang_path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(padang_path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Padang data with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Padang data from {padang_path}: {str(e)}\")\n",
    "\n",
    "def format_number_for_csv(x):\n",
    "    \"\"\"Format numbers for CSV output with Indonesian locale (comma as decimal, dot as thousand)\"\"\"\n",
    "    if pd.isna(x) or x == '':\n",
    "        return x\n",
    "    try:\n",
    "        if isinstance(x, (int, float)):\n",
    "            if x == int(x):  # Whole number\n",
    "                return f\"{int(x):,d}\".replace(\",\", \".\")\n",
    "            else:  # Decimal number\n",
    "                return f\"{x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def clean_and_convert(df):\n",
    "    \"\"\"Clean and convert DataFrame columns to appropriate types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string first to handle NaN/None consistently\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Define NA values that should be treated as empty/missing\n",
    "    na_values = list(NA_VALUES)\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        # Replace NA values with empty string first (treating them as literals, not regex)\n",
    "        df[col] = df[col].replace(na_values, '', regex=False)\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if df[col].empty:\n",
    "            continue\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if col in NUMERIC_COLUMNS:\n",
    "            # Convert to numeric, coercing errors to NaN, then fill with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            # For non-numeric columns, ensure they're strings and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace empty strings with NaN and then fill with empty string\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            # df[col] = df[col].replace('', np.nan).fillna('').infer_objects(copy=False)\n",
    "            df[col] = df[col].replace('', np.nan).fillna('')\n",
    "            df[col] = df[col].infer_objects(copy=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file with robust error handling for problematic values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nReading excel file: {file_path.name}...\")\n",
    "        \n",
    "        # First, read the file with openpyxl directly to handle the data more carefully\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = load_workbook(\n",
    "            filename=file_path,\n",
    "            read_only=True,    # Read-only mode is faster and uses less memory\n",
    "            data_only=True,    # Get the stored value instead of the formula\n",
    "            keep_links=False   # Don't load external links\n",
    "        )\n",
    "        \n",
    "        # Get the first sheet\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Get headers from the first row\n",
    "        headers = []\n",
    "        for idx, cell in enumerate(next(ws.iter_rows(values_only=True))):\n",
    "            header = str(cell).strip() if cell not in (None, '') else f\"Column_{idx + 1}\"\n",
    "            headers.append(header)\n",
    "        \n",
    "        # Initialize data rows\n",
    "        data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for row in ws.iter_rows(min_row=2, values_only=True):  # Skip header row\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                if cell is None:\n",
    "                    row_data.append('')\n",
    "                    continue\n",
    "\n",
    "                cell_str = str(cell).strip()\n",
    "                if cell_str.upper() in NA_VALUES:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    row_data.append(cell_str)\n",
    "            \n",
    "            # Only add row if it has data\n",
    "            if any(cell != '' for cell in row_data):\n",
    "                data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Normalize column data types\n",
    "        df = clean_and_convert(df)\n",
    "        \n",
    "        print(f\"✅ Successfully processed {file_path.name} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path.name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_file(df, file_path, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame to file with consistent extension and content type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path object or string for the output file\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments to pass to to_csv or to_excel\n",
    "        \n",
    "    Returns:\n",
    "        Path: The path where the file was saved\n",
    "    \"\"\"\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the correct file extension\n",
    "    if not file_path.suffix.lower() == f'.{file_format}':\n",
    "        file_path = file_path.with_suffix(f'.{file_format}')\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_output = df.copy()\n",
    "    \n",
    "    # Common preprocessing\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].astype(str).str.strip()\n",
    "        if file_format == 'xlsx':\n",
    "            # For Excel, wrap SKU in =\"...\" to preserve leading zeros\n",
    "            df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Format numbers for CSV if needed\n",
    "    if file_format == 'csv':\n",
    "        numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "    \n",
    "    # Save based on format\n",
    "    if file_format == 'csv':\n",
    "        df_output.to_csv(\n",
    "            file_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig',\n",
    "            **kwargs\n",
    "        )\n",
    "    elif file_format == 'xlsx':\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "            df_output.to_excel(writer, index=False, **kwargs)\n",
    "            \n",
    "            # Format SKU column as text in Excel\n",
    "            if 'SKU' in df_output.columns:\n",
    "                ws = writer.sheets[list(writer.sheets.keys())[0]]\n",
    "                sku_col_idx = df_output.columns.get_loc(\"SKU\") + 1\n",
    "                for row in ws.iter_rows(\n",
    "                    min_row=2,  # Skip header\n",
    "                    max_row=ws.max_row,\n",
    "                    min_col=sku_col_idx,\n",
    "                    max_col=sku_col_idx\n",
    "                ):\n",
    "                    for cell in row:\n",
    "                        cell.number_format = numbers.FORMAT_TEXT\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "    \n",
    "    print(f\"File saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_to_complete_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save Complete format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "\n",
    "    return save_file(df, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_m2_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save M2 format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_filtered = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[['Toko', 'SKU', 'HPP', 'final_updated_regular_po_qty']]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_M2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_M2_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_emergency_po_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save emergency PO format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with emergency PO qty > 0\n",
    "    df_filtered = df[df['emergency_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[[\n",
    "        'Brand', 'SKU', 'Nama', 'Toko', 'HPP', \n",
    "        'emergency_po_qty', 'emergency_po_cost'\n",
    "    ]]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_EMERGENCY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_EMERGENCY_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    supplier_df = load_supplier_data(SUPPLIER_PATH)\n",
    "    store_contrib = load_store_contribution(STORE_CONTRIBUTION_PATH)\n",
    "    all_summaries = []\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    CURRENT_DIR = INPUT_DIR / current_date\n",
    "\n",
    "    # get padang df first\n",
    "    df_padang = load_padang_data(CURRENT_DIR / '1. Miss Glam Padang.xlsx')\n",
    "\n",
    "    # test_xlsx_convert()\n",
    "\n",
    "    # Process each PO file\n",
    "    for file_path in sorted(CURRENT_DIR.glob('*.xlsx')):\n",
    "        try:\n",
    "            merged_df, summary = process_po_file(file_path, supplier_df, store_contrib, df_padang, is_excel_folder=True)\n",
    "\n",
    "            save_to_complete_format(merged_df, file_path.name, file_format='xlsx')\n",
    "            save_to_complete_format(merged_df, file_path.name)\n",
    "            save_to_m2_format(merged_df, file_path.name)\n",
    "            save_to_emergency_po_format(merged_df, file_path.name)\n",
    "\n",
    "            # summary['output_path'] = str(output_path)\n",
    "            output_path = OUTPUT_DIR / file_path.name\n",
    "            summary['output_path'] = str(output_path)\n",
    "\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  - Location: {summary['location']}\")\n",
    "            print(f\"  - Contribution: {summary['contribution_pct']}%\")\n",
    "            print(f\"  - Rows processed: {summary['total_rows']}\")\n",
    "            print(f\"  - 'Miss Glam Padang' suppliers: {summary['padang_suppliers']} rows\")\n",
    "            print(f\"  - Other suppliers: {summary['other_suppliers']} rows\")\n",
    "            print(f\"  - No supplier data: {summary['no_supplier']} rows\")\n",
    "            print(f\"  - Saved to: {output_path}\")\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Display final summary\n",
    "    if all_summaries:\n",
    "        print(\"\\nProcessing complete! Summary:\")\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Show sample of last processed file\n",
    "        print(\"\\nSample of the last processed file:\")\n",
    "        display(merged_df)\n",
    "    else:\n",
    "        print(\"\\nNo files were processed successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e089be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override merge_with_suppliers to prevent filling supplier data for rows with empty Brand\n",
    "print(\"Applying empty-Brand-safe merge_with_suppliers override...\")\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier info, skipping fallback for blank Brand rows.\"\"\"\n",
    "    print(\"Merging with suppliers (override)...\")\n",
    "\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "\n",
    "    df_clean = df_clean.copy()\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "\n",
    "    merged_df['_brand_clean'] = merged_df['Brand'].astype(str).str.strip()\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    fallback_mask = unmatched_mask & (merged_df['_brand_clean'] != '')\n",
    "\n",
    "    if fallback_mask.any():\n",
    "        print(\n",
    "            f\"Found {fallback_mask.sum()} rows without store match; attempting brand-only fallback for non-empty brands...\"\n",
    "        )\n",
    "        unmatched_rows = merged_df[fallback_mask].copy()\n",
    "        supplier_cols = [\n",
    "            col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand'\n",
    "        ]\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols, errors='ignore')\n",
    "        unmatched_rows['Brand'] = unmatched_rows['_brand_clean']\n",
    "\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "\n",
    "        matched_initial = merged_df[~fallback_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "\n",
    "    merged_df = merged_df.drop(columns=['_brand_clean'], errors='ignore')\n",
    "\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store',\n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "\n",
    "    return merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
