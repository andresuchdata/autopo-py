{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = os.listdir('data/rawpo/xlsx')\n",
    "display(raw_files)\n",
    "\n",
    "def convert_xlsx_to_csv(directory):\n",
    "    \"\"\"\n",
    "    Convert all .xlsx files in the specified directory to .csv format.\n",
    "    Skips files that already have a corresponding .csv file.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .xlsx files\n",
    "    \"\"\"\n",
    "    # Convert to Path object for easier handling\n",
    "    dir_path = Path(directory)\n",
    "    \n",
    "    # Find all .xlsx files in the directory\n",
    "    xlsx_files = list(dir_path.glob('*.xlsx'))\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No .xlsx files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} .xlsx files to process...\")\n",
    "    \n",
    "    for xlsx_file in xlsx_files:\n",
    "        # Create output filename with .csv extension\n",
    "        csv_file = xlsx_file.with_suffix('.csv')\n",
    "        \n",
    "        # Skip if CSV already exists\n",
    "        if csv_file.exists():\n",
    "            print(f\"Skipping {xlsx_file.name} - {csv_file.name} already exists\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read the Excel file\n",
    "            print(f\"\\n===========================================Converting {xlsx_file.name} to {csv_file.name}...\")\n",
    "            df = pd.read_excel(xlsx_file)\n",
    "            \n",
    "            # Write to CSV\n",
    "            df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully created {csv_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {xlsx_file.name}: {str(e)}\")\n",
    "    \n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "# Example usage:\n",
    "convert_xlsx_to_csv('data/rawpo/xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97300143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file, converting 'INF' to numpy.inf\n",
    "ori_df = pd.read_csv('data/rawpo/01 Miss Glam Padang.csv', sep=';', decimal=',')\n",
    "\n",
    "# Convert all numeric columns, handling infinity and NaN values\n",
    "for col in ori_df.select_dtypes(include=[np.number]).columns:\n",
    "    ori_df[col] = pd.to_numeric(ori_df[col], errors='coerce')\n",
    "\n",
    "df = ori_df.copy()\n",
    "df = df.rename(columns={'Stok': 'Stock'})\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# extract only the columns we need\n",
    "display(df.info())\n",
    "df = df[['Brand', 'SKU', 'Nama', 'Stock', 'Daily Sales', 'Max. Daily Sales', 'Lead Time', 'Max. Lead Time', 'Sedang PO', 'Min. Order', 'HPP']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "# contribution dictionary for each store location\n",
    "contribution_dict = {\n",
    "    'payakumbuh': 0.47,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplier mapping\n",
    "# to map an SKU and brand to specific supplier\n",
    "\n",
    "raw_supplier_df = pd.read_csv('data/supplier.csv', sep=';', decimal=',')\n",
    "raw_supplier_df = raw_supplier_df.fillna('')\n",
    "\n",
    "display(raw_supplier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f860f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert object columns to numeric where possible\n",
    "numeric_columns = ['Stock', 'Daily Sales', 'Lead Time', 'Max. Daily Sales', 'Max. Lead Time']\n",
    "\n",
    "df_clean = df.copy()\n",
    "display('Raw DataFrame: ', df)\n",
    "\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "for col in numeric_columns:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Now fill NA with 0 and convert to int\n",
    "df_clean = df_clean.fillna(0)\n",
    "\n",
    "# For non-numeric columns, keep them as they are\n",
    "non_numeric_columns = ['Brand', 'SKU']  # Add other non-numeric columns if needed\n",
    "for col in non_numeric_columns:\n",
    "    df_clean[col] = df[col]  # Keep original values\n",
    "\n",
    "# add new column 'Lead Time Sedang PO' default to 2 days\n",
    "df_clean['Lead Time Sedang PO'] = 5\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"Cleaned DataFrame:\")\n",
    "display(df_clean)\n",
    "\n",
    "# Show info of the cleaned DataFrame\n",
    "print(\"\\nDataFrame Info: Expected to have maximal non-null values...\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e2257",
   "metadata": {},
   "source": [
    "### Add Supply Chain params for AutoPO\n",
    "\n",
    "- Safety stock - (max sales x max lead time) - (avg sales x avg lead time)\n",
    "- Reorder point - avg sales x avg lead time + safety stock\n",
    "- Stock cover days (for 21 days) - avg sales x 21\n",
    "- RoP_Reference (1 -> RoP > Stock cover days, 0 -> RoP < Stock cover days)\n",
    "- Current stock days cover -> Current stock / avg sales\n",
    "- Is_open_po (1 -> Current Stock < Reorder point, 0 -> otherwise)\n",
    "- Initial_Qty_PO - Reorder point - Current stock\n",
    "\n",
    "- Is_emergency_PO - 1 -> Current stock days cover <= max lead time\n",
    "\n",
    "- Emergency_PO_Qty - (max lead time - Current stock days cover) x Avg sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683891c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# 1. Safety stock = (max sales x max lead time) - (avg sales x avg lead time)\n",
    "df_clean['Safety stock'] = (df_clean['Max. Daily Sales'] * df_clean['Max. Lead Time']) - (df_clean['Daily Sales'] * df_clean['Lead Time'])\n",
    "# round up safety stock\n",
    "df_clean['Safety stock'] = df_clean['Safety stock'].apply(lambda x: np.ceil(x)).astype(int)\n",
    "\n",
    "# 2. Reorder point = (avg sales x avg lead time) + safety stock\n",
    "df_clean['Reorder point'] = np.ceil((df_clean['Daily Sales'] * df_clean['Lead Time']) + \n",
    "                                   df_clean['Safety stock']).astype(int)\n",
    "\n",
    "# 3. Stock cover days (in Qty) for 30 days = avg sales x 30 \n",
    "df_clean['Stock cover 30 days'] = df_clean['Daily Sales'] * 30\n",
    "df_clean['Stock cover 30 days'] = df_clean['Stock cover 30 days'].apply(lambda x: np.ceil(x)).astype(int)\n",
    "\n",
    "# 5. Current stock days cover (in days) = Current stock / avg sales\n",
    "df_clean['current_stock_days_cover'] = (df_clean['Stock'].astype(float) * 1.0 ) / df_clean['Daily Sales'].astype(float)\n",
    "\n",
    "# 6. Is_open_po (1 -> Current stock < Reorder point, 0 -> otherwise)\n",
    "df_clean['is_open_po'] = np.where((df_clean['current_stock_days_cover'] <= 30) & (df_clean['Stock'] <= df_clean['Reorder point']), 1, 0)\n",
    "\n",
    "# 7. Initial_Qty_PO = Stock cover 30 days - Current stock - sedang PO\n",
    "df_clean['initial_qty_po'] = df_clean['Stock cover 30 days'] - df_clean['Stock'] - df_clean['Sedang PO']\n",
    "df_clean['initial_qty_po'] = np.where(df_clean['is_open_po'] == 1, df_clean['initial_qty_po'], 0)\n",
    "df_clean['initial_qty_po'] = df_clean['initial_qty_po'].apply(lambda x: x if x > 0 else 0).astype(int)\n",
    "\n",
    "# 9. Emergency_PO_Qty = (max lead time - Current stock days cover) x Avg sales\n",
    "# First, ensure 'Sedang PO' column exists and handle potential missing values\n",
    "if 'Sedang PO' not in df_clean.columns:\n",
    "    df_clean['Sedang PO'] = 0  # Default to 0 if column doesn't exist\n",
    "\n",
    "# Calculate emergency_po_qty based on the condition\n",
    "df_clean['emergency_po_qty'] = np.where(\n",
    "    df_clean['Sedang PO'] > 0,  # If there is 'Sedang PO' quantity\n",
    "    np.maximum(0, (df_clean['Lead Time Sedang PO'] - df_clean['current_stock_days_cover']) * \n",
    "              df_clean['Daily Sales']),\n",
    "    # Else use the original formula\n",
    "    np.ceil((df_clean['Max. Lead Time'] - df_clean['current_stock_days_cover']) * \n",
    "            df_clean['Daily Sales'])\n",
    ")\n",
    "\n",
    "# First, handle any infinite values and NaN values\n",
    "df_clean['emergency_po_qty'] = (\n",
    "    df_clean['emergency_po_qty']\n",
    "    .replace([np.inf, -np.inf], 0)  # Replace infinities with 0\n",
    "    .fillna(0)                      # Fill any remaining NaNs with 0\n",
    "    .astype(int)                    # Now safely convert to integers\n",
    ")\n",
    "\n",
    "# If you want to ensure no negative values (since it's a quantity)\n",
    "df_clean['emergency_po_qty'] = df_clean['emergency_po_qty'].clip(lower=0)\n",
    "\n",
    "# calculate updated po quantity\n",
    "df_clean['updated_regular_po_qty'] = df_clean['initial_qty_po'] - df_clean['emergency_po_qty']\n",
    "df_clean['updated_regular_po_qty'] = df_clean['updated_regular_po_qty'].apply(lambda x: x if x > 0 else 0).astype(int)\n",
    "\n",
    "# Final check updated regular PO - if less than Min. Order, use Min. Order qty\n",
    "df_clean['final_updated_regular_po_qty'] = np.where((df_clean['updated_regular_po_qty'] > 0) & (df_clean['updated_regular_po_qty'] < df_clean['Min. Order']), df_clean['Min. Order'], df_clean['updated_regular_po_qty'])\n",
    "\n",
    "\n",
    "# Calculate total cost (HPP * qty) for emergency PO and final updated regular PO\n",
    "df_clean['total_cost_emergency_po'] = df_clean['emergency_po_qty'] * df_clean['HPP']\n",
    "df_clean['total_cost_final_updated_regular_po'] = df_clean['final_updated_regular_po_qty'] * df_clean['HPP']\n",
    "\n",
    "# Handle any NaN or infinite values by replacing them with 0\n",
    "df_clean = df_clean.fillna(0)\n",
    "df_clean = df_clean.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Display the updated DataFrame with new columns\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = 'output/result.csv'\n",
    "df_clean.to_csv(csv_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "print(f\"CSV file saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178c94d",
   "metadata": {},
   "source": [
    "### Mapping Brand and SKU with supplier (add supplier column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89776ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make copies to avoid modifying originals\n",
    "df_clean_trimmed = df_clean.copy()\n",
    "raw_supplier_trimmed = raw_supplier_df.copy()\n",
    "\n",
    "# Trim whitespace from brand names\n",
    "df_clean_trimmed['Brand'] = df_clean_trimmed['Brand'].str.strip()\n",
    "raw_supplier_trimmed['Nama Brand'] = raw_supplier_trimmed['Nama Brand'].str.strip()\n",
    "\n",
    "# First, get all Padang suppliers\n",
    "padang_suppliers = raw_supplier_trimmed[\n",
    "    raw_supplier_trimmed['Nama Store'] == 'Miss Glam Padang'\n",
    "]\n",
    "\n",
    "# Then get all other suppliers (non-Padang)\n",
    "other_suppliers = raw_supplier_trimmed[\n",
    "    raw_supplier_trimmed['Nama Store'] != 'Miss Glam Padang'\n",
    "]\n",
    "\n",
    "# Step 1: Left join with Padang suppliers first (priority)\n",
    "merged_df = pd.merge(\n",
    "    df_clean_trimmed,\n",
    "    padang_suppliers,\n",
    "    left_on='Brand',\n",
    "    right_on='Nama Brand',\n",
    "    how='left',\n",
    "    suffixes=('_clean', '_supplier')\n",
    ")\n",
    "\n",
    "# Step 2: For rows without Padang supplier, try to find other suppliers\n",
    "# Get the indices of rows that didn't get a match with Padang suppliers\n",
    "no_padang_match = merged_df[merged_df['Nama Brand'].isna()].index\n",
    "\n",
    "if len(no_padang_match) > 0:\n",
    "    # Get the brands that need non-Padang suppliers\n",
    "    brands_needing_suppliers = merged_df.loc[no_padang_match, 'Brand'].unique()\n",
    "    \n",
    "    # Get the first matching supplier for each brand (you can change this logic if needed)\n",
    "    first_supplier_per_brand = other_suppliers.drop_duplicates(subset='Nama Brand')\n",
    "    \n",
    "    # Update the rows that didn't have Padang suppliers\n",
    "    for brand in brands_needing_suppliers:\n",
    "        supplier_data = first_supplier_per_brand[first_supplier_per_brand['Nama Brand'] == brand]\n",
    "        if not supplier_data.empty:\n",
    "            # Update the corresponding rows in merged_df\n",
    "            brand_mask = (merged_df['Brand'] == brand) & (merged_df['Nama Brand'].isna())\n",
    "            for col in supplier_data.columns:\n",
    "                if col in merged_df.columns and col != 'Brand':  # Don't overwrite the Brand column\n",
    "                    merged_df.loc[brand_mask, col] = supplier_data[col].values[0]\n",
    "\n",
    "# Clean up: For any remaining NaN values in supplier columns, fill with empty string or as needed\n",
    "supplier_columns = [\n",
    "    'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "    'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "    'Promo Factor', 'Delay Factor'\n",
    "]\n",
    "\n",
    "for col in supplier_columns:\n",
    "    if col in merged_df.columns:\n",
    "        if merged_df[col].dtype == 'object':\n",
    "            merged_df[col] = merged_df[col].fillna('')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# Show summary\n",
    "print(f\"Total rows in df_clean: {len(df_clean_trimmed)}\")\n",
    "print(f\"Total rows after merge: {len(merged_df)}\")\n",
    "\n",
    "# Count how many rows got Padang suppliers vs other suppliers vs no suppliers\n",
    "padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                       (merged_df['Nama Store'] != '')).sum()\n",
    "no_supplier = (merged_df['Nama Store'] == '').sum()\n",
    "\n",
    "print(f\"\\nSuppliers matched:\")\n",
    "print(f\"- 'Miss Glam Padang' suppliers: {padang_count} rows\")\n",
    "print(f\"- Other suppliers: {other_supplier_count} rows\")\n",
    "print(f\"- No supplier data: {no_supplier} rows\")\n",
    "\n",
    "# Save the result\n",
    "os.makedirs('output', exist_ok=True)\n",
    "output_path = 'output/merged_with_suppliers.csv'\n",
    "merged_df.to_csv(output_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "\n",
    "# Show a sample of the results\n",
    "print(\"\\nSample of merged data (first 5 rows):\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_clean with raw_supplier_df to see all supplier matches\n",
    "all_suppliers_merge = pd.merge(\n",
    "    df_clean_trimmed,\n",
    "    raw_supplier_trimmed,\n",
    "    left_on='Brand',\n",
    "    right_on='Nama Brand',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Group by Brand and SKU to count unique suppliers\n",
    "supplier_counts = all_suppliers_merge.groupby(['Brand', 'SKU'])['Nama Supplier'].nunique().reset_index()\n",
    "supplier_counts.columns = ['Brand', 'SKU', 'Supplier_Count']\n",
    "\n",
    "# Filter for brands/SKUs with multiple suppliers\n",
    "multi_supplier_items = supplier_counts[supplier_counts['Supplier_Count'] > 1]\n",
    "\n",
    "print(f\"Found {len(multi_supplier_items)} brand/SKU combinations with multiple suppliers\")\n",
    "print(\"\\nSample of items with multiple suppliers:\")\n",
    "display(multi_supplier_items.head())\n",
    "\n",
    "# If you want to see the actual supplier details for these items\n",
    "if not multi_supplier_items.empty:\n",
    "    print(\"\\nDetailed supplier information for multi-supplier items:\")\n",
    "    multi_supplier_details = all_suppliers_merge.merge(\n",
    "        multi_supplier_items[['Brand', 'SKU']],\n",
    "        on=['Brand', 'SKU']\n",
    "    )\n",
    "    display(multi_supplier_details[['Brand', 'SKU', 'Nama Supplier', 'Nama Store']].drop_duplicates().sort_values(['Brand', 'SKU']))\n",
    "\n",
    "    # List of SKUs to check\n",
    "skus_to_check = [\n",
    "    '8995232702124',  # ACNEMED\n",
    "    '8992821100293',  # ACNES\n",
    "    '8992821100309',  # ACNES\n",
    "    '8992821100323',  # ACNES\n",
    "    '8992821100354'   # ACNES\n",
    "]\n",
    "\n",
    "# Convert SKUs to integers (since they appear as integers in df_clean)\n",
    "skus_to_check = [int(sku) for sku in skus_to_check]\n",
    "\n",
    "# Check if these SKUs exist in df_clean\n",
    "found_skus = merged_df[merged_df['SKU'].isin(skus_to_check)]\n",
    "\n",
    "if not found_skus.empty:\n",
    "    print(\"Found matching SKUs in df_clean:\")\n",
    "    display(found_skus[['Brand', 'SKU', 'Nama']])\n",
    "else:\n",
    "    print(\"None of these SKUs were found in df_clean.\")\n",
    "    print(\"\\nChecking if there are any similar SKUs...\")\n",
    "    \n",
    "    # Check for any SKUs that contain these numbers\n",
    "    for sku in skus_to_check:\n",
    "        similar = merged_df[merged_df['SKU'].astype(str).str.contains(str(sku)[:8])]\n",
    "        if not similar.empty:\n",
    "            print(f\"\\nSKUs similar to {sku}:\")\n",
    "            display(similar[['Brand', 'SKU', 'Nama']])\n",
    "    \n",
    "    # Check the data types to ensure we're comparing correctly\n",
    "    print(\"\\nData type of SKU column:\", merged_df['SKU'].dtype)\n",
    "    print(\"Sample SKUs from df_clean:\", merged_df['SKU'].head().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a03",
   "metadata": {},
   "source": [
    "### Find brands who are missing suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find brands in df_clean that don't have a match in raw_supplier_df\n",
    "missing_brands = set(df_clean['Brand']) - set(raw_supplier_df['Nama Brand'].dropna().unique())\n",
    "\n",
    "print(f\"Number of brands in df_clean: {len(df_clean['Brand'].unique())}\")\n",
    "print(f\"Number of brands in raw_supplier_df: {len(raw_supplier_df['Nama Brand'].unique())}\")\n",
    "print(f\"\\nNumber of brands missing supplier data: {len(missing_brands)}\")\n",
    "print(\"\\nFirst 20 missing brands (alphabetical order):\")\n",
    "print(sorted(list(missing_brands))[:20])\n",
    "\n",
    "# Count how many rows are affected per missing brand\n",
    "missing_brand_counts = df_clean[df_clean['Brand'].isin(missing_brands)]['Brand'].value_counts()\n",
    "print(\"\\nTop 20 missing brands by row count:\")\n",
    "print(missing_brand_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8564e9",
   "metadata": {},
   "source": [
    "# Output grouped data per one brand and one SKU to separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'output_po'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a directory for brands without suppliers\n",
    "no_supplier_dir = os.path.join(output_dir, '0_no_suppliers')\n",
    "os.makedirs(no_supplier_dir, exist_ok=True)\n",
    "\n",
    "# Function to sanitize folder names\n",
    "def sanitize_folder_name(name):\n",
    "    # Remove or replace invalid characters\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        name = name.replace(char, '_')\n",
    "    return name.strip()\n",
    "\n",
    "# Process each group\n",
    "for (supplier_id, supplier_name, brand), group in final_df.groupby(['ID Supplier', 'Nama Supplier', 'Brand']):\n",
    "    # Skip if no supplier (shouldn't happen as we replaced NaN with defaults)\n",
    "    if pd.isna(supplier_id) or not supplier_name:\n",
    "        # Save to no_supplier_dir\n",
    "        brand_file = os.path.join(no_supplier_dir, f'{sanitize_folder_name(brand)}.csv')\n",
    "        group.to_csv(brand_file, index=False, sep=';', encoding='utf-8-sig')\n",
    "        continue\n",
    "    \n",
    "    # Create supplier directory\n",
    "    supplier_dir = os.path.join(output_dir, f'{int(supplier_id)}_{sanitize_folder_name(supplier_name)}')\n",
    "    os.makedirs(supplier_dir, exist_ok=True)\n",
    "    \n",
    "    # Save brand file\n",
    "    brand_file = os.path.join(supplier_dir, f'{sanitize_folder_name(brand)}.csv')\n",
    "    group.to_csv(brand_file, index=False, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data has been organized into supplier and brand-based folders in 'output_po'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6ba5",
   "metadata": {},
   "source": [
    "# Final batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce491334",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = [\n",
    "    'HPP', 'Harga', 'Ranking', 'Grade', 'Terjual', 'Stok', 'Lost Days',\n",
    "    'Velocity Capped', 'Daily Sales', 'Lead Time', 'Max. Daily Sales',\n",
    "    'Max. Lead Time', 'Min. Order', 'Safety Stok', 'ROP', '3W Cover',\n",
    "    'Sedang PO', 'Suggested', 'Amount', 'Promo Factor', 'Delay Factor',\n",
    "    'Stock Cover', 'Days to Backup', 'Qty to Backup'\n",
    "]\n",
    "\n",
    "NA_VALUES = {\n",
    "    'NAN', 'NA', '#N/A', 'NULL', 'NONE', '', '?', '-', 'INF', '-INF',\n",
    "    '+INF', 'INFINITY', '-INFINITY', '1.#INF', '-1.#INF', '1.#QNAN'\n",
    "}\n",
    "\n",
    "def _patch_openpyxl_number_casting():\n",
    "    \"\"\"Ensure openpyxl won't crash when encountering NAN/INF in numeric cells.\"\"\"\n",
    "    print(\"Calling _patch_openpyxl_number_casting...\")\n",
    "\n",
    "    try:\n",
    "        from openpyxl.worksheet import _reader\n",
    "\n",
    "        original_cast = _reader._cast_number\n",
    "\n",
    "        def _safe_cast_number(value):  # pragma: no cover - monkey patch\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().upper() in NA_VALUES:\n",
    "                    return 0\n",
    "            try:\n",
    "                return original_cast(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0 if value in (None, '') else value\n",
    "\n",
    "        _reader._cast_number = _safe_cast_number\n",
    "    except Exception:\n",
    "        # If patch fails we continue; runtime reader will still attempt default behaviour\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import display\n",
    "import locale\n",
    "from locale import atof\n",
    "import numpy as np\n",
    "from openpyxl.styles import numbers\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "\n",
    "_patch_openpyxl_number_casting()\n",
    "\n",
    "# Apply the formatting to numeric columns in your final output\n",
    "def format_dataframe_display(df):\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # Apply formatting to numeric columns\n",
    "    for col in df_display.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: format_id_number(x, 2) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/Users/andresuchitra/dev/missglam/autopo')\n",
    "SUPPLIER_PATH = BASE_DIR / 'data/supplier.csv'\n",
    "RAWPO_DIR = BASE_DIR / 'data/rawpo/csv'\n",
    "RAWPO_XLSX_DIR = BASE_DIR / 'data/rawpo/xlsx'\n",
    "STORE_CONTRIBUTION_PATH = BASE_DIR / 'data/store_contribution.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'output/complete'\n",
    "OUTPUT_EXCEL_DIR = BASE_DIR / 'output/excel'\n",
    "OUTPUT_M2_DIR = BASE_DIR / 'output/m2'\n",
    "OUTPUT_EMERGENCY_DIR = BASE_DIR / 'output/emergency'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EXCEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_M2_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EMERGENCY_DIR, exist_ok=True)\n",
    "\n",
    "def load_store_contribution(store_contribution_path):\n",
    "    \"\"\"Load and prepare store contribution data.\"\"\"\n",
    "    store_contrib = pd.read_csv(store_contribution_path, header=None, \n",
    "                              names=['store', 'contribution_pct'])\n",
    "    # Convert store names to lowercase for case-insensitive matching\n",
    "    store_contrib['store_lower'] = store_contrib['store'].str.lower()\n",
    "    return store_contrib\n",
    "\n",
    "def get_contribution_pct(location, store_contrib):\n",
    "    \"\"\"Get contribution percentage for a given location.\"\"\"\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    contrib_row = store_contrib[store_contrib['store_lower'] == location_lower]\n",
    "    if not contrib_row.empty:\n",
    "        return contrib_row['contribution_pct'].values[0]\n",
    "    print(f\"Warning: No contribution percentage found for {location}\")\n",
    "\n",
    "    return 100  # Default to 100% if not found\n",
    "\n",
    "def load_supplier_data(supplier_path):\n",
    "    \"\"\"Load and clean supplier data.\"\"\"\n",
    "    print(\"Loading supplier data...\")\n",
    "    df = pd.read_csv(supplier_path, sep=';', decimal=',').fillna('')\n",
    "    df['Nama Brand'] = df['Nama Brand'].str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_with_suppliers(df_clean, supplier_df):\n",
    "    \"\"\"Merge PO data with supplier information.\"\"\"\n",
    "    print(\"Merging with suppliers...\")\n",
    "    \n",
    "    # Clean supplier data\n",
    "    supplier_clean = supplier_df.copy()\n",
    "    supplier_clean['Nama Brand'] = supplier_clean['Nama Brand'].astype(str).str.strip()\n",
    "    supplier_clean['Nama Store'] = supplier_clean['Nama Store'].astype(str).str.strip()\n",
    "    \n",
    "    # Deduplicate to prevent row explosion - Unique Brand+Store\n",
    "    supplier_clean = supplier_clean.drop_duplicates(subset=['Nama Brand', 'Nama Store'])\n",
    "    \n",
    "    # Ensure PO data has clean columns for merging\n",
    "    df_clean['Brand'] = df_clean['Brand'].astype(str).str.strip()\n",
    "    df_clean['Toko'] = df_clean['Toko'].astype(str).str.strip()\n",
    "    \n",
    "    # 1. Primary Merge: Match on Brand AND Store (Toko)\n",
    "    # This prioritizes the specific supplier for that store\n",
    "    merged_df = pd.merge(\n",
    "        df_clean,\n",
    "        supplier_clean,\n",
    "        left_on=['Brand', 'Toko'],\n",
    "        right_on=['Nama Brand', 'Nama Store'],\n",
    "        how='left',\n",
    "        suffixes=('_clean', '_supplier')\n",
    "    )\n",
    "    \n",
    "    # 2. Fallback: For unmatched rows, try to find ANY supplier for that Brand\n",
    "    # Identify rows where merge failed (Nama Brand is NaN)\n",
    "    unmatched_mask = merged_df['Nama Brand'].isna()\n",
    "    \n",
    "    if unmatched_mask.any():\n",
    "        print(f\"Found {unmatched_mask.sum()} rows without direct store match. Attempting fallback...\")\n",
    "        \n",
    "        # Get the unmatched rows and drop the empty supplier columns\n",
    "        unmatched_rows = merged_df[unmatched_mask].copy()\n",
    "        supplier_cols = [col for col in supplier_clean.columns if col in unmatched_rows.columns and col != 'Brand']\n",
    "        unmatched_rows = unmatched_rows.drop(columns=supplier_cols)\n",
    "        \n",
    "        # Create fallback supplier list (one per brand)\n",
    "        # We take the first one found for each brand\n",
    "        fallback_suppliers = supplier_clean.drop_duplicates(subset=['Nama Brand'])\n",
    "        \n",
    "        # Merge unmatched rows with fallback suppliers\n",
    "        matched_fallback = pd.merge(\n",
    "            unmatched_rows,\n",
    "            fallback_suppliers,\n",
    "            left_on='Brand',\n",
    "            right_on='Nama Brand',\n",
    "            how='left',\n",
    "            suffixes=('_clean', '_supplier')\n",
    "        )\n",
    "        \n",
    "        # Combine the initially matched rows with the fallback-matched rows\n",
    "        matched_initial = merged_df[~unmatched_mask]\n",
    "        merged_df = pd.concat([matched_initial, matched_fallback], ignore_index=True)\n",
    "    \n",
    "    # Clean up supplier columns\n",
    "    supplier_columns = [\n",
    "        'ID Supplier', 'Nama Supplier', 'ID Brand', 'ID Store', \n",
    "        'Nama Store', 'Hari Order', 'Min. Purchase', 'Trading Term',\n",
    "        'Promo Factor', 'Delay Factor'\n",
    "    ]\n",
    "    for col in supplier_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna('' if merged_df[col].dtype == 'object' else 0)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def calculate_inventory_metrics(df_clean):\n",
    "    \"\"\"\n",
    "    Calculate various inventory metrics including safety stock, reorder points, and PO quantities.\n",
    "    \n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Input dataframe with required columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added calculated columns\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "    df = df_clean.copy()\n",
    "    \n",
    "    # Set display options\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "    # Normalise stock column name\n",
    "    stock_col = 'Stok' if 'Stok' in df.columns else 'Stock'\n",
    "\n",
    "    # Force the columns we need into numeric form\n",
    "    numeric_cols = [\n",
    "        stock_col, 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "        'Max. Lead Time', 'Sedang PO', 'HPP', 'Lead Time Sedang PO'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    try:\n",
    "        # 1. Safety stock calculation\n",
    "        df['Safety stock'] = (df['Max. Daily Sales'] * df['Max. Lead Time']) - (df['Daily Sales'] * df['Lead Time'])\n",
    "        df['Safety stock'] = df['Safety stock'].apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 2. Reorder point calculation\n",
    "        df['Reorder point'] = np.ceil((df['Daily Sales'] * df['Lead Time']) + df['Safety stock']).fillna(0).astype(int)\n",
    "        \n",
    "        # 3. Stock cover for 30 days\n",
    "        df['Stock cover 30 days'] = (df['Daily Sales'] * 30).apply(lambda x: np.ceil(x)).fillna(0).astype(int)\n",
    "        \n",
    "        # 4. Current stock days cover\n",
    "        df['current_stock_days_cover'] = np.where(\n",
    "            df['Daily Sales'] > 0,\n",
    "            df[stock_col] / df['Daily Sales'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # 5. Is open PO flag\n",
    "        df['is_open_po'] = np.where(\n",
    "            (df['current_stock_days_cover'] < 30) & \n",
    "            (df['Stok'] <= df['Reorder point']), 1, 0\n",
    "        )\n",
    "        \n",
    "        # 6. Initial PO quantity\n",
    "        df['initial_qty_po'] = df['Stock cover 30 days'] - df[stock_col] - df.get('Sedang PO', 0)\n",
    "        df['initial_qty_po'] = (\n",
    "            pd.Series(\n",
    "                np.where(df['is_open_po'] == 1, df['initial_qty_po'], 0),\n",
    "                index=df.index\n",
    "            )\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 7. Emergency PO quantity\n",
    "        df['emergency_po_qty'] = np.where(\n",
    "            df.get('Sedang PO', 0) > 0,\n",
    "            np.maximum(0, (df['Lead Time Sedang PO'] - df['current_stock_days_cover']) * df['Daily Sales']),\n",
    "            np.ceil((df['Max. Lead Time'] - df['current_stock_days_cover']) * df['Daily Sales'])\n",
    "        )\n",
    "        \n",
    "        # Clean up emergency PO quantities\n",
    "        df['emergency_po_qty'] = (\n",
    "            df['emergency_po_qty']\n",
    "            .replace([np.inf, -np.inf], 0)\n",
    "            .fillna(0)\n",
    "            .clip(lower=0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        \n",
    "        # 8. Updated regular PO quantity\n",
    "        df['updated_regular_po_qty'] = (df['initial_qty_po'] - df['emergency_po_qty']).clip(lower=0).astype(int)\n",
    "        \n",
    "        # 9. Final updated regular PO quantity (enforce minimum order)\n",
    "        df['final_updated_regular_po_qty'] = np.where(\n",
    "            (df['updated_regular_po_qty'] > 0) & \n",
    "            (df['updated_regular_po_qty'] < df['Min. Order']),\n",
    "            df['Min. Order'],\n",
    "            df['updated_regular_po_qty']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 10. Calculate costs if by multiplying with contribution percentage\n",
    "        df['emergency_po_cost'] = (df['emergency_po_qty'] * df['HPP']).round(2)\n",
    "        df['final_updated_regular_po_cost'] = (df['final_updated_regular_po_qty'] * df['HPP']).round(2)\n",
    "        \n",
    "        # Clean up any remaining NaN or infinite values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_inventory_metrics: {str(e)}\")\n",
    "        return df_clean\n",
    "\n",
    "def clean_po_data_v1(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "        df['contribution_ratio'] = contribution_pct / 100\n",
    "\n",
    "        # Add default values for other required columns\n",
    "        if 'Lead Time Sedang PO' not in df.columns:\n",
    "            df['Lead Time Sedang PO'] = ''\n",
    "\n",
    "        location_upper = location.upper()\n",
    "        exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        print(f\"Processing store: {location} - {contribution_pct}%\")\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        if padang_sales is not None:\n",
    "            padang_skus = set(padang_sales['SKU'].astype(str).unique())\n",
    "            df['Is in Padang'] = df['SKU'].astype(str).isin(padang_skus).astype(int)\n",
    "        else:\n",
    "            print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "            df['Is in Padang'] = 0\n",
    "\n",
    "        if not needs_padang_override:\n",
    "            # If no override needed, ensure we have the original sales columns\n",
    "            if 'Daily Sales' not in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "                df['Daily Sales'] = df['Orig Daily Sales']\n",
    "            if 'Max. Daily Sales' not in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "                df['Max. Daily Sales'] = df['Orig Max. Daily Sales']\n",
    "            return df\n",
    "\n",
    "        if padang_sales is None:\n",
    "            raise ValueError(\n",
    "                \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "                \"or any store with contribution < 100%.\"\n",
    "            )\n",
    "\n",
    "        # Process Padang sales data - keep original column names\n",
    "        padang_df = padang_sales.copy()\n",
    "        padang_df.columns = padang_df.columns.str.strip()  # Only strip whitespace\n",
    "        \n",
    "        # Ensure required columns exist in padang_df\n",
    "        required_padang_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        missing_padang_cols = [col for col in required_padang_cols if col not in padang_df.columns]\n",
    "        \n",
    "        if missing_padang_cols:\n",
    "            raise ValueError(\n",
    "                f\"Padang sales data is missing required columns: {missing_padang_cols}. \"\n",
    "                f\"Available columns: {padang_df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        if 'Daily Sales' in df.columns and 'Orig Daily Sales' not in df.columns:\n",
    "            df = df.rename(columns={'Daily Sales': 'Orig Daily Sales'})\n",
    "        if 'Max. Daily Sales' in df.columns and 'Orig Max. Daily Sales' not in df.columns:\n",
    "            df = df.rename(columns={'Max. Daily Sales': 'Orig Max. Daily Sales'})\n",
    "\n",
    "        print(\"Overriding with Padang sales data...\")\n",
    "        contribution_ratio = contribution_pct / 100\n",
    "\n",
    "        # Merge with Padang's sales data using original column names\n",
    "        df = df.merge(\n",
    "            padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "                'Daily Sales': 'Padang Daily Sales',\n",
    "                'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "            }),\n",
    "            on='SKU',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "            df['Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Daily Sales'] * contribution_ratio,\n",
    "                df['Orig Daily Sales']\n",
    "            )\n",
    "            \n",
    "        if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "            df['Max. Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Max Daily Sales'] * contribution_ratio,\n",
    "                df['Orig Max. Daily Sales']\n",
    "            )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        columns_to_drop = [\n",
    "            'Padang Daily Sales', 'Padang Max Daily Sales'\n",
    "        ]\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Return empty DataFrame with required columns if there's an error\n",
    "        desired_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'HPP', 'Toko', 'Stok', \n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time', \n",
    "            'Max. Lead Time', 'Sedang PO', 'contribution_pct',\n",
    "            'emergency_po_cost', 'final_updated_regular_po_cost',\n",
    "            'Is in Padang'  # Added new column\n",
    "        ]\n",
    "        return pd.DataFrame(columns=desired_columns)\n",
    "\n",
    "def clean_po_data(df, location, contribution_pct=100, padang_sales=None):\n",
    "    \"\"\"Clean and prepare PO data with contribution calculations.\"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = df.copy()\n",
    "\n",
    "        # Keep original column names but strip any extra whitespace\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Define required columns (using original case)\n",
    "        required_columns = [\n",
    "            'Brand', 'SKU', 'Nama', 'Toko', 'Stok',\n",
    "            'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Min. Order', 'Sedang PO', 'HPP'\n",
    "        ]\n",
    "        \n",
    "        # Find actual column names in the DataFrame (case-sensitive)\n",
    "        available_columns = {col.strip(): col for col in df.columns}\n",
    "        columns_to_keep = []\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col in available_columns:\n",
    "                columns_to_keep.append(available_columns[col])\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in input data\")\n",
    "                # Add as empty column if it's required\n",
    "                if col in ['Brand', 'SKU', 'HPP']:  # These are critical\n",
    "                    df[col] = ''\n",
    "\n",
    "        # Select only the columns we need\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Check for missing required columns\n",
    "        missing_columns = [col for col in ['Brand', 'SKU', 'HPP'] if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns: {missing_columns}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Clean brand column\n",
    "        if 'Brand' in df.columns:\n",
    "            df['Brand'] = df['Brand'].astype(str).str.strip()\n",
    "\n",
    "        # Convert SKU to string and clean it\n",
    "        if 'SKU' in df.columns:\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "\n",
    "        # Convert numeric columns with better error handling\n",
    "        numeric_columns = [\n",
    "            'Stok', 'Daily Sales', 'Max. Daily Sales', 'Lead Time',\n",
    "            'Max. Lead Time', 'Sedang PO', 'HPP', 'Min. Order'\n",
    "        ]\n",
    "\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    # First convert to string, clean, then to numeric\n",
    "                    df[col] = (\n",
    "                        df[col]\n",
    "                        .astype(str)\n",
    "                        .str.replace(r'[^\\d.,-]', '', regex=True)  # Remove non-numeric except .,-\n",
    "                        .str.replace(',', '.', regex=False)         # Convert commas to decimal points\n",
    "                        .replace('', '0')                           # Empty strings to '0'\n",
    "                        .astype(float)                              # Convert to float\n",
    "                        .fillna(0)                                  # Fill any remaining NaNs with 0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not convert column '{col}' to numeric: {str(e)}\")\n",
    "                    df[col] = 0  # Set to 0 if conversion fails\n",
    "\n",
    "        # Add contribution percentage and calculate costs\n",
    "        contribution_pct = float(contribution_pct)\n",
    "        df['contribution_pct'] = contribution_pct\n",
    "        df['contribution_ratio'] = contribution_pct / 100\n",
    "\n",
    "        # Add default values for other required columns\n",
    "        if 'Lead Time Sedang PO' not in df.columns:\n",
    "            df['Lead Time Sedang PO'] = 5  # Default value\n",
    "\n",
    "        location_upper = location.upper()\n",
    "        exempt_stores = {\"PADANG\", \"SOETA\", \"BALIKPAPAN\"}\n",
    "        needs_padang_override = (location_upper not in exempt_stores) or (contribution_pct < 100)\n",
    "\n",
    "        print(f\"Processing store: {location} - {contribution_pct}%\")\n",
    "\n",
    "        # Add 'Is in Padang' column\n",
    "        if padang_sales is not None:\n",
    "            # Ensure padang_sales has the required columns\n",
    "            padang_sales = padang_sales.copy()\n",
    "            padang_sales.columns = padang_sales.columns.str.strip()\n",
    "            \n",
    "            # Convert SKU to string in both dataframes\n",
    "            df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "            padang_sales['SKU'] = padang_sales['SKU'].astype(str).str.strip()\n",
    "            \n",
    "            padang_skus = set(padang_sales['SKU'].unique())\n",
    "            df['Is in Padang'] = df['SKU'].isin(padang_skus).astype(int)\n",
    "        else:\n",
    "            print(\"Warning: No Padang sales data provided. 'Is in Padang' will be set to 0 for all SKUs.\")\n",
    "            df['Is in Padang'] = 0\n",
    "\n",
    "        if not needs_padang_override:\n",
    "            return df\n",
    "\n",
    "        if padang_sales is None:\n",
    "            raise ValueError(\n",
    "                \"Padang sales data is required for stores outside Padang/Soeta/Balikpapan \"\n",
    "                \"or any store with contribution < 100%.\"\n",
    "            )\n",
    "\n",
    "        # Process Padang sales data\n",
    "        padang_df = padang_sales.copy()\n",
    "        padang_df.columns = padang_df.columns.str.strip()\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['SKU', 'Daily Sales', 'Max. Daily Sales']\n",
    "        missing_cols = [col for col in required_cols if col not in padang_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in Padang sales data: {missing_cols}\")\n",
    "\n",
    "        # Save original sales columns if they exist\n",
    "        if 'Daily Sales' in df.columns:\n",
    "            df['Orig Daily Sales'] = df['Daily Sales']\n",
    "        if 'Max. Daily Sales' in df.columns:\n",
    "            df['Orig Max. Daily Sales'] = df['Max. Daily Sales']\n",
    "\n",
    "        print(\"Overriding with Padang sales data...\")\n",
    "        \n",
    "        # Ensure SKU is string in both dataframes before merge\n",
    "        df['SKU'] = df['SKU'].astype(str)\n",
    "        padang_df['SKU'] = padang_df['SKU'].astype(str)\n",
    "        \n",
    "        # Merge with Padang's sales data\n",
    "        df = df.merge(\n",
    "            padang_df[['SKU', 'Daily Sales', 'Max. Daily Sales']].rename(columns={\n",
    "                'Daily Sales': 'Padang Daily Sales',\n",
    "                'Max. Daily Sales': 'Padang Max Daily Sales'\n",
    "            }),\n",
    "            on='SKU',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate adjusted sales based on contribution and 'Is in Padang' flag\n",
    "        if 'Padang Daily Sales' in df.columns and 'Orig Daily Sales' in df.columns:\n",
    "            df['Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Daily Sales']\n",
    "            )\n",
    "            \n",
    "        if 'Padang Max Daily Sales' in df.columns and 'Orig Max. Daily Sales' in df.columns:\n",
    "            df['Max. Daily Sales'] = np.where(\n",
    "                df['Is in Padang'] == 1,\n",
    "                df['Padang Max Daily Sales'] * df['contribution_ratio'],\n",
    "                df['Orig Max. Daily Sales']\n",
    "            )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        columns_to_drop = [\n",
    "            'Padang Daily Sales', 'Padang Max Daily Sales', \n",
    "            'Orig Daily Sales', 'Orig Max. Daily Sales'\n",
    "        ]\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_po_data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_store_name_from_filename(filename):\n",
    "    \"\"\"Extract store name from filename, handling different patterns.\"\"\"\n",
    "    # Remove file extension and split by spaces\n",
    "    name_parts = Path(filename).stem.split()\n",
    "    \n",
    "    # Handle cases like \"002 Miss Glam Pekanbaru.csv\" -> \"Pekanbaru\"\n",
    "    # or \"01 Miss Glam Padang.csv\" -> \"Padang\"\n",
    "    if len(name_parts) >= 3 and name_parts[1].lower() == 'miss' and name_parts[2].lower() == 'glam':\n",
    "        return ' '.join(name_parts[3:]).strip().upper()\n",
    "    elif len(name_parts) >= 2 and name_parts[0].lower() == 'miss' and name_parts[1].lower() == 'glam':\n",
    "        return ' '.join(name_parts[2:]).strip().upper()\n",
    "    # Fallback: take everything after the first space\n",
    "    elif ' ' in filename:\n",
    "        return ' '.join(name_parts[1:]).strip().upper()\n",
    "    return name_parts[0].upper()\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    # List of (separator, encoding) combinations to try\n",
    "    formats_to_try = [\n",
    "        (',', 'utf-8'),      # Standard CSV with comma\n",
    "        (';', 'utf-8'),      # Semicolon with UTF-8\n",
    "        (',', 'latin1'),     # Comma with Latin1\n",
    "        (';', 'latin1'),     # Semicolon with Latin1\n",
    "        (',', 'cp1252'),     # Windows-1252 encoding\n",
    "        (';', 'cp1252')\n",
    "    ]\n",
    "    \n",
    "    for sep, enc in formats_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=sep,\n",
    "                decimal=',',\n",
    "                thousands='.',\n",
    "                encoding=enc,\n",
    "                engine='python'  # More consistent behavior with Python engine\n",
    "            )\n",
    "            # If we get here, the file was read successfully\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError, pd.errors.EmptyDataError) as e:\n",
    "            continue  # Try next format\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error reading {file_path} with sep='{sep}', encoding='{enc}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we get here, all attempts failed\n",
    "    print(f\"Failed to read {file_path} with any known format\")\n",
    "    return None\n",
    "\n",
    "def process_po_file(file_path, supplier_df, store_contrib, df_padang):\n",
    "    \"\"\"Process a single PO file and return merged data and summary.\"\"\"\n",
    "    print(f\"\\nProcessing PO file: {file_path.name} ....\")\n",
    "    \n",
    "    try:\n",
    "        # Extract location from filename using the new function\n",
    "        location = get_store_name_from_filename(file_path.name)\n",
    "        print(f\"  - Extracted location: {location}\")  # Debug print\n",
    "        \n",
    "        contribution_pct = get_contribution_pct(location, store_contrib)\n",
    "        \n",
    "        # Read the CSV with error handling\n",
    "        try:\n",
    "            # Try reading with different encodings if needed\n",
    "            # df = read_csv_file(file_path)\n",
    "            df = read_excel_file(file_path)\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "                \n",
    "            # Clean the data\n",
    "            df_clean = clean_po_data(df,location, contribution_pct, df_padang)\n",
    "            \n",
    "            # Skip if cleaning failed\n",
    "            if df_clean.empty:\n",
    "                raise ValueError(\"Data cleaning failed\")\n",
    "            \n",
    "            # Merge with suppliers\n",
    "            merged_df = merge_with_suppliers(df_clean, supplier_df)\n",
    "\n",
    "            # calculate metrics PO\n",
    "            merged_df = calculate_inventory_metrics(merged_df)\n",
    "            \n",
    "            # Generate summary\n",
    "            padang_count = (merged_df['Nama Store'] == 'Miss Glam Padang').sum()\n",
    "            other_supplier_count = ((merged_df['Nama Store'] != 'Miss Glam Padang') & \n",
    "                                  (merged_df['Nama Store'] != '')).sum()\n",
    "            \n",
    "            summary = {\n",
    "                'file': file_path.name,\n",
    "                'location': location,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'total_rows': len(merged_df),\n",
    "                'padang_suppliers': int(padang_count),\n",
    "                'other_suppliers': int(other_supplier_count),\n",
    "                'no_supplier': int((merged_df['Nama Store'] == '').sum()),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            return merged_df, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file data: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {file_path.name}: {str(e)}\"\n",
    "        print(f\"  - {error_msg}\")\n",
    "        return None, {\n",
    "            'file': file_path.name,\n",
    "            'location': location if 'location' in locals() else 'Unknown',\n",
    "            'contribution_pct': contribution_pct if 'contribution_pct' in locals() else 0,\n",
    "            'total_rows': 0,\n",
    "            'padang_suppliers': 0,\n",
    "            'other_suppliers': 0,\n",
    "            'no_supplier': 0,\n",
    "            'status': f\"Error: {str(e)[:100]}\"  # Truncate long error messages\n",
    "        }\n",
    "\n",
    "def load_padang_data_v(padang_path):\n",
    "    print(\"Parsing Padang data...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"Padang data loaded successfully..\")\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading Padang data: {str(e)}\")\n",
    "\n",
    "def load_padang_data(padang_path):\n",
    "    \"\"\"Load Padang data from either CSV or Excel file.\n",
    "    \n",
    "    Args:\n",
    "        padang_path: Path to the input file (CSV or XLSX)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and cleaned Padang data\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported or file cannot be read\n",
    "    \"\"\"\n",
    "    print(f\"Loading Padang data from {padang_path}...\")\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = str(padang_path).lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'csv':\n",
    "            # Read CSV with multiple possible delimiters and encodings\n",
    "            try:\n",
    "                df = pd.read_csv(padang_path, sep=';', decimal=',', thousands='.', encoding='utf-8-sig')\n",
    "            except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                df = pd.read_csv(padang_path, sep=',', decimal='.', thousands=',', encoding='latin1')\n",
    "                \n",
    "        elif file_ext in ['xlsx', 'xls']:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(padang_path, engine='openpyxl')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Please provide a CSV or Excel file.\")\n",
    "            \n",
    "        # Basic data cleaning\n",
    "        if not df.empty:\n",
    "            # Strip whitespace from string columns\n",
    "            df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "            \n",
    "            # Convert column names to standard format\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Ensure SKU column is string type\n",
    "            if 'SKU' in df.columns:\n",
    "                df['SKU'] = df['SKU'].astype(str).str.strip()\n",
    "                \n",
    "        print(f\"Successfully loaded Padang data with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading Padang data from {padang_path}: {str(e)}\")\n",
    "\n",
    "def format_number_for_csv(x):\n",
    "    \"\"\"Format numbers for CSV output with Indonesian locale (comma as decimal, dot as thousand)\"\"\"\n",
    "    if pd.isna(x) or x == '':\n",
    "        return x\n",
    "    try:\n",
    "        if isinstance(x, (int, float)):\n",
    "            if x == int(x):  # Whole number\n",
    "                return f\"{int(x):,d}\".replace(\",\", \".\")\n",
    "            else:  # Decimal number\n",
    "                return f\"{x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def save_to_csv(df, filename):\n",
    "    df_output = df.copy()\n",
    "\n",
    "    # Ensure SKU stays textual\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "\n",
    "    # Only touch numeric columns\n",
    "    numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "    df_output.to_csv(output_path, index=False, sep=';', decimal=',', encoding='utf-8-sig')\n",
    "    print(f\"File saved to {output_path}\")\n",
    "\n",
    "def clean_and_convert(df):\n",
    "    \"\"\"Clean and convert DataFrame columns to appropriate types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string first to handle NaN/None consistently\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Define NA values that should be treated as empty/missing\n",
    "    na_values = list(NA_VALUES)\n",
    "    \n",
    "    # Process each column\n",
    "    for col in df.columns:\n",
    "        # Replace NA values with empty string first (treating them as literals, not regex)\n",
    "        df[col] = df[col].replace(na_values, '', regex=False)\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if df[col].empty:\n",
    "            continue\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if col in NUMERIC_COLUMNS:\n",
    "            # Convert to numeric, coercing errors to NaN, then fill with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            # For non-numeric columns, ensure they're strings and strip whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace empty strings with NaN and then fill with empty string\n",
    "            df[col] = df[col].replace('', np.nan).fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file with robust error handling for problematic values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nReading excel file: {file_path.name}...\")\n",
    "        \n",
    "        # First, read the file with openpyxl directly to handle the data more carefully\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = load_workbook(\n",
    "            filename=file_path,\n",
    "            read_only=True,    # Read-only mode is faster and uses less memory\n",
    "            data_only=True,    # Get the stored value instead of the formula\n",
    "            keep_links=False   # Don't load external links\n",
    "        )\n",
    "        \n",
    "        # Get the first sheet\n",
    "        ws = wb.active\n",
    "        \n",
    "        # Get headers from the first row\n",
    "        headers = []\n",
    "        for idx, cell in enumerate(next(ws.iter_rows(values_only=True))):\n",
    "            header = str(cell).strip() if cell not in (None, '') else f\"Column_{idx + 1}\"\n",
    "            headers.append(header)\n",
    "        \n",
    "        # Initialize data rows\n",
    "        data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for row in ws.iter_rows(min_row=2, values_only=True):  # Skip header row\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                if cell is None:\n",
    "                    row_data.append('')\n",
    "                    continue\n",
    "\n",
    "                cell_str = str(cell).strip()\n",
    "                if cell_str.upper() in NA_VALUES:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    row_data.append(cell_str)\n",
    "            \n",
    "            # Only add row if it has data\n",
    "            if any(cell != '' for cell in row_data):\n",
    "                data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Normalize column data types\n",
    "        df = clean_and_convert(df)\n",
    "        \n",
    "        print(f\" Successfully processed {file_path.name} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {file_path.name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_file(df, file_path, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame to file with consistent extension and content type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path object or string for the output file\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments to pass to to_csv or to_excel\n",
    "        \n",
    "    Returns:\n",
    "        Path: The path where the file was saved\n",
    "    \"\"\"\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the correct file extension\n",
    "    if not file_path.suffix.lower() == f'.{file_format}':\n",
    "        file_path = file_path.with_suffix(f'.{file_format}')\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_output = df.copy()\n",
    "    \n",
    "    # Common preprocessing\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].astype(str).str.strip()\n",
    "        if file_format == 'xlsx':\n",
    "            # For Excel, wrap SKU in =\"...\" to preserve leading zeros\n",
    "            df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Format numbers for CSV if needed\n",
    "    if file_format == 'csv':\n",
    "        numeric_cols = df_output.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            df_output[col] = df_output[col].apply(format_number_for_csv)\n",
    "    \n",
    "    # Save based on format\n",
    "    if file_format == 'csv':\n",
    "        df_output.to_csv(\n",
    "            file_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig',\n",
    "            **kwargs\n",
    "        )\n",
    "    elif file_format == 'xlsx':\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "            df_output.to_excel(writer, index=False, **kwargs)\n",
    "            \n",
    "            # Format SKU column as text in Excel\n",
    "            if 'SKU' in df_output.columns:\n",
    "                ws = writer.sheets[list(writer.sheets.keys())[0]]\n",
    "                sku_col_idx = df_output.columns.get_loc(\"SKU\") + 1\n",
    "                for row in ws.iter_rows(\n",
    "                    min_row=2,  # Skip header\n",
    "                    max_row=ws.max_row,\n",
    "                    min_col=sku_col_idx,\n",
    "                    max_col=sku_col_idx\n",
    "                ):\n",
    "                    for cell in row:\n",
    "                        cell.number_format = numbers.FORMAT_TEXT\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "    \n",
    "    print(f\"File saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_to_complete_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save Complete format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_output = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "\n",
    "    # Ensure SKU stays textual\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_m2_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save M2 format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with regular PO qty > 0\n",
    "    df_filtered = df[df['final_updated_regular_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[['Toko', 'SKU', 'HPP', 'final_updated_regular_po_qty']]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_M2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_M2_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def save_to_emergency_po_format(df, filename, file_format='csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Save emergency PO format file with consistent extension.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        filename: Output filename (with or without extension)\n",
    "        file_format: 'csv' or 'xlsx'\n",
    "        **kwargs: Additional arguments for save_file\n",
    "    \"\"\"\n",
    "    # Filter to only include rows with emergency PO qty > 0\n",
    "    df_filtered = df[df['emergency_po_qty'] > 0].copy()\n",
    "    df_output = df_filtered[[\n",
    "        'Brand', 'SKU', 'Nama', 'Toko', 'HPP', \n",
    "        'emergency_po_qty', 'emergency_po_cost'\n",
    "    ]]\n",
    "    \n",
    "    # Ensure SKU stays textual\n",
    "    if 'SKU' in df_output.columns:\n",
    "        df_output['SKU'] = df_output['SKU'].apply(lambda x: f'=\"{x}\"')\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    OUTPUT_EMERGENCY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with consistent extension\n",
    "    output_path = OUTPUT_EMERGENCY_DIR / filename\n",
    "    return save_file(df_output, output_path, file_format=file_format, **kwargs)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    supplier_df = load_supplier_data(SUPPLIER_PATH)\n",
    "    store_contrib = load_store_contribution(STORE_CONTRIBUTION_PATH)\n",
    "    all_summaries = []\n",
    "\n",
    "    # get padang df first\n",
    "    df_padang = load_padang_data('data/rawpo/xlsx/1. Miss Glam Padang.xlsx')\n",
    "\n",
    "    # test_xlsx_convert()\n",
    "\n",
    "    # Process each PO file\n",
    "    for file_path in sorted(RAWPO_XLSX_DIR.glob('*.xlsx')):\n",
    "        print(f\"Reading file path: {file_path}\")\n",
    "        try:\n",
    "            merged_df, summary = process_po_file(file_path, supplier_df, store_contrib, df_padang)\n",
    "\n",
    "            save_to_complete_format(merged_df, file_path.stem)\n",
    "            save_to_m2_format(merged_df, file_path.stem)\n",
    "            save_to_emergency_po_format(merged_df, file_path.stem)\n",
    "\n",
    "            summary['output_path'] = str(output_path)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  - Location: {summary['location']}\")\n",
    "            print(f\"  - Contribution: {summary['contribution_pct']}%\")\n",
    "            print(f\"  - Rows processed: {summary['total_rows']}\")\n",
    "            print(f\"  - 'Miss Glam Padang' suppliers: {summary['padang_suppliers']} rows\")\n",
    "            print(f\"  - Other suppliers: {summary['other_suppliers']} rows\")\n",
    "            print(f\"  - No supplier data: {summary['no_supplier']} rows\")\n",
    "            print(f\"  - Saved to: {output_path}\")\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Display final summary\n",
    "    if all_summaries:\n",
    "        print(\"\\nProcessing complete! Summary:\")\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        display(summary_df)\n",
    "        \n",
    "        # Show sample of last processed file\n",
    "        print(\"\\nSample of the last processed file:\")\n",
    "        display(merged_df)\n",
    "    else:\n",
    "        print(\"\\nNo files were processed successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
